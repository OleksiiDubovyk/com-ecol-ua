<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.5 Тестування гіпотез | Вступ до Екології Угруповань</title>
  <meta name="description" content="Непідручник" />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="3.5 Тестування гіпотез | Вступ до Екології Угруповань" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Непідручник" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.5 Тестування гіпотез | Вступ до Екології Угруповань" />
  
  <meta name="twitter:description" content="Непідручник" />
  

<meta name="author" content="Олексій Дубовик" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="3.4-pdf-pmf.html"/>
<link rel="next" href="3.6-stat-models.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Вступ до Екології Угруповань</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Вітання</a></li>
<li class="chapter" data-level="" data-path="передмова.html"><a href="передмова.html"><i class="fa fa-check"></i>Передмова</a>
<ul>
<li class="chapter" data-level="0.0.1" data-path="передмова.html"><a href="передмова.html#about-author"><i class="fa fa-check"></i><b>0.0.1</b> Трішки про автора</a></li>
<li class="chapter" data-level="0.0.2" data-path="передмова.html"><a href="передмова.html#whythiswork"><i class="fa fa-check"></i><b>0.0.2</b> Навіщо ця робота</a></li>
<li class="chapter" data-level="0.0.3" data-path="передмова.html"><a href="передмова.html#more-about-author"><i class="fa fa-check"></i><b>0.0.3</b> Ще трішки про автора</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="подяки.html"><a href="подяки.html"><i class="fa fa-check"></i>Подяки</a></li>
<li class="chapter" data-level="1" data-path="1-introduction.html"><a href="1-introduction.html"><i class="fa fa-check"></i><b>1</b> Вступ</a>
<ul>
<li class="chapter" data-level="1.0.1" data-path="1-introduction.html"><a href="1-introduction.html#community-def"><i class="fa fa-check"></i><b>1.0.1</b> Екологічне угруповання</a></li>
<li class="chapter" data-level="1.0.2" data-path="1-introduction.html"><a href="1-introduction.html#comm-ecol-today"><i class="fa fa-check"></i><b>1.0.2</b> Екологія угруповань сьогодні</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-about-book.html"><a href="2-about-book.html"><i class="fa fa-check"></i><b>2</b> Про книгу та Зміст</a>
<ul>
<li class="chapter" data-level="2.0.1" data-path="2-about-book.html"><a href="2-about-book.html#readme"><i class="fa fa-check"></i><b>2.0.1</b> Дисклеймер</a></li>
<li class="chapter" data-level="2.0.2" data-path="2-about-book.html"><a href="2-about-book.html#how-built"><i class="fa fa-check"></i><b>2.0.2</b> Як побудована ця книга</a></li>
<li class="chapter" data-level="2.0.3" data-path="2-about-book.html"><a href="2-about-book.html#expect"><i class="fa fa-check"></i><b>2.0.3</b> Чого чекати від цієї книги</a></li>
<li class="chapter" data-level="2.0.4" data-path="2-about-book.html"><a href="2-about-book.html#expected"><i class="fa fa-check"></i><b>2.0.4</b> Чого ця книга чекає від читача</a></li>
<li class="chapter" data-level="2.0.5" data-path="2-about-book.html"><a href="2-about-book.html#notexpect"><i class="fa fa-check"></i><b>2.0.5</b> На що не варто розраховувати</a></li>
<li class="chapter" data-level="2.1" data-path="2.1-зміст.html"><a href="2.1-зміст.html"><i class="fa fa-check"></i><b>2.1</b> Зміст</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numerical-ecology.html"><a href="3-numerical-ecology.html"><i class="fa fa-check"></i><b>3</b> Базові математичні підходи в екології</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3.1-algebra.html"><a href="3.1-algebra.html"><i class="fa fa-check"></i><b>3.1</b> Математична пам’ятка</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="3.1-algebra.html"><a href="3.1-algebra.html#дроби"><i class="fa fa-check"></i><b>3.1.1</b> Дроби</a></li>
<li class="chapter" data-level="3.1.2" data-path="3.1-algebra.html"><a href="3.1-algebra.html#математичні-символи"><i class="fa fa-check"></i><b>3.1.2</b> Математичні символи</a></li>
<li class="chapter" data-level="3.1.3" data-path="3.1-algebra.html"><a href="3.1-algebra.html#нерівності"><i class="fa fa-check"></i><b>3.1.3</b> Нерівності</a></li>
<li class="chapter" data-level="3.1.4" data-path="3.1-algebra.html"><a href="3.1-algebra.html#ступені"><i class="fa fa-check"></i><b>3.1.4</b> Ступені</a></li>
<li class="chapter" data-level="3.1.5" data-path="3.1-algebra.html"><a href="3.1-algebra.html#ряди-чисел"><i class="fa fa-check"></i><b>3.1.5</b> Ряди чисел</a></li>
<li class="chapter" data-level="3.1.6" data-path="3.1-algebra.html"><a href="3.1-algebra.html#ступені-арифметичних-операцій"><i class="fa fa-check"></i><b>3.1.6</b> Ступені арифметичних операцій</a></li>
<li class="chapter" data-level="3.1.7" data-path="3.1-algebra.html"><a href="3.1-algebra.html#лінійні-та-поліноміальні-функції"><i class="fa fa-check"></i><b>3.1.7</b> Лінійні та поліноміальні функції</a></li>
<li class="chapter" data-level="3.1.8" data-path="3.1-algebra.html"><a href="3.1-algebra.html#logs"><i class="fa fa-check"></i><b>3.1.8</b> Логарифми</a></li>
<li class="chapter" data-level="3.1.9" data-path="3.1-algebra.html"><a href="3.1-algebra.html#поширені-математичні-функції"><i class="fa fa-check"></i><b>3.1.9</b> Поширені математичні функції</a></li>
<li class="chapter" data-level="3.1.10" data-path="3.1-algebra.html"><a href="3.1-algebra.html#властивості-сум"><i class="fa fa-check"></i><b>3.1.10</b> Властивості сум</a></li>
<li class="chapter" data-level="3.1.11" data-path="3.1-algebra.html"><a href="3.1-algebra.html#властивості-добутків"><i class="fa fa-check"></i><b>3.1.11</b> Властивості добутків</a></li>
<li class="chapter" data-level="3.1.12" data-path="3.1-algebra.html"><a href="3.1-algebra.html#диференціювання"><i class="fa fa-check"></i><b>3.1.12</b> Диференціювання</a></li>
<li class="chapter" data-level="3.1.13" data-path="3.1-algebra.html"><a href="3.1-algebra.html#інтегрування"><i class="fa fa-check"></i><b>3.1.13</b> Інтегрування</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3.2-matrices.html"><a href="3.2-matrices.html"><i class="fa fa-check"></i><b>3.2</b> Лінійна алгебра</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-matrices.html"><a href="3.2-matrices.html#визначення-матриці"><i class="fa fa-check"></i><b>3.2.1</b> Визначення матриці</a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-matrices.html"><a href="3.2-matrices.html#трансформації-матриць"><i class="fa fa-check"></i><b>3.2.2</b> Трансформації матриць</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-matrices.html"><a href="3.2-matrices.html#операції-над-матрицями"><i class="fa fa-check"></i><b>3.2.3</b> Операції над матрицями</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-matrices.html"><a href="3.2-matrices.html#детермінант-власні-вектори-та-власне-значення"><i class="fa fa-check"></i><b>3.2.4</b> Детермінант, власні вектори, та власне значення</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-matrices.html"><a href="3.2-matrices.html#matrices_art"><i class="fa fa-check"></i><b>3.2.5</b> Геометричний зміст матриць</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-stats.html"><a href="3.3-stats.html"><i class="fa fa-check"></i><b>3.3</b> Ймовірність у статистиці</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-stats.html"><a href="3.3-stats.html#prob"><i class="fa fa-check"></i><b>3.3.1</b> Ймовірність</a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-stats.html"><a href="3.3-stats.html#bayes"><i class="fa fa-check"></i><b>3.3.2</b> Теорема Баєса</a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-stats.html"><a href="3.3-stats.html#mle"><i class="fa fa-check"></i><b>3.3.3</b> Правдоподібність</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-pdf-pmf.html"><a href="3.4-pdf-pmf.html"><i class="fa fa-check"></i><b>3.4</b> Розподіли ймовірності</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-pdf-pmf.html"><a href="3.4-pdf-pmf.html#pdfs"><i class="fa fa-check"></i><b>3.4.1</b> Функції розподілу ймовірності</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-pdf-pmf.html"><a href="3.4-pdf-pmf.html#bars"><i class="fa fa-check"></i><b>3.4.2</b> Опис розподілу змінної (описова статистика)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-basic-hypotheses.html"><a href="3.5-basic-hypotheses.html"><i class="fa fa-check"></i><b>3.5</b> Тестування гіпотез</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-basic-hypotheses.html"><a href="3.5-basic-hypotheses.html#hypothesis"><i class="fa fa-check"></i><b>3.5.1</b> Статистична гіпотеза</a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-basic-hypotheses.html"><a href="3.5-basic-hypotheses.html#nulldistr"><i class="fa fa-check"></i><b>3.5.2</b> Нульовий розподіл</a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-basic-hypotheses.html"><a href="3.5-basic-hypotheses.html#pval"><i class="fa fa-check"></i><b>3.5.3</b> Тестування гіпотез</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-basic-hypotheses.html"><a href="3.5-basic-hypotheses.html#paradigms"><i class="fa fa-check"></i><b>3.5.4</b> Парадигми статистичного аналізу</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-stat-models.html"><a href="3.6-stat-models.html"><i class="fa fa-check"></i><b>3.6</b> Експеримент і модель</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="3.6-stat-models.html"><a href="3.6-stat-models.html#pseudoreplication"><i class="fa fa-check"></i><b>3.6.1</b> Експериментальний дизайн та псевдореплікація</a></li>
<li class="chapter" data-level="3.6.2" data-path="3.6-stat-models.html"><a href="3.6-stat-models.html#regression"><i class="fa fa-check"></i><b>3.6.2</b> Дані та проблема моделювання</a></li>
<li class="chapter" data-level="3.6.3" data-path="3.6-stat-models.html"><a href="3.6-stat-models.html#aic"><i class="fa fa-check"></i><b>3.6.3</b> Парсимонійна модель та вибір моделі</a></li>
<li class="chapter" data-level="3.6.4" data-path="3.6-stat-models.html"><a href="3.6-stat-models.html#prcomp"><i class="fa fa-check"></i><b>3.6.4</b> Багатовимірна статистика</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="3.7-infer.html"><a href="3.7-infer.html"><i class="fa fa-check"></i><b>3.7</b> Передбачення, умовивід, та валідація</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-infer.html"><a href="3.7-infer.html#inference"><i class="fa fa-check"></i><b>3.7.1</b> Статистичний умовивід і обґрунтоване передбачення</a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-infer.html"><a href="3.7-infer.html#crossval"><i class="fa fa-check"></i><b>3.7.2</b> Крос-валідація</a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-infer.html"><a href="3.7-infer.html#bias-variance"><i class="fa fa-check"></i><b>3.7.3</b> Компроміс між упередженням та варіацією</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-popeco.html"><a href="4-popeco.html"><i class="fa fa-check"></i><b>4</b> Початки популяційної екології</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4.1-species.html"><a href="4.1-species.html"><i class="fa fa-check"></i><b>4.1</b> Вид</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-species.html"><a href="4.1-species.html#поняття-виду"><i class="fa fa-check"></i><b>4.1.1</b> Поняття виду</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-species.html"><a href="4.1-species.html#види-та-площа"><i class="fa fa-check"></i><b>4.1.2</b> Види та площа</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-population.html"><a href="4.2-population.html"><i class="fa fa-check"></i><b>4.2</b> Популяція</a></li>
<li class="chapter" data-level="4.3" data-path="4.3-pop-factors.html"><a href="4.3-pop-factors.html"><i class="fa fa-check"></i><b>4.3</b> Чинники, що впливають на популяції</a></li>
<li class="chapter" data-level="4.4" data-path="4.4-cmr.html"><a href="4.4-cmr.html"><i class="fa fa-check"></i><b>4.4</b> Моделі мічення-відлову</a></li>
<li class="chapter" data-level="4.5" data-path="4.5-intraspecific.html"><a href="4.5-intraspecific.html"><i class="fa fa-check"></i><b>4.5</b> Внутрішньовидові взаємодії</a></li>
<li class="chapter" data-level="4.6" data-path="4.6-pop-dynamics.html"><a href="4.6-pop-dynamics.html"><i class="fa fa-check"></i><b>4.6</b> Динаміка та стабільність</a></li>
<li class="chapter" data-level="4.7" data-path="4.7-metapopulation.html"><a href="4.7-metapopulation.html"><i class="fa fa-check"></i><b>4.7</b> Метапопуляція</a></li>
<li class="chapter" data-level="4.8" data-path="4.8-Leslie-matrix.html"><a href="4.8-Leslie-matrix.html"><i class="fa fa-check"></i><b>4.8</b> Моделювання популяцій</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-foundations.html"><a href="5-foundations.html"><i class="fa fa-check"></i><b>5</b> Фундаментальні поняття екології</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5.1-niche.html"><a href="5.1-niche.html"><i class="fa fa-check"></i><b>5.1</b> Екологічна ніша</a></li>
<li class="chapter" data-level="5.2" data-path="5.2-food-chain.html"><a href="5.2-food-chain.html"><i class="fa fa-check"></i><b>5.2</b> Трофічні ланцюги</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-food-webs.html"><a href="5.3-food-webs.html"><i class="fa fa-check"></i><b>5.3</b> Трофічні мережі</a></li>
<li class="chapter" data-level="5.4" data-path="5.4-keystones.html"><a href="5.4-keystones.html"><i class="fa fa-check"></i><b>5.4</b> Ключові види</a></li>
<li class="chapter" data-level="5.5" data-path="5.5-succession.html"><a href="5.5-succession.html"><i class="fa fa-check"></i><b>5.5</b> Сукцесія та клімакс</a></li>
<li class="chapter" data-level="5.6" data-path="5.6-life-history.html"><a href="5.6-life-history.html"><i class="fa fa-check"></i><b>5.6</b> Історія життя</a></li>
<li class="chapter" data-level="5.7" data-path="5.7-detectability.html"><a href="5.7-detectability.html"><i class="fa fa-check"></i><b>5.7</b> Ймовірність детекції</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-interspecific.html"><a href="6-interspecific.html"><i class="fa fa-check"></i><b>6</b> Міжвидові взаємодії</a>
<ul>
<li class="chapter" data-level="6.1" data-path="6.1-relationships.html"><a href="6.1-relationships.html"><i class="fa fa-check"></i><b>6.1</b> Типи міжвидових зв’язків</a></li>
<li class="chapter" data-level="6.2" data-path="6.2-rstar.html"><a href="6.2-rstar.html"><i class="fa fa-check"></i><b>6.2</b> Теорія поділу ресурсів</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-cascade.html"><a href="6.3-cascade.html"><i class="fa fa-check"></i><b>6.3</b> Трофічні каскади</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-comecol.html"><a href="7-comecol.html"><i class="fa fa-check"></i><b>7</b> Екологічні угруповання</a>
<ul>
<li class="chapter" data-level="7.1" data-path="7.1-sad.html"><a href="7.1-sad.html"><i class="fa fa-check"></i><b>7.1</b> Структура угруповання</a></li>
<li class="chapter" data-level="7.2" data-path="7.2-sad-model.html"><a href="7.2-sad-model.html"><i class="fa fa-check"></i><b>7.2</b> Моделі розподілів чисельності</a></li>
<li class="chapter" data-level="7.3" data-path="7.3-diversity.html"><a href="7.3-diversity.html"><i class="fa fa-check"></i><b>7.3</b> Різноманіття</a></li>
<li class="chapter" data-level="7.4" data-path="7.4-similarity.html"><a href="7.4-similarity.html"><i class="fa fa-check"></i><b>7.4</b> Подібність угруповань</a></li>
<li class="chapter" data-level="7.5" data-path="7.5-fd.html"><a href="7.5-fd.html"><i class="fa fa-check"></i><b>7.5</b> Функціональне й філогенетичне різноманіття</a></li>
<li class="chapter" data-level="7.6" data-path="7.6-islands.html"><a href="7.6-islands.html"><i class="fa fa-check"></i><b>7.6</b> Острівна біогеографія</a></li>
<li class="chapter" data-level="7.7" data-path="7.7-env-filter.html"><a href="7.7-env-filter.html"><i class="fa fa-check"></i><b>7.7</b> Середовищне фільтрування</a></li>
<li class="chapter" data-level="7.8" data-path="7.8-untb.html"><a href="7.8-untb.html"><i class="fa fa-check"></i><b>7.8</b> Нейтральна теорія біорізноманіття</a></li>
<li class="chapter" data-level="7.9" data-path="7.9-rarefaction.html"><a href="7.9-rarefaction.html"><i class="fa fa-check"></i><b>7.9</b> Рарефакція та екстраполяція</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="післяслово.html"><a href="післяслово.html"><i class="fa fa-check"></i>Післяслово</a></li>
<li class="chapter" data-level="" data-path="контакти-автора.html"><a href="контакти-автора.html"><i class="fa fa-check"></i>Контакти автора</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Вступ до Екології Угруповань</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="basic-hypotheses" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Тестування гіпотез<a href="3.5-basic-hypotheses.html#basic-hypotheses" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="hypothesis" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Статистична гіпотеза<a href="3.5-basic-hypotheses.html#hypothesis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Статистичне тестування гіпотез має багато спільного із філософією науки. <strong>Гіпотеза</strong> є припущенням, яке може бути істинним або ні. Будь-яке твердження може бути гіпотезою (наприклад, “гроза є виявом злості бородатого дядька на небі” є валідною гіпотезою), однак наукова гіпотеза має бути такою, яку можливо емпірично перевірити (див. <a href="https://uk.wikipedia.org/wiki/%D0%A1%D0%BF%D1%80%D0%BE%D1%81%D1%82%D0%BE%D0%B2%D1%83%D0%B2%D0%B0%D0%BD%D1%96%D1%81%D1%82%D1%8C">критерій спростовуваності Поппера</a>). В процесі наукового пізнання так чи інакше доводиться приймати чи відхиляти гіпотези залежно від наявних даних, однак варто пам’ятати що жодна гіпотеза не є істиною: навіть якщо всі попередні експерименти підтримують робочу гіпотезу, це не означає що наступний експеримент також її підтримає.</p>
<p>Статистичні гіпотези є прикладами гіпотез, особливістю яких є дуже формальний чисельний їх опис. Для кожної статистичної гіпотези має бути можливість записати її у вигляді рівняння, нерівності, чи логіки. Гіпотеза “гроза є виявом злості бородатого дядька на небі” не є валідною статистичною гіпотезою, однак її можна переформулювати в “ймовірність виникнення грози лінійно асоційована із густиною злих бородатих дядьків в об’ємі неба”. Так, методологічно таку гіпотезу все одно перевірити складно, але тепер у неї є математична складова, тож її можливо перевірити.</p>
<p>Статистичні гіпотези зазвичай є доволі простими твердженнями, аби їх можна було перевірити. На практиці, це часто означає пошук балансу між поглибленою трансформацією даних й тестуванням простої гіпотези. Застосовуючи <a href="https://uk.wikipedia.org/wiki/%D0%91%D1%80%D0%B8%D1%82%D0%B2%D0%B0_%D0%9E%D0%BA%D0%BA%D0%B0%D0%BC%D0%B0">принцип Оккама</a>, якщо декілька статистичних гіпотез відповідають ідентичній науковій гіпотезі і не мають суттєвої різниці між собою, варто обирати найпростішу статистичну гіпотезу. Наприклад, якщо ви намагаєтесь порівняти вибірки мас тіла в двох субпопуляціях виду, <span class="math inline">\(A\)</span> і <span class="math inline">\(B\)</span>, і припускаєте що між ними є істотна різниця, існує декілька способів сформулювати статистичну гіпотезу: <strong>(1)</strong> всі значення в <span class="math inline">\(A\)</span> більші/менші за всі значення в <span class="math inline">\(B\)</span> (не є хорошою гіпотезою, адже вона занадто консервативна – спрацює тільки коли дві хмари точок не перетинаються – і не є занадто простою, адже включає дві статистичні гіпотези <span class="math inline">\(a_i &gt; b_i \forall a_i, a_b\)</span> та <span class="math inline">\(a_i &lt; b_i \forall a_i, a_b\)</span>); <strong>(2)</strong> середнє значення вибірки <span class="math inline">\(A\)</span> більше/менше за середнє значення <span class="math inline">\(B\)</span> (менш консервативна, але все ще складна гіпотеза, що включатиме дві простіші гіпотези <span class="math inline">\(\bar{a} &gt; \bar{b}\)</span> і <span class="math inline">\(\bar{a} &lt; \bar{b}\)</span>); <strong>(3)</strong> середнє значення <span class="math inline">\(A\)</span> не дорівнює середньому значенню <span class="math inline">\(B\)</span> (включає лише одну елементарну гіпотезу <span class="math inline">\(\bar{a} \neq \bar{b}\)</span>); <strong>(4)</strong> різниця між середніми значеннями вибірок не дорівнює нулю (мабуть, є найпростішою гіпотезою, до якої можна звести це питання про маси тіла в субпопуляціях, <span class="math inline">\((\bar{a} - \bar{b}) \neq 0\)</span>).</p>
</div>
<div id="nulldistr" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Нульовий розподіл<a href="3.5-basic-hypotheses.html#nulldistr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Тестування статистичних гіпотез є доволі цікавим і, певною мірою, контрінтуїтивним процесом. Зазвичай, тести не кажуть “так, ваша статистична гіпотеза має право на життя”, а виходять із протилежного твердження – <strong>нульової гіпотези</strong> (<em>null hypothesis</em>), і, відтак, радше кажуть “не знаю як щодо вашої статистичної гіпотези, але альтернатива їй взагалі не підтверджується наявними даними”. Відтак, для кожної статистичної гіпотези (твердження <span class="math inline">\(H\)</span>) існує певна нульова гіпотеза, яка намагається пояснити дані із припущення того, що <span class="math inline">\(H\)</span> не відповідає дійсності (позначимо нульову гіпотезу як <span class="math inline">\(H_0\)</span>). Як альтернатива нульовій гіпотезі існує <strong>альтернативна гіпотеза</strong> (<em>alternative hypothesis</em>) <span class="math inline">\(H_A\)</span>, яка робить твердження протилежне до <span class="math inline">\(H_0\)</span> і, відтак, узгоджується із <span class="math inline">\(H\)</span>.</p>
<p>Навіщо це потрібно? Візьмемо до уваги попередній приклад: нашою науковою гіпотезою є те, що особини субпопуляцій А і B відрізняються масою тіла. Для перевірки цієї наукової гіпотези ми формулюємо чітку статистичну гіпотезу <span class="math inline">\(H: (\bar{a} - \bar{b}) \neq 0\)</span>. Як перевірити цю статистичну гіпотезу? Можна, звісно, порахувати середні двох вибірок, і можна гарантувати що їх різниця не дорівнюватиме нулю навіть якщо вони не надто сильно різняться – отримати вибірки із ідентичними середніми арифметичними дуже малоймовірно. Відтак, ця різниця буде відрізнятись від нуля, і для перевірки статистичної гіпотези необхідно зрозуміти яка різниця між середніми є достатньо великою, аби не вважатись просто статистичним шумом. В цій ситуації у нас є дві взаємозаперечні статистичні гіпотези: нульова <span class="math inline">\(H_0: (\bar{a} - \bar{b}) = 0\)</span> та альтернативна <span class="math inline">\(H_A: (\bar{a} - \bar{b}) \neq 0\)</span>. Відтак, для відповіді на попереднє питання необхідно знати, який розподіл би мала різниця між середніми <span class="math inline">\((\bar{a} - \bar{b})\)</span> за умови що <span class="math inline">\(H_0\)</span> є істинною. Цей розподіл можна назвати нульовим (null distribution), із яким можна порівняти спостережене значення <span class="math inline">\((\bar{a} - \bar{b})\)</span> і вирішити чи “так, спостережена різниця набагато більша від випадкового шуму навколо нуля в нульовому розподілі” (відповідно, відхилити <span class="math inline">\(H_0\)</span> та прийняти <span class="math inline">\(H_A\)</span>) або “ні, спостережена різниця настільки незначна, що її можна було б очікувати навіть якщо насправді різниці нема” (відхилити <span class="math inline">\(H_A\)</span> та прийняти <span class="math inline">\(H_0\)</span>).</p>
<p>За усієї своєї простоти, адекватний статистичний аналіз неможливий без адекватної нульової моделі (<a href="https://www.jstor.org/stable/2096971">Harvey et al. 1983</a>) – і ця тема настільки важлива, що їй присвячені цілі книги (<a href="https://www.uvm.edu/~ngotelli/nullmodelspage.html">Gotelli &amp; Graves 1996</a>)! Готеллі визначає <strong>нульову модель</strong> як “<em>модель, що генерує тренди на підставі рандомізованих екологічних даних чи випадкової вибірки із відомого чи уявного розподілу […] створеного для утворення трендів, які можна було би очікувати за відсутності певного механізму [в якому ми зацікавлені]</em>”.</p>
<p>Цікавим екологічним прикладом є відомий факт того, що біологічне різноманіття поблизу екватору нашої планети набагато вище порівняно із різноманіттям поблизу полюсів. Існує декілька наукових гіпотез, що пояснюють це спостереження: <strong>(1)</strong> продуктивність екосистем поблизу екватору вища, відповідно, є більше ресурсів для більшої кількості особин, що означає більше видів (<a href="https://doi.org/10.1111/j.1461-0248.2004.00671.x">Currie et al. 2004</a>); <strong>(2)</strong> тропіки є найбільшим біомом, тож є більше площі для підтримки більшої кількості видів (<a href="https://doi.org/10.2307/3546528">Rosenzweig &amp; Sandlin 1997</a>); <strong>(3)</strong> тропіки є старішим біомом, відповідно, було більше часу для видоутворення (<a href="https://doi.org/10.1146/annurev-ecolsys-112414-054102">Fine 2015</a>); <strong>(4)</strong> навколо тропіків темпи видоутворення вищі, а вимирання – нижчі (<a href="https://doi.org/10.1111/j.1461-0248.2007.01020.x">Mittelbach et al. 2007</a>). Яку нульову модель використати для статистичної перевірки цих гіпотез? Мабуть, рівномірний розподіл різноманіття видів не є найкращою моделлю зважаючи на те, що навколо полюсів менше площі, та й не зрозуміло як врахувати форму планети тощо. Натомість, найпростішою нульовою моделлю варто вважати ефект середнього домену (mid-domain effect): якщо випадкові ареали видів розподілені між полюсами випадково, то суто із геометричних причин більше ареалів перетинатимуться десь між полюсами, й, відповідно, кількість видів буде найбільша посередині, коло екватору (Рис. <a href="3.5-basic-hypotheses.html#fig:fig-3-11">3.11</a>).</p>
<div class="figure"><span style="display:block;" id="fig:fig-3-11"></span>
<img src="images/mid_domain.png" alt="Ілюстрація ефекту середнього домену для пояснення розподілу біологічного різноманіття на планеті олівцями в коробці: якщо ареали видів випадкових діапазонів широт випадково розподілені між полюсами, то найбільше перетинів видів існуватиме поблизу полюсу. Фіолетові комірки відповідають кількості пересікань із олівцями за горизонталлю." width="1650" />
<p class="caption">
Рис. 3.11: Ілюстрація ефекту середнього домену для пояснення розподілу біологічного різноманіття на планеті олівцями в коробці: якщо ареали видів випадкових діапазонів широт випадково розподілені між полюсами, то найбільше перетинів видів існуватиме поблизу полюсу. Фіолетові комірки відповідають кількості пересікань із олівцями за горизонталлю.
</p>
</div>
</div>
<div id="pval" class="section level3 hasAnchor" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Тестування гіпотез<a href="3.5-basic-hypotheses.html#pval" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Суть будь-якого статистичного тесту полягає в оцінці певної статистики тесту (метрики) і оцінці значущості цієї метрики для висновку щодо істинності статистичної гіпотези за певного нульового розподілу. Залежно від парадигми статистичного аналізу, існують різні способи отримати нульовий розподіл. Наприклад, параметричні тести виходять із багатьох припущень і виводять параметричні нульові розподіли. Саме тому настільки важливо дотримуватись припущень тестів (вибірки мають бути нормально розподілені, мати однакові параметри варіації, бути незалежними тощо – в кожного тесту свій набір припущень).</p>
<p>Результатом статистичного тесту, зазвичай, є оцінка метрики тесту і асоційоване <span class="math inline">\(p\)</span>-значення – cвящений грааль і наріжний камінь всякого аналізу, який всі хочуть оцінити але не всі знають що то таке. Мета статистичного тесту – це оцінити наскільки ймовірно було би отримати певні результати тесту якщо нульова гіпотеза є істинною. Статистичне тестування подібне до судового процесу. Уявіть собі, що підсудного звинувачують у скоєнні злочину (нульова гіпотеза за презумпції невинуватості – підсудний невинний, альтернативна – підсудний винний). Перед судом постає відповідальна й непроста задача прийняти рішення щодо винуватості підсудного за наявних даних. Реальність же може відповідати одному з двох варіантів: або підсудний справді вчинив злочин, або ні.</p>
<ul>
<li><p>Якщо підсудний справді не вчиняв злочину (нульова гіпотеза істинна),</p>
<ul>
<li><p>рішення суду щодо винуватості підсудного (хибно-позитивний результат) відправить невинну людину за ґрати, або</p></li>
<li><p>рішення суду щодо невинуватості (дійсно-негативний результат) залишить і підсудного, і суспільство задоволеними,</p></li>
</ul></li>
<li><p>якщо ж підсудний насправді вчинив злочин (альтернативна гіпотеза істинна), то</p>
<ul>
<li><p>рішення суду щодо винуватості підсудного (дійсно-позитивний результат) виллється у відбування заслуженого покарання, або</p></li>
<li><p>рішення суду щодо невинуватості (хибно-негативний результат) відправить злочинця на волю, що є небезпечним для суспільства.</p></li>
</ul></li>
</ul>
<p>Відтак, в такому уявному судовому процесі, залежно від реальності, можна припуститись однієї з двох критичних помилок:</p>
<ul>
<li><p><strong>помилки першого роду</strong> – відправити невинну людину за ґрати – прийняти альтернативну гіпотезу, коли нульова є істинною, або</p></li>
<li><p><strong>помилки другого роду</strong> – відпустити злочинця на волю – прийняти нульову гіпотезу, коли альтернативна є істинною.</p></li>
</ul>
<p>Яка із цих помилок є страшнішою, мабуть, є філософським питанням, і кожне суспільство нехай відповідає на нього самостійно. В науці ж помилка першого роду відповідатиме видаванню за істину доказів, які не відповідають дійсності; в той час як помилка другого роду – ігноруванню фактів. Як на мене, помилка першого роду шкодитиме науковому знанню сильніше.</p>
<p>Якби істина була відома, тоді в кожній парі нульової-альтернативної гіпотез можна було би оцінити дві ймовірності:</p>
<ul>
<li><p><span class="math inline">\(\alpha = P(\text{помилка I роду} = P(\text{відхилити } H_0| H_0 \text{ істина})\)</span>, яку ще називають <strong>рівнем значущості</strong> (<em>significance level</em>), та</p></li>
<li><p><span class="math inline">\(\beta = P(\text{помилка II роду} = P(\text{не відхилити } H_0| H_0 \text{ хибна})\)</span>.</p></li>
</ul>
<p>Два значення <span class="math inline">\(\alpha\)</span> і <span class="math inline">\(\beta\)</span> є зворотньо пов’язаними (Рис. <a href="3.5-basic-hypotheses.html#fig:fig-3-12">3.12</a>): збільшення одного значення зменшить інше. Відтак, завдання відповідального статистичного аналізу – збалансувати <span class="math inline">\(\alpha\)</span> і <span class="math inline">\(\beta\)</span>, знайти якесь максимальне значення <span class="math inline">\(\alpha\)</span>, яке ми можемо толерувати. На практиці, за таке значення часто приймають <span class="math inline">\(\alpha = 0.05\)</span> (за незалежного повторення 20 експериментів, ми очікуємо принаймні одного хибно-позитивного результату). Втім, критичне значення <span class="math inline">\(\alpha\)</span> для будь-якого тесту повинне враховувати баланс між <span class="math inline">\(\alpha\)</span> і <span class="math inline">\(\beta\)</span>, тим, наскільки небажаними є помилки першого та другого роду. Отже, потрібно завжди пам’ятати що порогове значення <span class="math inline">\(\alpha = 0.05\)</span> є лише умовністю.</p>
<p>Цікавою метрикою тесту, яку варто побічно згадати, є також <strong>потужність тесту</strong> (<em>power</em>) <span class="math inline">\((1 - \beta)\)</span>. Аналіз потужності (<em>power analysis</em>) є поширеним методом знаходження мінімального розміру вибірки для адекватної інтерпретації результатів статистичного тесту. Цей тип аналізу допоможе відповісти на питання “якщо я хочу провести [цей конкретний статистичний тест] із рівнем значущості [<span class="math inline">\(\alpha\)</span>], то який мінімальний розмір вибірки потрібно було би набрати на стадії збору даних?”.</p>
<div class="figure"><span style="display:block;" id="fig:fig-3-12"></span>
<img src="bookdown-demo_files/figure-html/fig-3-12-1.png" alt="***(a)*** Уявімо, що можливо оцінити розподіл статистики тесту $z$ за нульової ($H_0$) та альтернативної ($H_A$) гіпотез. Тоді було би можливо оцінити пов'язані ймовірності припуститися помилки першого роду ($\alpha$, хибно-позитивне рішення) та помилки другого роду ($\beta$, хибно-негативне рішення) як площі під кривими розподілів статистик. За спостереження статистики тесту $z_{obs}$ на рисунку ***(b)***, $p$-значення відповідає ймовірності спостерігати таке ж або більш екстремальне значення статистики за нульової гіпотези." width="1152" />
<p class="caption">
Рис. 3.12: <strong><em>(a)</em></strong> Уявімо, що можливо оцінити розподіл статистики тесту <span class="math inline">\(z\)</span> за нульової (<span class="math inline">\(H_0\)</span>) та альтернативної (<span class="math inline">\(H_A\)</span>) гіпотез. Тоді було би можливо оцінити пов’язані ймовірності припуститися помилки першого роду (<span class="math inline">\(\alpha\)</span>, хибно-позитивне рішення) та помилки другого роду (<span class="math inline">\(\beta\)</span>, хибно-негативне рішення) як площі під кривими розподілів статистик. За спостереження статистики тесту <span class="math inline">\(z_{obs}\)</span> на рисунку <strong><em>(b)</em></strong>, <span class="math inline">\(p\)</span>-значення відповідає ймовірності спостерігати таке ж або більш екстремальне значення статистики за нульової гіпотези.
</p>
</div>
<p>Підхід <strong><span class="math inline">\(p\)</span>-значення</strong> (p-value) у статистичному тестуванні намагається оцінити ймовірність того, що <em>значення статистики тесту за істинності нульової гіпотези дорівнюватиме або буде ще більш екстремальним порівняно із спостереженим значенням</em>. Варто зазначити, що на практиці оперують лише точковим спостереженим значенням статистики тесту (наприклад, різницею середніх значень вибірок <span class="math inline">\((\bar{a} - \bar{b})\)</span>) та нульовим розподілом (як би були розподілені <span class="math inline">\((\bar{a} - \bar{b})\)</span> якщо <span class="math inline">\(H_0: \bar{a} = \bar{b}\)</span>?). Відтак, <span class="math inline">\(p\)</span>-значення каже наскільки можна очікувати отриманого результату тесту за нульової гіпотези. Якщо ця ймовірність дуже маленька (наприклад, менше за обране порогове значення <span class="math inline">\(\alpha\)</span>), тоді варто нульову гіпотезу відхилити й існують підстави вважати, що між двома вибірками є істотна різниця. Якщо ж ця ймовірність значна, тоді немає підстав вважати, що отримані результати є чимось більшим аніж статистичним шумом за нульової гіпотези.</p>
<p>Варто звернути увагу на поняття <strong><em>двосторонніх та односторонніх тестів</em></strong> (<em>two-sided</em>, <em>upper-tail one-sided</em>, <em>lower-tail one-sided</em>), які відповідають чітко сформульованій альтернативній гіпотезі. Наприклад, якщо ми перевіряємо факт різниці між двома середніми, то альтернативна гіпотеза виглядає як <span class="math inline">\(H_A: \bar{a} \neq \bar{b} \Leftrightarrow (\bar{a} - \bar{b}) \neq {0}\)</span>, отже, зони відхилення нульової гіпотези будуть знаходитись по обидва боки нульового розподілу із ймовірностями <span class="math inline">\(\alpha/2\)</span>. Якщо ж є підстави вважати що одне середнє більше за інше, альтернативна гіпотеза виглядатиме як <span class="math inline">\(H_A: \bar{a} &gt; \bar{b} \Leftrightarrow (\bar{a} - \bar{b}) &gt; {0}\)</span> або <span class="math inline">\(H_A: \bar{a} &lt; \bar{b} \Leftrightarrow (\bar{a} - \bar{b}) &lt; {0}\)</span>. Відповідно, в такому випадку зона відхилення нульової гіпотези буде знаходитись у верхньому або нижньому хвості нульового розподілу і її ймовірність становитиме <span class="math inline">\(\alpha\)</span>.</p>
<p>Отже, тестування статистичної гіпотези включає наступні кроки:</p>
<ul>
<li><p>формулювання нульової <span class="math inline">\(H_0\)</span> та альтернативної <span class="math inline">\(H_A\)</span> гіпотез,</p></li>
<li><p>вибір критичного значення рівня значущості <span class="math inline">\(\alpha\)</span>,</p></li>
<li><p>отримання вибірки достатнього розміру і обчислення статистики обраного тесту <span class="math inline">\(z\)</span>,</p></li>
<li><p>порівняння спостереженої статистики тесту із нульовим розподілом<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a>. Є два поширених способи провести цю операцію:</p>
<ul>
<li><p>древній метод: поглянути в таблицю критичних значень за певних параметрів нульового розподілу (<span class="math inline">\(z_{\alpha}\)</span>) і порівняти спостережену статистику із критичною для певного <span class="math inline">\(\alpha\)</span>,</p></li>
<li><p>адекватніший підхід: обчислити точне значення для параметризованого нульового розподілу та спостереженої статистики тесту,</p></li>
</ul></li>
<li><p>зробити висновки щодо відхилення нульової гіпотези.</p></li>
</ul>
</div>
<div id="paradigms" class="section level3 hasAnchor" number="3.5.4">
<h3><span class="header-section-number">3.5.4</span> Парадигми статистичного аналізу<a href="3.5-basic-hypotheses.html#paradigms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Різниця між трьома основними парадигмами статистичного аналізу, їх переваги й недоліки, та чим вони відрізняються доволі влучно описано в підручнику Готеллі та Еллісон <a href="https://learninglink.oup.com/access/gotelli-a-primer-of-ecological-statistics-2e">“Початки екологічної статистики”</a>. В цьому підрозділі я наводжу лише основні ідеї цих парадигм, але варто пам’ятати, що дослідженню кожної з них можна приділити роки життя.</p>
<div id="частотницька-або-фреквентистська-парадигма" class="section level4 hasAnchor" number="3.5.4.1">
<h4><span class="header-section-number">3.5.4.1</span> Частотницька, або фреквентистська парадигма<a href="3.5-basic-hypotheses.html#частотницька-або-фреквентистська-парадигма" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Найпоширеніший підхід до статистичного аналізу – частотницький (<em>frequentist approach</em>) – ґрунтується на припущенні, що ймовірність описує ніщо інше, як частоту подій за безкінечного повторення експерименту, й, відповідно, ми можемо спостерігати лише якусь зліченну кількість подій і на їх підставі спробувати оцінити асоційовані ймовірності. Цей підхід часто спрощує спостережувану реальність до ідеальних моделей (які можуть бути дуже складними математично, але все ж простішими за дійсність), які відображені у параметричних моделях – математичних функціях із відносно незначною кількістю параметрів. Відтак, і тестування гіпотез базується на нульових розподілах із визначеним математичним формулюванням й параметрами (такими розподілами є, наприклад, нормальний, t-розподіл Стьюдента, F-розподіл Фішера тощо).</p>
<p>Параметричні методи завжди мають набір припущень, і перед застосуванням цих методів завжди необхідно перевіряти чи ваші дані відповідають цим припущенням. Більшість параметричних методів матимуть припущення, що <strong>(1)</strong> всі спостереження в даних зібрані незалежно й випадково, та <strong>(2)</strong> дані зібрані із генеральних сукупностей із певним визначеним розподілом. Перше припущення не те щоб є специфічним для частотницької парадигми, а є критичним для експериментального дизайну за будь-якого статистичного підходу: вибір залежних між собою спостережень або невипадковий підбір спостережень викликатиме упередження в даних, тож результати всякого статистичного аналізу не будуть адекватними. Друге припущення найчастіше каже, що вибірки в аналізі повинні бути розподілені нормально.</p>
<p>Існують, звісно, і непараметричні методи, однак їх підхід до визначення ймовірності події залишається незмінним. Непараметричні підходи мають послаблені вимоги до розподілу вибірки і є чудовою альтернативою параметричним тестам коли, скажімо, не вдається підтвердити що ваші дані розподілені нормально. Із непараметричними методами завжди варто бути обережними, оскільки навіть якщо вони є надійними (robust) за порушення припущення про розподіл даних, вони є чутливими до всіх інших припущень<a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a>.</p>
</div>
<div id="баєсівська-парадигма" class="section level4 hasAnchor" number="3.5.4.2">
<h4><span class="header-section-number">3.5.4.2</span> Баєсівська парадигма<a href="3.5-basic-hypotheses.html#баєсівська-парадигма" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Баєсівська парадигма є складнішою для інтуїтивного розуміння і вимагає більшої підготовки. Крім того, я неодноразово чув думку, що Баєсівський метод – то часто лише модний спосіб тестувати гіпотезу, для якої звичайний параметричний тест дав би таку ж відповідь. Філософія Баєсівського підходу полягає в тому, що часто в розпорядженні існують попередні дані щодо тестованої гіпотези. У частотницькій парадигмі кожен окремий експеримент відбувається “наосліп”, адже існує припущення, що кожен експеримент отримає якусь репрезентативну вибірку із генеральної сукупності і із його результатів можна судити про тренди в самій генеральній сукупності. Однак, якщо ви проводите експеримент, подібний до якого вже хтось колись проводив, то чи не розсудливіше врахувати ті попередні, <em>пріорні</em> дані?</p>
<p>Баєсівський аналіз намагається поглибити наші знання щодо наукової гіпотези із кожним експериментом із врахуванням попередніх даних, що, в цілому, відповідає сучасному науковому підходу: із кожним експериментом ми шліфуємо наявне наукове знання. Скажімо, ми намагаємось оцінити якийсь параметр в генеральній сукупності. Першим кроком у Баєсівському аналізі було би змиритись із думкою про те, що якщо ми намагаємось оцінити цей параметр і його оновлене оцінене значення, скоріш за все, буде відрізнятись від пріорної оцінки, то цей параметр є не фіксованим значенням, а, радше, розподілом значень. Із попередніх експериментів мають бути наявні оцінки розподілу параметру, тож цей розподіл ми назвемо <em>пріорним</em>, і тепер нашим завданням є оцінити <em>постеріорний</em> розподіл параметру за даних нового експерименту, що можна зробити із застосуванням теореми Баєса:</p>
<p><span class="math display">\[P(\text{гіпотеза}|\text{дані}) = \frac{P(\text{гіпотеза})P(\text{дані}|\text{гіпотеза})}{P(\text{дані})}\]</span></p>
<p>Отже, якщо частотницькі методи питають <em>яка ймовірність отримати значення статистики рівне або більш екстремальне за спостережене значення в наявних даних, якщо нульова гіпотеза істинна</em>, то Баєсівський підхід намагається відповісти <em>яка ймовірність гіпотези про статистику тесту за наявних даних</em>. У цій формулі <span class="math inline">\(P(\text{гіпотеза}|\text{дані})\)</span> називають <strong>постеріорною ймовірністю</strong>, <span class="math inline">\(P(\text{гіпотеза})\)</span> – <strong>пріорною ймовірністю</strong>, <span class="math inline">\(P(\text{дані}|\text{гіпотеза})\)</span> – <strong><a href="3.3-stats.html#mle">правдоподібністю</a></strong> (що відображає ймовірність спостереження цього конкретного набору даних якщо гіпотеза істинна), в той час як знаменник <span class="math inline">\(P(\text{дані})\)</span> у Баєсівській теоремі є лише нормалізуючою константою (ймовірність даних за усіх можливих гіпотез), якою можна знехтувати і переписати вираз як</p>
<p><span class="math display">\[P(\text{гіпотеза}|\text{дані}) \propto P(\text{гіпотеза})P(\text{дані}|\text{гіпотеза})\]</span></p>
<p>Аби ще сильніше ускладнити інтуїтивне розуміння цієї парадигми, кожну із ймовірностей у цій формулі варто уявляти не як точкове значення ймовірності, а як <a href="3.4-pdf-pmf.html#pdfs">розподіли густини ймовірності</a>. Вибір <em>пріорного розподілу</em> залежить від попередньо існуючих даних: навіть із описових даних можна спробувати визначити певний виправданий розподіл. Якщо ж цього не вдається зробити, альтернативою (яку дуже часто використовують) є визначення <em>неінформативного розподілу</em> (uninformative prior), наприклад, нормального розподілу із настільки високим параметром <span class="math inline">\(\sigma^2\)</span>, що на локальних діапазонах він апроксимує до плаского рівномірного розподілу. У виборі неінформативного пріорного розподілу й криється критика повсюдного застосування Баєсівської парадигми: якщо в моделі відсутнє адекватно визначене пріорне знання, то Баєсівський аналіз не має жодних переваг над частотнитцькими методами (<a href="https://doi.org/10.1111/oik.05985">Lemoine 2019</a>). Відтак, вибір пріорного розподілу повинен бути обґрунтованим.</p>
<p>Подальші кроки вимагають оцінки функції правдоподібності даних за істинності гіпотези (<span class="math inline">\(P(\text{дані}|\text{гіпотеза}\)</span>) та нормалізуючої константи інтегрованої правдоподібності (<em>marginal likelihood</em>), що іноді можна зробити аналітично, але, зазвичай, розв’язується за допомогою алгоритмів ітеративно. На виході Баєсівський підхід повертає розподіл постеріорної ймовірності. Як із розподілу зробити висновок? Поширеним інструментом є оцінка <strong>імовірного інтервалу</strong> (<em>credibility interval</em>, не плутати із довірчим інтервалом, <em>confidence interval</em>), наприклад, 95% імовірного інтервалу як 2.5%- і 97.5%-ті перцентилі постеріорного розподілу. Розташування спостереженої статистики тесту відносно 95% імовірного інтервалу постеріорного розподілу (в межах або за межами), відтак, дає підставу зробити висновок щодо статистичної гіпотези.</p>
<p>В контексті Баєсівської парадигми варто згадати методи Монте-Карло ланцюгів Маркова (<em>Markov chain Monte Carlo</em>, MCMC). <strong>Процес Маркова</strong>, або <strong>ланцюг Маркова</strong> – це такий процес, в якому об’єкт в момент часу має певне значення, і переходить в інший стан в наступний момент часу. Такі стани можуть бути як неперервною змінною, так і дискретною; найпростіше для розуміння уявляти скінченний, бажано невеликий, набір дискретних станів. Популярним прикладом є погодні умови в певний день. Скажімо, набір можливих станів в цій системі є <span class="math inline">\(\{\text{сонячно}, \text{хмарно}, \text{дощ}\}\)</span>, і кожен день приймає один із цих станів. Якби такий процес був Марковським, то в кожен окремий день ймовірність стану залежить тільки від стану в попередній день; <em>випадковий процес, в якому ймовірність стану в момент часу за відомої послідовності станів в усі попередні кроки залежить тільки від стану протягом останнього попереднього кроку</em> називають таким, що має властивість Маркова. Ланцюг Маркова описується набором ймовірностей переходу від станів. Ланцюги Маркова часто зображають у вигляді графічних ланцюгів (Рис. <a href="3.5-basic-hypotheses.html#fig:fig-3-13">3.13</a>), в той час як їх математична репрезентація виглядає як <a href="3.2-matrices.html#matrices">матриця</a> ймовірностей переходів – <strong>матриця переходів</strong> <span class="math inline">\(\mathbf{T}\)</span> (<em>transition matrix</em>). В таких матрицях рядки відповідають попереднім станам, а колонки – наступним. Сума ймовірностей переходів із певного стану (сума рядків) повинна дорівнювати одиниці.</p>
<div class="figure"><span style="display:block;" id="fig:fig-3-13"></span>
<img src="images/mc.png" alt="Гіпотетичний ланцюг Маркова у графічному вигляді та у вигляді матриці ймовірностей переходів, що описує погоду протягом дня як один із трьох можливих станів. Після сонячного дня варто очікувати сонячний день, після хмарного -- дощів, після дощового -- сонячного тощо." width="1631" />
<p class="caption">
Рис. 3.13: Гіпотетичний ланцюг Маркова у графічному вигляді та у вигляді матриці ймовірностей переходів, що описує погоду протягом дня як один із трьох можливих станів. Після сонячного дня варто очікувати сонячний день, після хмарного – дощів, після дощового – сонячного тощо.
</p>
</div>
<p>Звісно, розвиток подій в ланцюзі Маркова може залежати від розподілу станів на стадії його ініціалізації – а отже, і на кожному кроці частоту станів можна описати як функцію розподілу ймовірності. Певні ланцюги Маркова можуть досягнути рівноважного стану, коли розподіли ймовірності перестають змінюватись із наступним кроком. Відповідно, <em>стаціонарним</em> (<em>stationary</em>) розподілом називають такий розподіл <span class="math inline">\(\vec{\pi}\)</span>, для якого справджується умова, що <span class="math inline">\(\vec{\pi} \mathbf{T} = \vec{\pi}\)</span>. Пермутації Монте-Карло ланцюгів Маркова відповідають алгоритмам, які ітерують велику кількість кроків крізь такий Марковський процес, стаціонарний розподіл якого відповідає шуканому розподілу. Найвідомішим є <strong>алгоритм Метрополіса-Хастінгса</strong> (<em>Metropolis-Hastings algorithm</em>) для отримання випадкової вибірки <span class="math inline">\(X\)</span> із певного складного розподілу або просто функції <span class="math inline">\(f(x)\)</span>, із якої непросто отримати випадкову змінну аналітичним шляхом. В цьому алгоритмі нарощується ланцюг випадкових значень, що на початку алгоритму обирається із заданого пріорного розподілу. На кожному кроці <span class="math inline">\((i)\)</span> алгоритм бере до уваги значення <span class="math inline">\(x_{i-1}\)</span> із розподілу попереднього кроку, і будує навколо <span class="math inline">\(x_{i-1}\)</span> якийсь визначений розподіл <span class="math inline">\(g(x_{i-1})\)</span> (наприклад, нормальний із фіксованою між кроками варіацією <span class="math inline">\(\mathcal{N}(\mu = x, \sigma^2)\)</span>). Із цього розподілу обираєтся випадкове значення-кандидат <span class="math inline">\(x&#39;\)</span> таке що <span class="math inline">\(x` \sim g(x_{i-1})\)</span> (якщо було обрано нормальний розподіл, то<span class="math inline">\(x&#39; \sim \mathcal{N}(\mu = x_{i-1}, \sigma^2)\)</span>). Після цього алгоритм обирає, чи взяти до уваги <span class="math inline">\(x&#39;\)</span>, при чому ймовірність вибору значення-кандидата <span class="math inline">\(x&#39;\)</span> визначається відношенням <span class="math inline">\(g(x&#39;)/g(x_{i-1})\)</span>. Якщо обрано <span class="math inline">\(x&#39;\)</span>, то нове значення в розподілі нарощене протягом цього кроку становитиме <span class="math inline">\(x_i = x&#39;\)</span>, якщо ж ні, то <span class="math inline">\(x_i = x_{i-1}\)</span>. Процедура повторюється багаторазово (розряду тисяч разів), і в результаті видає ланцюг <span class="math inline">\(\{x_1, x_2, x_3, \cdots, x_n\}\)</span>. Із цього ланцюга викидається певна кількість перших <span class="math inline">\(m\)</span> ланок, оскільки в них алгоритм іноді видає невдалі значення – цей період називають розігрівом (<em>burn-in</em>). Залишок ланцюга <span class="math inline">\(\{x_{m+1}, x_{m+2}, \cdots, x_n\}\)</span> ж утворює вибірку, розподіл якої апроксимує до шуканого. Якщо шуканий розподіл відповідає постеріорному розподілу в Баєсівській парадигмі, а функція <span class="math inline">\(g(x)\)</span> пропорційна до функції правдоподібності даних за тестованої гіпотези, то цей алгоритм є вдалим вибором для Баєсівського аналізу. В мережі нескладно знайти <a href="https://blog.djnavarro.net/posts/2023-04-12_metropolis-hastings/">приклади</a> простих алгоритмів МСМС на R.</p>
</div>
<div id="permutation-paradigm" class="section level4 hasAnchor" number="3.5.4.3">
<h4><span class="header-section-number">3.5.4.3</span> Пермутаційний аналіз<a href="3.5-basic-hypotheses.html#permutation-paradigm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Принцип роботи МСМС навіює певний настрій ітеративного стилю статистичного аналізу: якщо не вдається вирішити проблему аналітично, просто напишіть алгоритм, який оцінить оптимальне рішення! Ба більше, як вже було згадано вище, параметричні та непараметричні тести працюють лише коли дані відповідають всім припущенням цих тестів. Якщо це не відповідає дійсності, то ітеративні обчислення також можуть стати в нагоді, варто лише пам’ятати процедури тестування гіпотез.</p>
<p>Уявіть два угруповання, <span class="math inline">\(A\)</span> і <span class="math inline">\(B\)</span>. Кожне угруповання описується як кількість особин певного виду (обмежимо <span class="math inline">\(\gamma\)</span>-різноманіття, тобто сумарне різноманіття між угрупованнями, до десяти видів), що для цієї ілюстрації ми згенеруємо випадково із розподілу Пуасона.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="3.5-basic-hypotheses.html#cb24-1" tabindex="-1"></a>comA <span class="ot">&lt;-</span> <span class="fu">rpois</span>(<span class="dv">10</span>, <span class="dv">5</span>)</span>
<span id="cb24-2"><a href="3.5-basic-hypotheses.html#cb24-2" tabindex="-1"></a>comB <span class="ot">&lt;-</span> <span class="fu">rpois</span>(<span class="dv">10</span>, <span class="dv">5</span>)</span></code></pre></div>
<p>Якщо позначити кожен вид, то ми можемо поглянути на ці два угрупованні як на таблицю:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="3.5-basic-hypotheses.html#cb25-1" tabindex="-1"></a>coms <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">A =</span> comA, <span class="at">B =</span> comB, <span class="at">row.names =</span> letters[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>])</span>
<span id="cb25-2"><a href="3.5-basic-hypotheses.html#cb25-2" tabindex="-1"></a>coms</span></code></pre></div>
<pre><code>##    A B
## a  5 4
## b 10 4
## c  6 4
## d  7 3
## e  5 7
## f  3 3
## g  7 4
## h  9 2
## i  4 4
## j  3 3</code></pre>
<p>І от питання: наскільки ці угруповання подібні між собою? Для відповіді є цілий набір різноманітних <a href="7.4-similarity.html#similarity">індексів подібності</a>, однак в цьому прикладі можемо обмежитись доволі стародавньою метрикою – Евклідовою відстанню. Зі школи можна пригадати, що квадрат гіпотенузи дорівнює сумі квадратів катетів. Якщо задуматись, то довжина гіпотенузи є дистанцією між двома кутами прямокутного трикутника, координати яких становлять <span class="math inline">\((x = \text{довжина катету 2}, y = 0)\)</span> та <span class="math inline">\((x = 0, y = \text{довжина катету 1})\)</span>. Відповідно, цю теорему Піфагора можна використати для розрахунку дистанції між двома точками <span class="math inline">\(p\)</span> і <span class="math inline">\(q\)</span> в двовимірному просторі із координатами <span class="math inline">\((p_1, p_2)\)</span> та <span class="math inline">\((q_1, q_2)\)</span>: <span class="math inline">\(d_{p, q}= \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}\)</span>. Краса Евклідової дистанції в тому, що її формулу можна екстраполювати на будь-яку кількість вимірів (позначимо вимірність як <span class="math inline">\(n: i = 1, 2, 3, \cdots, n\)</span>): <span class="math inline">\(d(p, q) = \sqrt{\sum_{i=1}^n (p_i - q_i)^2}\)</span>. Евклідова дистанція вказує на те, наскільки два об’єкти близькі один до одного в просторі, і якщо уявити угруповання такими об’єктами, то Евклідову дистанцію можна спробувати використати для оцінки того, наскільки два угруповання подібні один до одного<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a>.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="3.5-basic-hypotheses.html#cb27-1" tabindex="-1"></a>eucl_dist <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y){</span>
<span id="cb27-2"><a href="3.5-basic-hypotheses.html#cb27-2" tabindex="-1"></a>  <span class="fu">sqrt</span>(<span class="fu">sum</span>((x <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb27-3"><a href="3.5-basic-hypotheses.html#cb27-3" tabindex="-1"></a>}</span>
<span id="cb27-4"><a href="3.5-basic-hypotheses.html#cb27-4" tabindex="-1"></a>d_obs <span class="ot">&lt;-</span> <span class="fu">eucl_dist</span>(coms<span class="sc">$</span>A, coms<span class="sc">$</span>B)</span>
<span id="cb27-5"><a href="3.5-basic-hypotheses.html#cb27-5" tabindex="-1"></a>d_obs</span></code></pre></div>
<pre><code>## [1] 10.90871</code></pre>
<p>І отже ми отримуємо якесь значення дистанції – нашу статистику. Тепер постає інше питання, що це значення значить? Чи наші угруповання подібні між собою, чи ні? Це є гарним питанням для застосування статистичного тестування, в якому <em>нульовою гіпотезою</em> є те, що два угруповання походять із однієї генеральної сукупності (метаугруповання – множини угруповань), а <em>альтернативна гіпотеза</em> – що два угруповання походять із різних метаугруповань. Для тестування нульової гіпотези не залишається нічого, окрім як отримати нульовий розподіл для наших даних.</p>
<p>Можливо, існує спосіб аналітично знайти розподіл очікуваних Евклідових дистанцій для вибірок із однієї генеральної сукупності, однак вирішення такої задачі вимагатиме чимало часу і ми не знаємо чи це взагалі можливо. Тут у нагоді стає метод <strong>бутстреп</strong> (<em>bootstrap</em>), який дозволяє оцінити нульовий розподіл на підставі наявних даних. Симулювати нульову гіпотезу нескладно: для цього лише необхідно перемішати чисельності видів в спостережених угрупованнях. В цій конкретній ситуації, втім, постає питання адекватних нульових розподілів, адже за нульової гіпотези чисельності видів можуть бути маніпульовані тільки для кожного виду окремо (тобто не буде коректним замінити чисельність виду a в угруповання А чисельністю виду j із угруповання В, адже чисельності видів не є незалежними; втім, це не було би проблемою для незалежних змінних на кшталт результатів морфологічних промірів випадкових особин). Відтак, визначмо функцію, яка перемішуватиме чисельності видів таким чином, що симульована чисельність виду в угрупованні може рівноймовірно походити з угруповання А чи В. Варто зазначити, що для бутстрепу необхідно використовувати процедуру відбору із заміщенням (<em>sampling with replacement</em>)<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a>.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="3.5-basic-hypotheses.html#cb29-1" tabindex="-1"></a><span class="co"># для всяких пермутацій необхідно повернути випадкове зерно до замовчування</span></span>
<span id="cb29-2"><a href="3.5-basic-hypotheses.html#cb29-2" tabindex="-1"></a><span class="fu">rm</span>(.Random.seed, <span class="at">envir=</span><span class="fu">globalenv</span>())</span>
<span id="cb29-3"><a href="3.5-basic-hypotheses.html#cb29-3" tabindex="-1"></a></span>
<span id="cb29-4"><a href="3.5-basic-hypotheses.html#cb29-4" tabindex="-1"></a>mix_coms <span class="ot">&lt;-</span> <span class="cf">function</span>(com){</span>
<span id="cb29-5"><a href="3.5-basic-hypotheses.html#cb29-5" tabindex="-1"></a>  out <span class="ot">&lt;-</span> <span class="fu">apply</span>(com, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">sample</span>(<span class="at">x =</span> x, <span class="at">size =</span> <span class="dv">2</span>, <span class="at">replace =</span> T)) <span class="sc">%&gt;%</span> <span class="fu">t</span>() <span class="sc">%&gt;%</span> <span class="fu">as.data.frame</span>()</span>
<span id="cb29-6"><a href="3.5-basic-hypotheses.html#cb29-6" tabindex="-1"></a>  <span class="fu">colnames</span>(out) <span class="ot">&lt;-</span> <span class="fu">colnames</span>(com)</span>
<span id="cb29-7"><a href="3.5-basic-hypotheses.html#cb29-7" tabindex="-1"></a>  <span class="fu">return</span>(out)</span>
<span id="cb29-8"><a href="3.5-basic-hypotheses.html#cb29-8" tabindex="-1"></a>}</span>
<span id="cb29-9"><a href="3.5-basic-hypotheses.html#cb29-9" tabindex="-1"></a>new_coms <span class="ot">&lt;-</span> <span class="fu">mix_coms</span>(coms)</span>
<span id="cb29-10"><a href="3.5-basic-hypotheses.html#cb29-10" tabindex="-1"></a>new_coms</span></code></pre></div>
<pre><code>##    A  B
## a  4  4
## b 10 10
## c  6  6
## d  7  7
## e  5  5
## f  3  3
## g  4  7
## h  2  9
## i  4  4
## j  3  3</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="3.5-basic-hypotheses.html#cb31-1" tabindex="-1"></a><span class="fu">eucl_dist</span>(new_coms<span class="sc">$</span>A, new_coms<span class="sc">$</span>B)</span></code></pre></div>
<pre><code>## [1] 7.615773</code></pre>
<p>Тепер цю операцію можна повторити багато разів, скажімо, десять тисяч разів, аби згенерувати розподіл статистики.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="3.5-basic-hypotheses.html#cb33-1" tabindex="-1"></a>d_null <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">10000</span>)</span>
<span id="cb33-2"><a href="3.5-basic-hypotheses.html#cb33-2" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10000</span>){</span>
<span id="cb33-3"><a href="3.5-basic-hypotheses.html#cb33-3" tabindex="-1"></a>  new_coms <span class="ot">&lt;-</span> <span class="fu">mix_coms</span>(coms)</span>
<span id="cb33-4"><a href="3.5-basic-hypotheses.html#cb33-4" tabindex="-1"></a>  d_null[i] <span class="ot">&lt;-</span> <span class="fu">eucl_dist</span>(new_coms<span class="sc">$</span>A, new_coms<span class="sc">$</span>B)</span>
<span id="cb33-5"><a href="3.5-basic-hypotheses.html#cb33-5" tabindex="-1"></a>}</span></code></pre></div>
<p>Погляньмо на розподіл цієї статистики (чорна лінія відповідає ядерній оцінці густини розподілу) і порівняймо його із спостереженим значенням (червона лінія).</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="3.5-basic-hypotheses.html#cb34-1" tabindex="-1"></a><span class="fu">density</span>(d_null) <span class="sc">%&gt;%</span> <span class="fu">plot</span>(<span class="at">xlab =</span> <span class="st">&quot;Евклідова відстань&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;KDE&quot;</span>, <span class="at">main =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb34-2"><a href="3.5-basic-hypotheses.html#cb34-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> d_obs, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Оскільки за нульової гіпотези можна очікувати, що значення відстані наближатиметься до нуля (до того ж, дистанція не може бути негативною), екстремальні значення нульового розподілу відповідатимуть значним позитивним значенням. Ця логіка дозволяє оцінити псевдо-<span class="math inline">\(p\)</span>-значення як для одностороннього тесту:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="3.5-basic-hypotheses.html#cb35-1" tabindex="-1"></a><span class="fu">length</span>(d_null[d_null <span class="sc">&gt;=</span> d_obs])<span class="sc">/</span><span class="fu">length</span>(d_null)</span></code></pre></div>
<pre><code>## [1] 0.0082</code></pre>
<p>Отже, в генерованому розподілі нульової вибірки, ймовірність отримати спостережене або більше за спостережене значення статистики <span class="math inline">\(p \approx 0.01\)</span> (оскільки процес стохастичний, оцінка відрізнятиметься за кожного компілювання), що дозволяє відхилити нульову гіпотезу і стверджувати що два угруповання відрізняються між собою.</p>
<p>Пермутаційні методи є доволі пластичними, адже їх можна застосовувати для будь-яких статистик і будь-яких розподілів вихідних даних. Великим недоліком, втім, є те, що цей метод є комп’ютер-інтенсивним, і для великих наборів даних обчислення вимагатимуть чимало комп’ютерного часу (секунди, хвилини, іноді дні).</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="31">
<li id="fn31"><p>Нульовий розподіл може бути згенеровано в межах параметричного тесту (наприклад, як t-розподіл в t-тесті Стьюдента) або пермутаційно.<a href="3.5-basic-hypotheses.html#fnref31" class="footnote-back">↩︎</a></p></li>
<li id="fn32"><p>Наприклад, для порівняння середніх двох вибірок хорошим вибором статистичного тесту є t-тест Стьюдента. Однак якщо ваші дані не розподілені нормально, вам можуть порадити тест Вілкоксона (Wilcoxon signed-rank test) або тест Манна-Вітні (Mann–Whitney U test). Мало хто знає, втім, що ці тести дуже чутливі до власних припущень: тест Вілкоксона працює лише для незалежних пар залежних (в межах пар) значень, а тест Манна-Вітні передбачає <em>ідентичні</em> розподіли між двома групами за нульової гіпотези.<a href="3.5-basic-hypotheses.html#fnref32" class="footnote-back">↩︎</a></p></li>
<li id="fn33"><p>Евклідова дистанція не є найбільш поширеним показником подібності угруповань, але в цьому контексті є зручною для прикладу.<a href="3.5-basic-hypotheses.html#fnref33" class="footnote-back">↩︎</a></p></li>
<li id="fn34"><p>Якщо із вибірки обрати певний елемент, після чого цей елемент не можна обрати ще раз, такий відбір називається відбором без заміщення (<em>sampling without replacement</em>). Наприклад, ви намагаєтесь оцінити розподіл кольорів в кошику із кольоровими кульками: у відборі без заміщення кожну кульку дістати можна тільки раз. На противагу, якщо ви дістаєте одну кульку, записуєте її колір, і кладете назад у кошик, це є прикладом відбору із заміщенням. Очевидно, розмір вибірки не може бути більшим за розмір вихідної вибірки за відбору без заміщення, однак може бути більшим за відбору із заміщенням.<a href="3.5-basic-hypotheses.html#fnref34" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3.4-pdf-pmf.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3.6-stat-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/index.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
