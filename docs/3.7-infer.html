<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.7 Передбачення, умовивід, та валідація | Вступ до Екології Угруповань</title>
  <meta name="description" content="Непідручник" />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="3.7 Передбачення, умовивід, та валідація | Вступ до Екології Угруповань" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Непідручник" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.7 Передбачення, умовивід, та валідація | Вступ до Екології Угруповань" />
  
  <meta name="twitter:description" content="Непідручник" />
  

<meta name="author" content="Олексій Дубовик" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="3.6-stat-models.html"/>
<link rel="next" href="4-popeco.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Вступ до Екології Угруповань</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Вітання</a></li>
<li class="chapter" data-level="" data-path="передмова.html"><a href="передмова.html"><i class="fa fa-check"></i>Передмова</a>
<ul>
<li class="chapter" data-level="0.0.1" data-path="передмова.html"><a href="передмова.html#about-author"><i class="fa fa-check"></i><b>0.0.1</b> Трішки про автора</a></li>
<li class="chapter" data-level="0.0.2" data-path="передмова.html"><a href="передмова.html#whythiswork"><i class="fa fa-check"></i><b>0.0.2</b> Навіщо ця робота</a></li>
<li class="chapter" data-level="0.0.3" data-path="передмова.html"><a href="передмова.html#more-about-author"><i class="fa fa-check"></i><b>0.0.3</b> Ще трішки про автора</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="подяки.html"><a href="подяки.html"><i class="fa fa-check"></i>Подяки</a></li>
<li class="chapter" data-level="1" data-path="1-introduction.html"><a href="1-introduction.html"><i class="fa fa-check"></i><b>1</b> Вступ</a>
<ul>
<li class="chapter" data-level="1.0.1" data-path="1-introduction.html"><a href="1-introduction.html#community-def"><i class="fa fa-check"></i><b>1.0.1</b> Екологічне угруповання</a></li>
<li class="chapter" data-level="1.0.2" data-path="1-introduction.html"><a href="1-introduction.html#comm-ecol-today"><i class="fa fa-check"></i><b>1.0.2</b> Екологія угруповань сьогодні</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-about-book.html"><a href="2-about-book.html"><i class="fa fa-check"></i><b>2</b> Про книгу та Зміст</a>
<ul>
<li class="chapter" data-level="2.0.1" data-path="2-about-book.html"><a href="2-about-book.html#readme"><i class="fa fa-check"></i><b>2.0.1</b> Дисклеймер</a></li>
<li class="chapter" data-level="2.0.2" data-path="2-about-book.html"><a href="2-about-book.html#how-built"><i class="fa fa-check"></i><b>2.0.2</b> Як побудована ця книга</a></li>
<li class="chapter" data-level="2.0.3" data-path="2-about-book.html"><a href="2-about-book.html#expect"><i class="fa fa-check"></i><b>2.0.3</b> Чого чекати від цієї книги</a></li>
<li class="chapter" data-level="2.0.4" data-path="2-about-book.html"><a href="2-about-book.html#expected"><i class="fa fa-check"></i><b>2.0.4</b> Чого ця книга чекає від читача</a></li>
<li class="chapter" data-level="2.0.5" data-path="2-about-book.html"><a href="2-about-book.html#notexpect"><i class="fa fa-check"></i><b>2.0.5</b> На що не варто розраховувати</a></li>
<li class="chapter" data-level="2.1" data-path="2.1-зміст.html"><a href="2.1-зміст.html"><i class="fa fa-check"></i><b>2.1</b> Зміст</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numerical-ecology.html"><a href="3-numerical-ecology.html"><i class="fa fa-check"></i><b>3</b> Базові математичні підходи в екології</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3.1-algebra.html"><a href="3.1-algebra.html"><i class="fa fa-check"></i><b>3.1</b> Математична пам’ятка</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="3.1-algebra.html"><a href="3.1-algebra.html#дроби"><i class="fa fa-check"></i><b>3.1.1</b> Дроби</a></li>
<li class="chapter" data-level="3.1.2" data-path="3.1-algebra.html"><a href="3.1-algebra.html#математичні-символи"><i class="fa fa-check"></i><b>3.1.2</b> Математичні символи</a></li>
<li class="chapter" data-level="3.1.3" data-path="3.1-algebra.html"><a href="3.1-algebra.html#нерівності"><i class="fa fa-check"></i><b>3.1.3</b> Нерівності</a></li>
<li class="chapter" data-level="3.1.4" data-path="3.1-algebra.html"><a href="3.1-algebra.html#ступені"><i class="fa fa-check"></i><b>3.1.4</b> Ступені</a></li>
<li class="chapter" data-level="3.1.5" data-path="3.1-algebra.html"><a href="3.1-algebra.html#ряди-чисел"><i class="fa fa-check"></i><b>3.1.5</b> Ряди чисел</a></li>
<li class="chapter" data-level="3.1.6" data-path="3.1-algebra.html"><a href="3.1-algebra.html#ступені-арифметичних-операцій"><i class="fa fa-check"></i><b>3.1.6</b> Ступені арифметичних операцій</a></li>
<li class="chapter" data-level="3.1.7" data-path="3.1-algebra.html"><a href="3.1-algebra.html#лінійні-та-поліноміальні-функції"><i class="fa fa-check"></i><b>3.1.7</b> Лінійні та поліноміальні функції</a></li>
<li class="chapter" data-level="3.1.8" data-path="3.1-algebra.html"><a href="3.1-algebra.html#logs"><i class="fa fa-check"></i><b>3.1.8</b> Логарифми</a></li>
<li class="chapter" data-level="3.1.9" data-path="3.1-algebra.html"><a href="3.1-algebra.html#поширені-математичні-функції"><i class="fa fa-check"></i><b>3.1.9</b> Поширені математичні функції</a></li>
<li class="chapter" data-level="3.1.10" data-path="3.1-algebra.html"><a href="3.1-algebra.html#властивості-сум"><i class="fa fa-check"></i><b>3.1.10</b> Властивості сум</a></li>
<li class="chapter" data-level="3.1.11" data-path="3.1-algebra.html"><a href="3.1-algebra.html#властивості-добутків"><i class="fa fa-check"></i><b>3.1.11</b> Властивості добутків</a></li>
<li class="chapter" data-level="3.1.12" data-path="3.1-algebra.html"><a href="3.1-algebra.html#диференціювання"><i class="fa fa-check"></i><b>3.1.12</b> Диференціювання</a></li>
<li class="chapter" data-level="3.1.13" data-path="3.1-algebra.html"><a href="3.1-algebra.html#інтегрування"><i class="fa fa-check"></i><b>3.1.13</b> Інтегрування</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3.2-matrices.html"><a href="3.2-matrices.html"><i class="fa fa-check"></i><b>3.2</b> Лінійна алгебра</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-matrices.html"><a href="3.2-matrices.html#визначення-матриці"><i class="fa fa-check"></i><b>3.2.1</b> Визначення матриці</a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-matrices.html"><a href="3.2-matrices.html#трансформації-матриць"><i class="fa fa-check"></i><b>3.2.2</b> Трансформації матриць</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-matrices.html"><a href="3.2-matrices.html#операції-над-матрицями"><i class="fa fa-check"></i><b>3.2.3</b> Операції над матрицями</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-matrices.html"><a href="3.2-matrices.html#детермінант-власні-вектори-та-власне-значення"><i class="fa fa-check"></i><b>3.2.4</b> Детермінант, власні вектори, та власне значення</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-matrices.html"><a href="3.2-matrices.html#matrices_art"><i class="fa fa-check"></i><b>3.2.5</b> Геометричний зміст матриць</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-stats.html"><a href="3.3-stats.html"><i class="fa fa-check"></i><b>3.3</b> Ймовірність у статистиці</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-stats.html"><a href="3.3-stats.html#prob"><i class="fa fa-check"></i><b>3.3.1</b> Ймовірність</a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-stats.html"><a href="3.3-stats.html#bayes"><i class="fa fa-check"></i><b>3.3.2</b> Теорема Баєса</a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-stats.html"><a href="3.3-stats.html#mle"><i class="fa fa-check"></i><b>3.3.3</b> Правдоподібність</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-pdf-pmf.html"><a href="3.4-pdf-pmf.html"><i class="fa fa-check"></i><b>3.4</b> Розподіли ймовірності</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-pdf-pmf.html"><a href="3.4-pdf-pmf.html#pdfs"><i class="fa fa-check"></i><b>3.4.1</b> Функції розподілу ймовірності</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-pdf-pmf.html"><a href="3.4-pdf-pmf.html#bars"><i class="fa fa-check"></i><b>3.4.2</b> Опис розподілу змінної (описова статистика)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-basic-hypotheses.html"><a href="3.5-basic-hypotheses.html"><i class="fa fa-check"></i><b>3.5</b> Тестування гіпотез</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-basic-hypotheses.html"><a href="3.5-basic-hypotheses.html#hypothesis"><i class="fa fa-check"></i><b>3.5.1</b> Статистична гіпотеза</a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-basic-hypotheses.html"><a href="3.5-basic-hypotheses.html#nulldistr"><i class="fa fa-check"></i><b>3.5.2</b> Нульовий розподіл</a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-basic-hypotheses.html"><a href="3.5-basic-hypotheses.html#pval"><i class="fa fa-check"></i><b>3.5.3</b> Тестування гіпотез</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-basic-hypotheses.html"><a href="3.5-basic-hypotheses.html#paradigms"><i class="fa fa-check"></i><b>3.5.4</b> Парадигми статистичного аналізу</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-stat-models.html"><a href="3.6-stat-models.html"><i class="fa fa-check"></i><b>3.6</b> Експеримент і модель</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="3.6-stat-models.html"><a href="3.6-stat-models.html#pseudoreplication"><i class="fa fa-check"></i><b>3.6.1</b> Експериментальний дизайн та псевдореплікація</a></li>
<li class="chapter" data-level="3.6.2" data-path="3.6-stat-models.html"><a href="3.6-stat-models.html#regression"><i class="fa fa-check"></i><b>3.6.2</b> Дані та проблема моделювання</a></li>
<li class="chapter" data-level="3.6.3" data-path="3.6-stat-models.html"><a href="3.6-stat-models.html#aic"><i class="fa fa-check"></i><b>3.6.3</b> Парсимонійна модель та вибір моделі</a></li>
<li class="chapter" data-level="3.6.4" data-path="3.6-stat-models.html"><a href="3.6-stat-models.html#prcomp"><i class="fa fa-check"></i><b>3.6.4</b> Багатовимірна статистика</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="3.7-infer.html"><a href="3.7-infer.html"><i class="fa fa-check"></i><b>3.7</b> Передбачення, умовивід, та валідація</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-infer.html"><a href="3.7-infer.html#inference"><i class="fa fa-check"></i><b>3.7.1</b> Статистичний умовивід і обґрунтоване передбачення</a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-infer.html"><a href="3.7-infer.html#crossval"><i class="fa fa-check"></i><b>3.7.2</b> Крос-валідація</a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-infer.html"><a href="3.7-infer.html#bias-variance"><i class="fa fa-check"></i><b>3.7.3</b> Компроміс між упередженням та варіацією</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-popeco.html"><a href="4-popeco.html"><i class="fa fa-check"></i><b>4</b> Початки популяційної екології</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4.1-species.html"><a href="4.1-species.html"><i class="fa fa-check"></i><b>4.1</b> Вид</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-species.html"><a href="4.1-species.html#поняття-виду"><i class="fa fa-check"></i><b>4.1.1</b> Поняття виду</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-species.html"><a href="4.1-species.html#види-та-площа"><i class="fa fa-check"></i><b>4.1.2</b> Види та площа</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-population.html"><a href="4.2-population.html"><i class="fa fa-check"></i><b>4.2</b> Популяція</a></li>
<li class="chapter" data-level="4.3" data-path="4.3-metapopulation.html"><a href="4.3-metapopulation.html"><i class="fa fa-check"></i><b>4.3</b> Метапопуляція</a></li>
<li class="chapter" data-level="4.4" data-path="4.4-pop-factors.html"><a href="4.4-pop-factors.html"><i class="fa fa-check"></i><b>4.4</b> Чинники, що впливають на популяції</a></li>
<li class="chapter" data-level="4.5" data-path="4.5-intraspecific.html"><a href="4.5-intraspecific.html"><i class="fa fa-check"></i><b>4.5</b> Внутрішньовидові взаємодії</a></li>
<li class="chapter" data-level="4.6" data-path="4.6-pop-dynamics.html"><a href="4.6-pop-dynamics.html"><i class="fa fa-check"></i><b>4.6</b> Динаміка та стабільність</a></li>
<li class="chapter" data-level="4.7" data-path="4.7-Leslie-matrix.html"><a href="4.7-Leslie-matrix.html"><i class="fa fa-check"></i><b>4.7</b> Моделювання популяцій</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-foundations.html"><a href="5-foundations.html"><i class="fa fa-check"></i><b>5</b> Фундаментальні поняття екології</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5.1-niche.html"><a href="5.1-niche.html"><i class="fa fa-check"></i><b>5.1</b> Екологічна ніша</a></li>
<li class="chapter" data-level="5.2" data-path="5.2-food-chain.html"><a href="5.2-food-chain.html"><i class="fa fa-check"></i><b>5.2</b> Трофічні ланцюги</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-food-webs.html"><a href="5.3-food-webs.html"><i class="fa fa-check"></i><b>5.3</b> Трофічні мережі</a></li>
<li class="chapter" data-level="5.4" data-path="5.4-keystones.html"><a href="5.4-keystones.html"><i class="fa fa-check"></i><b>5.4</b> Ключові види</a></li>
<li class="chapter" data-level="5.5" data-path="5.5-succession.html"><a href="5.5-succession.html"><i class="fa fa-check"></i><b>5.5</b> Сукцесія та клімакс</a></li>
<li class="chapter" data-level="5.6" data-path="5.6-life-history.html"><a href="5.6-life-history.html"><i class="fa fa-check"></i><b>5.6</b> Історія життя</a></li>
<li class="chapter" data-level="5.7" data-path="5.7-detectability.html"><a href="5.7-detectability.html"><i class="fa fa-check"></i><b>5.7</b> Ймовірність детекції</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-interspecific.html"><a href="6-interspecific.html"><i class="fa fa-check"></i><b>6</b> Міжвидові взаємодії</a>
<ul>
<li class="chapter" data-level="6.1" data-path="6.1-relationships.html"><a href="6.1-relationships.html"><i class="fa fa-check"></i><b>6.1</b> Типи міжвидових зв’язків</a></li>
<li class="chapter" data-level="6.2" data-path="6.2-rstar.html"><a href="6.2-rstar.html"><i class="fa fa-check"></i><b>6.2</b> Теорія поділу ресурсів</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-cascade.html"><a href="6.3-cascade.html"><i class="fa fa-check"></i><b>6.3</b> Трофічні каскади</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-comecol.html"><a href="7-comecol.html"><i class="fa fa-check"></i><b>7</b> Екологічні угруповання</a>
<ul>
<li class="chapter" data-level="7.1" data-path="7.1-sad.html"><a href="7.1-sad.html"><i class="fa fa-check"></i><b>7.1</b> Структура угруповання</a></li>
<li class="chapter" data-level="7.2" data-path="7.2-sad-model.html"><a href="7.2-sad-model.html"><i class="fa fa-check"></i><b>7.2</b> Моделі розподілів чисельності</a></li>
<li class="chapter" data-level="7.3" data-path="7.3-diversity.html"><a href="7.3-diversity.html"><i class="fa fa-check"></i><b>7.3</b> Різноманіття</a></li>
<li class="chapter" data-level="7.4" data-path="7.4-similarity.html"><a href="7.4-similarity.html"><i class="fa fa-check"></i><b>7.4</b> Подібність угруповань</a></li>
<li class="chapter" data-level="7.5" data-path="7.5-fd.html"><a href="7.5-fd.html"><i class="fa fa-check"></i><b>7.5</b> Функціональне й філогенетичне різноманіття</a></li>
<li class="chapter" data-level="7.6" data-path="7.6-islands.html"><a href="7.6-islands.html"><i class="fa fa-check"></i><b>7.6</b> Острівна біогеографія</a></li>
<li class="chapter" data-level="7.7" data-path="7.7-env-filter.html"><a href="7.7-env-filter.html"><i class="fa fa-check"></i><b>7.7</b> Середовищне фільтрування</a></li>
<li class="chapter" data-level="7.8" data-path="7.8-untb.html"><a href="7.8-untb.html"><i class="fa fa-check"></i><b>7.8</b> Нейтральна теорія біорізноманіття</a></li>
<li class="chapter" data-level="7.9" data-path="7.9-rarefaction.html"><a href="7.9-rarefaction.html"><i class="fa fa-check"></i><b>7.9</b> Рарефакція та екстраполяція</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="післяслово.html"><a href="післяслово.html"><i class="fa fa-check"></i>Післяслово</a></li>
<li class="chapter" data-level="" data-path="контакти-автора.html"><a href="контакти-автора.html"><i class="fa fa-check"></i>Контакти автора</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Вступ до Екології Угруповань</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="infer" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Передбачення, умовивід, та валідація<a href="3.7-infer.html#infer" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote>
<p>“Всі моделі є хибними.</p>
<p>Але деякі з них ще й корисні.”</p>
<p>— за <a href="https://doi.org/10.1080%2F01621459.1976.10480949">Джорджем Боксом</a></p>
</blockquote>
<p>Здавалось би, гаразд, якщо у дослідника є базове розуміння математичних тем, усвідомлення поняття ймовірності, достатньо ґрунтовні навички використання наукового методу для тестування гіпотез, яке-не-яке розуміння статистичного аналізу, то із цим усім можна проводити доволі непогані дослідження в екології. Загалом, мабуть, воно так і є, однак ще до початку проведення аналізу (а, як так, то і до проведення експерименту) необхідно розуміти кінцеву мету дослідження — чи ми очікуємо відповідь на конкретне фундаментальне питання, чи результати матимуть більше прикладну роль для подальших розробок. Хоча ці два звірі і є двома боками боками однієї монети, у них доволі специфічні й відмінні присмаки. Цей розділ буде завершальним у цій математичній секції, і являтиме собою настанову, фінальними міркуваннями, про котрі варто іноді задуматись в аналізі даних: (1) що саме ми хочемо досягнути цим аналізом, і (2) чи проведений аналіз є притомним.</p>
<div id="inference" class="section level3 hasAnchor" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Статистичний умовивід і обґрунтоване передбачення<a href="3.7-infer.html#inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Всякий статистичний аналіз можна звести до ситуації, коли існує набір предикторів <span class="math inline">\(X\)</span>, певна залежна змінна <span class="math inline">\(Y\)</span>, і аналіз зводиться до пошуку такої функції <span class="math inline">\(f\)</span>, що <span class="math inline">\(Y = f(X) + \epsilon\)</span>, де <span class="math inline">\(\epsilon\)</span> позначає випадкову помилку, а функція <span class="math inline">\(f\)</span> може бути <a href="3.6-stat-models.html#regression">регресією, класифікатором</a>, чи якоюсь складною моделлю. Ми ніколи не дізнаємось справжній вигляд <span class="math inline">\(f\)</span> (бо це може бути дуже складною функцією), але інструменти статистичного аналізу дозволяють знайти якусь апроксимацію до <span class="math inline">\(f\)</span>, скажімо, <span class="math inline">\(\hat{f}\)</span>, що дозволятиме побудувати модель вигляду <span class="math inline">\(\hat{Y} = \hat{f}(X)\)</span>. Навіщо ж оцінювати <span class="math inline">\(\hat{f}\)</span>? Власне, оцінка <span class="math inline">\(\hat{f}\)</span> дозволяє нам робити <strong>передбачення</strong> (<em>prediction</em>) або <strong>умовивід</strong> (<em>inference</em>) (<a href="https://doi.org/10.1007/978-1-0716-1418-1_2">James et al. 2021</a>).</p>
<div id="передбачення" class="section level4 hasAnchor" number="3.7.1.1">
<h4><span class="header-section-number">3.7.1.1</span> Передбачення<a href="3.7-infer.html#передбачення" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Передбачення корисне в ситуаціях, коли дані щодо змінних в <span class="math inline">\(X\)</span> можна легко отримати у великих вибірках<a href="#fn41" class="footnote-ref" id="fnref41"><sup>41</sup></a>, однак збір даних щодо <span class="math inline">\(Y\)</span> вимагає інтенсивної польової роботи. Якщо є необхідність оцінити <span class="math inline">\(Y\)</span> опосередковано через <span class="math inline">\(X\)</span>, тоді можна скористатися моделлю <span class="math inline">\(\hat{Y} = \hat{f}(X)\)</span>. В такому випадку, сама модель <span class="math inline">\(\hat{f}\)</span> не є у фокусі нашої уваги – нехай це буде хоч <em>якась</em> модель, аби тільки <span class="math inline">\(\hat{Y}\)</span> приблизно відповідало дійсності.</p>
<p>Звісно, всяка модель є лише спрощенням складної реальності, і оцінене <span class="math inline">\(\hat{Y}\)</span> ніколи не дорівнюватиме невідомому та реальному <span class="math inline">\(Y\)</span>. Помилка передбачення (різниця між <span class="math inline">\(\hat{Y}\)</span> та <span class="math inline">\(Y\)</span>) складається із двох частин: по-перше, завжди існує <em>незменшувана помилка</em> (<em>irreducible error</em>), що походить із статистичного шуму в досліджуваній системі – та сама <span class="math inline">\(\epsilon\)</span>; по-друге, модель сама по собі може містити певні <a href="3.7-infer.html#bias-variance">упередження щодо даних</a>, що можна змінити модифікаціями моделі, й, відтак, ця помилка є <em>зменшуваною</em> (<em>reducible error</em>). Якщо уявити що вся варіація в даних походить від <span class="math inline">\(\epsilon\)</span>, то <a href="3.4-pdf-pmf.html#pdfs">математичне очікування</a> помилки передбачення дорівнює</p>
<p><span class="math display">\[\mathbb{E}[(Y - \hat{Y})^2] = \mathbb{E} [[f(X) + \epsilon - \hat{f}(X)]^2] = [f(X) - \hat{f}(X)]^2 + Var(\epsilon)\]</span></p>
<p>де <span class="math inline">\([f(X) - \hat{f}(X)]^2\)</span> відповідає зменшуваній помилці – усередненому очікуванню помилки передбачення, а <span class="math inline">\(Var(\epsilon)\)</span> – варіації незменшуваної помилки.</p>
<p>Варто зауважити, що оскільки проблема передбачення фокусується на <span class="math inline">\(\hat{Y}\)</span>, в той час як сама модель <span class="math inline">\(\hat{f}\)</span> є, радше, інструментом<a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a>, то використання статистичного аналізу для передбачення іноді дозволяє послабити вимоги до припущень методів. Наприклад, однією із залізних вимог множинної лінійної регресії є відсутність колінеарності між предикторами – тобто предиктори не мають мати кореляцію між собою. Регресійна модель із колінеарними предикторами не є адекватною, адже весь математичний апарат регресії ґрунтується на припущенні, що предиктори є незалежними<a href="#fn43" class="footnote-ref" id="fnref43"><sup>43</sup></a>. Іронія полягає в тому, що використання такої моделі є цілком прийнятним для передбачення якщо <a href="3.7-infer.html#crossval">її передбачення є точними</a> (знову ж, нам все одно <em>як</em> працює модель, аби тільки вона працювала), хоча її й не можна використовувати для умовиводу (див. приклад нижче).</p>
</div>
<div id="умовивід" class="section level4 hasAnchor" number="3.7.1.2">
<h4><span class="header-section-number">3.7.1.2</span> Умовивід<a href="3.7-infer.html#умовивід" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Як вже можна було здогадатись, якщо для моделі <span class="math inline">\(\hat{Y} = \hat{f}(X)\)</span> задача передбачення звертає найбільше уваги на результат моделі <span class="math inline">\(\hat{Y}\)</span>, то задача умовиводу більше дивиться на саму модель <span class="math inline">\(\hat{f}\)</span>. У фундаментальних дослідженнях дослідник часто питатиме щось на кшталт “а як впливає ця змінна на нашу систему?” або “чи ця змінна є важливою?” В таких ситуаціях до моделі не можна ставитись як до чорної скриньки, а навпаки, ми намагаємось розплутати взаємозв’язки в досліджуваній системі і з’ясувати що до чого шляхом тествання чітко сформульованих статистичних гіпотез. Очікувано, чим складніші математичні методи застосовуються в аналізі, тим складніше буває інтерпретація результатів. Відтак, для умовиводу краще застосовувати простіші методи (наприклад, лінійну регресію замість узагальнених додатніх моделей) – хоча всяку модель, яку можна використати для умовиводу, можна використати і для передбачення. Звісно, завжди варто мати на увазі що статистична значущість не тотожна біологічній значущості, отже, задача умовиводу в чистому вигляді не повинна всеціло покладатись на статистичний аналіз, а, радше, використовувати його для аргументації. Чистий умовивід тісно переплітається із філософією науки та базовими методами <a href="3.5-basic-hypotheses.html#pval">перевірки гіпотез</a>. На практиці ж, наукове дослідження балансуватиме між умовиводом та передбаченням, тож варто просто мати на увазі ці два кути погляду на статистичний аналіз.</p>
<p>На завершення, різницю між передбаченням і умовиводом із лінійною регресією можна проілюструвати простим прикладом із генерованими даними.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="3.7-infer.html#cb55-1" tabindex="-1"></a><span class="co"># створимо випадкову змінну із нормальним розподілом та 100 елементами</span></span>
<span id="cb55-2"><a href="3.7-infer.html#cb55-2" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb55-3"><a href="3.7-infer.html#cb55-3" tabindex="-1"></a></span>
<span id="cb55-4"><a href="3.7-infer.html#cb55-4" tabindex="-1"></a><span class="co"># функція для генерації випадкової змінної із визначеною кореляцією</span></span>
<span id="cb55-5"><a href="3.7-infer.html#cb55-5" tabindex="-1"></a><span class="co"># до вхідної змінної</span></span>
<span id="cb55-6"><a href="3.7-infer.html#cb55-6" tabindex="-1"></a>corvar <span class="ot">&lt;-</span> <span class="cf">function</span>(x, rho) {</span>
<span id="cb55-7"><a href="3.7-infer.html#cb55-7" tabindex="-1"></a>  orth <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">runif</span>(<span class="fu">length</span>(x)) <span class="sc">~</span> x)<span class="sc">$</span>residuals</span>
<span id="cb55-8"><a href="3.7-infer.html#cb55-8" tabindex="-1"></a>  rho<span class="sc">*</span><span class="fu">sd</span>(orth)<span class="sc">*</span>x <span class="sc">+</span> orth<span class="sc">*</span><span class="fu">sd</span>(x)<span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">-</span> rho<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb55-9"><a href="3.7-infer.html#cb55-9" tabindex="-1"></a>}</span>
<span id="cb55-10"><a href="3.7-infer.html#cb55-10" tabindex="-1"></a></span>
<span id="cb55-11"><a href="3.7-infer.html#cb55-11" tabindex="-1"></a><span class="co"># створимо другу випадкову змінну</span></span>
<span id="cb55-12"><a href="3.7-infer.html#cb55-12" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">corvar</span>(<span class="at">x =</span> x1, <span class="at">rho =</span> <span class="fl">0.99</span>)</span>
<span id="cb55-13"><a href="3.7-infer.html#cb55-13" tabindex="-1"></a></span>
<span id="cb55-14"><a href="3.7-infer.html#cb55-14" tabindex="-1"></a><span class="co"># перевіримо чи кореляція між х1 і х2 становить 0.99</span></span>
<span id="cb55-15"><a href="3.7-infer.html#cb55-15" tabindex="-1"></a><span class="fu">cor</span>(x1, x2)</span></code></pre></div>
<pre><code>## [1] 0.99</code></pre>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="3.7-infer.html#cb57-1" tabindex="-1"></a><span class="co"># згенеруємо залежну змінну як функцію х1 та х2 із рандомною помилкою</span></span>
<span id="cb57-2"><a href="3.7-infer.html#cb57-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">1</span><span class="sc">*</span>x1 <span class="sc">+</span> <span class="dv">1</span><span class="sc">*</span>x2 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="fu">length</span>(x1), <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fl">0.25</span>)</span>
<span id="cb57-3"><a href="3.7-infer.html#cb57-3" tabindex="-1"></a></span>
<span id="cb57-4"><a href="3.7-infer.html#cb57-4" tabindex="-1"></a><span class="co"># побудуймо множинну регресію</span></span>
<span id="cb57-5"><a href="3.7-infer.html#cb57-5" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2)</span>
<span id="cb57-6"><a href="3.7-infer.html#cb57-6" tabindex="-1"></a></span>
<span id="cb57-7"><a href="3.7-infer.html#cb57-7" tabindex="-1"></a><span class="co"># погляньмо на коефіцієнти регресії</span></span>
<span id="cb57-8"><a href="3.7-infer.html#cb57-8" tabindex="-1"></a><span class="co"># ми очікуємо на 1 для інтерцепту, 1 для х1, та 1 для х2</span></span>
<span id="cb57-9"><a href="3.7-infer.html#cb57-9" tabindex="-1"></a>fit <span class="sc">%&gt;%</span> <span class="fu">summary</span>() <span class="sc">%&gt;%</span> .<span class="sc">$</span>coefficients <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)    1.028      0.022  45.874    0.000
## x1             1.000      0.157   6.369    0.000
## x2             0.938      0.524   1.790    0.077</code></pre>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="3.7-infer.html#cb59-1" tabindex="-1"></a><span class="co"># як бачимо, оцінка параметрів неадекватна, </span></span>
<span id="cb59-2"><a href="3.7-infer.html#cb59-2" tabindex="-1"></a><span class="co"># цю модель не можна використовувати для умовиводу</span></span>
<span id="cb59-3"><a href="3.7-infer.html#cb59-3" tabindex="-1"></a></span>
<span id="cb59-4"><a href="3.7-infer.html#cb59-4" tabindex="-1"></a><span class="co"># наскільки передбачення моделі далекі від істини?</span></span>
<span id="cb59-5"><a href="3.7-infer.html#cb59-5" tabindex="-1"></a></span>
<span id="cb59-6"><a href="3.7-infer.html#cb59-6" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> y,</span>
<span id="cb59-7"><a href="3.7-infer.html#cb59-7" tabindex="-1"></a>     <span class="at">y =</span> fit<span class="sc">$</span>fitted.values,</span>
<span id="cb59-8"><a href="3.7-infer.html#cb59-8" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&#39;Згенеровані (&quot;реальні&quot;) значення Y&#39;</span>, <span class="at">ylab =</span> <span class="st">&quot;Передбачені значення Y&quot;</span>,</span>
<span id="cb59-9"><a href="3.7-infer.html#cb59-9" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb59-10"><a href="3.7-infer.html#cb59-10" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">1</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="3.7-infer.html#cb60-1" tabindex="-1"></a><span class="co"># виглядає, що модель передбачає Y доволі точно</span></span></code></pre></div>
</div>
</div>
<div id="crossval" class="section level3 hasAnchor" number="3.7.2">
<h3><span class="header-section-number">3.7.2</span> Крос-валідація<a href="3.7-infer.html#crossval" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Коли у дослідника є побудована модель (не важливо, чи для передбачення, чи для умовиводу), очевидним питанням є наскільки ця модель є адекватною. Звісно, конкретні методи мають певні метрики пристосованості (<em>fitness</em>) даних до моделі, як-то <span class="math inline">\(R^2\)</span> і <span class="math inline">\(R_{adj}^2\)</span> для лінійних моделей, що описує частку варіації в даних що пояснюється моделлю, не кажучи вже про <a href="3.6-stat-models.html#aic">інформаційні метрики</a>. Цікавим підходом до валідації моделі, що є набагато більш гнучким відносно різноманіття методів є <strong>крос-валідація</strong> (<em>cross-validation</em>).</p>
<p>Із прикладу в попередньому розділі можна побачити, що за наявності вхідних даних, робочої статистичної моделі, та передбачень моделі можна порівняти вихідну залежну змінну <span class="math inline">\(Y\)</span> та її апроксимацію, передбачену моделлю (<span class="math inline">\(\hat{Y}\)</span>). Для кожного спостереження можна оцінити, наскільки далеко передбачення моделі від реальності: <span class="math inline">\((y_i - \hat{y_i})^2\)</span>, а відтак можна і додати відхилення в межах усієї моделі у певну статистику, що зветься <strong>середнім квадратом відхилень</strong> (<em>mean sqared error</em>, <strong><em>MSE</em></strong>):</p>
<p><span class="math display">\[\frac{1}{n} \sum_{i = 1}^n ( y_i - \hat{y_i} )^2\]</span></p>
<p>Звісно, для одного набору даних і однієї моделі можна розрахувати MSE тільки раз, та й це не надто має сенс, адже якщо в даних вже є відома залежна змінна, то навіщо її передбачати? Однак, пригадаймо <a href="3.5-basic-hypotheses.html#permutation-paradigm">пермутаційні методи</a> – якщо є лише одна вибірка, то із неї все одно можна згенерувати багато значень статистики. Подібний принцип працює і із крос-валідацією: чому б не розділити вхідний набір даних на два випадкові набори? Тоді можна побудувати модель на одному наборі (<em>тренувальні дані</em>), спробувати передбачити залежну змінну в іншому наборі (<em>валідаційні дані</em>), і розрахувати MSE для передбачених значень у валідаційному наборі. Оскільки поділ вхідних даних на тренувальний та валідаційний набори є довільним, цю процедуру можна повторювати багаторазово із випадковим розподілом спостережень між наборами.</p>
<p>Існує чимало алгоритмів крос-валідації, із яких популярними є два:</p>
<ul>
<li><p><strong>крос-валідація із виключенням по одному</strong> (<em>leave-one-out cross-validation</em>, <strong><em>LOOCV</em></strong>):</p>
<ol style="list-style-type: decimal">
<li><p>для кожного спостереження <span class="math inline">\(i\)</span> у наборі даних із <span class="math inline">\(n\)</span> спостережень, побудуймо окрему модель (<span class="math inline">\(\hat{f_i}\)</span>) на тренувальних даних, із яких видалено <span class="math inline">\(i\)</span>-те спостереження (розмір тренувального набору становитиме <span class="math inline">\(n-1\)</span>);</p></li>
<li><p>використаймо модель <span class="math inline">\(\hat{f_i}\)</span> на валідаційному спостереженні <span class="math inline">\(x_i\)</span> для передбачення значення залежної змінної <span class="math inline">\(\hat{y_i}\)</span>, розрахуймо квадрат відхилення <span class="math inline">\((y_i - \hat{y_i})^2\)</span>;</p></li>
<li><p>повторимо для всіх <span class="math inline">\(i = 1, 2, \cdots, n\)</span>, і розрахуймо MSE як <span class="math inline">\(\frac{1}{n} \sum_{i = 1}^n ( y_i - \hat{y_i} )^2\)</span>.</p></li>
</ol></li>
<li><p><strong>k-кратна крос-валідація</strong> (<em>k-fold cross-validation</em>, <strong><em>k-fold CV</em></strong>):</p>
<ol style="list-style-type: decimal">
<li><p>розділимо вхідний набір даних на <span class="math inline">\(k\)</span> випадкових набори приблизно однакового розміру (<span class="math inline">\(\approx n/k\)</span>): <span class="math inline">\(T_1, T_2, \cdots, T_k\)</span>;</p></li>
<li><p>для кожного <span class="math inline">\(i = 1, 2, \cdots, k\)</span>, використаймо всі набори окрім <span class="math inline">\(i\)</span>-того в якості тренувальних даних для моделі <span class="math inline">\(\hat{f_i}\)</span>;</p></li>
<li><p>використаймо модель <span class="math inline">\(\hat{f_i}\)</span> на валідаційному наборі <span class="math inline">\(T_i\)</span> для передбачення значень залежної змінної <span class="math inline">\(\hat{Y_i}\)</span>, розрахуймо MSE;</p></li>
<li><p>повторення процедури для всіх <span class="math inline">\(T_1, T_2, \cdots, T_k\)</span> генерує <span class="math inline">\(k\)</span> оцінок MSE, які можна усереднити як <span class="math inline">\(\frac{1}{k}\sum_{i=1}^k \text{MSE}_i\)</span>.</p></li>
</ol></li>
</ul>
<p>Можна легко уявити, що LOOCV є окремим випадком k-fold CV, в якому <span class="math inline">\(k=n\)</span>.</p>
<p>Процедура крос-валідації корисна для моделей із гнучкими параметрами, котрі обираються довільно, і оптимальне значення параметру залежить від даних. Наприклад, в алгоритмі <a href="3.6-stat-models.html#classifier">k-найближчих сусідів (KNN)</a> можна використати будь-яке значення <span class="math inline">\(k\)</span>, однак яке із них найкраще? Для крос-валідації в R існують спеціальні бібліотеки (наприклад, <a href="https://rpubs.com/njvijay/16444"><code>caret::train()</code></a>), однак, залежно від ситуації (наприклад, використання незвичної родини моделей), іноді простіше буває писати алгоритм самостійно.</p>
</div>
<div id="bias-variance" class="section level3 hasAnchor" number="3.7.3">
<h3><span class="header-section-number">3.7.3</span> Компроміс між упередженням та варіацією<a href="3.7-infer.html#bias-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>На завершення, із параметризацією моделей завжди варто бути обережними, бо із цим дуже легко перестаратись. Наприклад, із використанням поліномільної регресії буває нескладно побудувати таку криву, яка майже точно проходить через всі точки. Наприклад, згенеруймо хмару випадкових точок і побудуймо поліноміальні моделі різної складності:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="3.7-infer.html#cb61-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">11</span>)</span>
<span id="cb61-2"><a href="3.7-infer.html#cb61-2" tabindex="-1"></a></span>
<span id="cb61-3"><a href="3.7-infer.html#cb61-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">20</span>)</span>
<span id="cb61-4"><a href="3.7-infer.html#cb61-4" tabindex="-1"></a>y <span class="ot">&lt;-</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x), <span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb61-5"><a href="3.7-infer.html#cb61-5" tabindex="-1"></a></span>
<span id="cb61-6"><a href="3.7-infer.html#cb61-6" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> x, <span class="at">y =</span> y) <span class="sc">%&gt;%</span></span>
<span id="cb61-7"><a href="3.7-infer.html#cb61-7" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb61-8"><a href="3.7-infer.html#cb61-8" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">formula =</span> <span class="st">&quot;y ~ poly(x, 1)&quot;</span>, <span class="at">se =</span> F,</span>
<span id="cb61-9"><a href="3.7-infer.html#cb61-9" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> <span class="st">&quot;p = 1&quot;</span>), <span class="at">show_guide =</span> T) <span class="sc">+</span></span>
<span id="cb61-10"><a href="3.7-infer.html#cb61-10" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">formula =</span> <span class="st">&quot;y ~ poly(x, 2)&quot;</span>, <span class="at">se =</span> F, </span>
<span id="cb61-11"><a href="3.7-infer.html#cb61-11" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> <span class="st">&quot;p = 2&quot;</span>), <span class="at">show_guide =</span> T) <span class="sc">+</span></span>
<span id="cb61-12"><a href="3.7-infer.html#cb61-12" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">formula =</span> <span class="st">&quot;y ~ poly(x, 5)&quot;</span>, <span class="at">se =</span> F, </span>
<span id="cb61-13"><a href="3.7-infer.html#cb61-13" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> <span class="st">&quot;p = 5&quot;</span>), <span class="at">show_guide =</span> T) <span class="sc">+</span></span>
<span id="cb61-14"><a href="3.7-infer.html#cb61-14" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">formula =</span> <span class="st">&quot;y ~ poly(x, 10)&quot;</span>, <span class="at">se =</span> F, </span>
<span id="cb61-15"><a href="3.7-infer.html#cb61-15" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> <span class="st">&quot;p = 9&quot;</span>), <span class="at">show_guide =</span> T) <span class="sc">+</span></span>
<span id="cb61-16"><a href="3.7-infer.html#cb61-16" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">name =</span> <span class="st">&quot;Порядок&quot;</span>,</span>
<span id="cb61-17"><a href="3.7-infer.html#cb61-17" tabindex="-1"></a>                     <span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;p = 1&quot;</span> <span class="ot">=</span> <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;p = 2&quot;</span> <span class="ot">=</span> <span class="st">&quot;darkblue&quot;</span>, </span>
<span id="cb61-18"><a href="3.7-infer.html#cb61-18" tabindex="-1"></a>                                <span class="st">&quot;p = 5&quot;</span> <span class="ot">=</span> <span class="st">&quot;red&quot;</span>, <span class="st">&quot;p = 9&quot;</span> <span class="ot">=</span> <span class="st">&quot;darkred&quot;</span>)) <span class="sc">+</span></span>
<span id="cb61-19"><a href="3.7-infer.html#cb61-19" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Очевидно, що поліноміальна регресія 9-го порядку набагато краще описує дані, аніж першого. З іншого боку, модель всього лише описує позитивний зв’язок змінних <span class="math inline">\(x\)</span> та <span class="math inline">\(y\)</span>, і дані мали би описуватись як <span class="math inline">\(y = 1 + 1 x + \epsilon\)</span>, а не як cкладний поліном на кшталт</p>
<p><span class="math display">\[y = -0.5 + 4x + 0.6x^2 - 0.6x^3 -0.2x^4 -0.1x^5 - 0.2x^6 - 0.1x^7 + 0.2x^8 + 0.8x^9 + \epsilon\]</span></p>
<p>Ці коефіцієнти можна знайти у відповідній моделі:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="3.7-infer.html#cb62-1" tabindex="-1"></a>fit9 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">9</span>))</span>
<span id="cb62-2"><a href="3.7-infer.html#cb62-2" tabindex="-1"></a>fit9<span class="sc">$</span>coefficients <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="dv">1</span>)</span></code></pre></div>
<pre><code>## (Intercept) poly(x, 9)1 poly(x, 9)2 poly(x, 9)3 poly(x, 9)4 poly(x, 9)5 
##        -0.5         4.0         0.6        -0.6        -0.2        -0.1 
## poly(x, 9)6 poly(x, 9)7 poly(x, 9)8 poly(x, 9)9 
##        -0.2        -0.1         0.2         0.8</code></pre>
<p>Тож наскільки вдалою є складна поліноміальна модель для опису цих даних? Для власне <em>цього</em> набору даних – дуже вдалою, але проблема в тому, що складна модель підходить тільки для <em>цих</em> даних, і якщо ми згенеруємо інший випадковий набір даних, то попередня модель раптом стане дуже невдалою:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="3.7-infer.html#cb64-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12</span>)</span>
<span id="cb64-2"><a href="3.7-infer.html#cb64-2" tabindex="-1"></a></span>
<span id="cb64-3"><a href="3.7-infer.html#cb64-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">20</span>)</span>
<span id="cb64-4"><a href="3.7-infer.html#cb64-4" tabindex="-1"></a>y <span class="ot">&lt;-</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x), <span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb64-5"><a href="3.7-infer.html#cb64-5" tabindex="-1"></a></span>
<span id="cb64-6"><a href="3.7-infer.html#cb64-6" tabindex="-1"></a>y9 <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit9, <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="fl">1.538</span>, <span class="fl">1.323</span>, <span class="fl">0.01</span>)), <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb64-7"><a href="3.7-infer.html#cb64-7" tabindex="-1"></a></span>
<span id="cb64-8"><a href="3.7-infer.html#cb64-8" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb64-9"><a href="3.7-infer.html#cb64-9" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(x, y), <span class="at">size =</span> <span class="dv">2</span>,</span>
<span id="cb64-10"><a href="3.7-infer.html#cb64-10" tabindex="-1"></a>             <span class="at">data =</span> <span class="fu">tibble</span>(x, y)) <span class="sc">+</span></span>
<span id="cb64-11"><a href="3.7-infer.html#cb64-11" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(x, y),</span>
<span id="cb64-12"><a href="3.7-infer.html#cb64-12" tabindex="-1"></a>            <span class="at">data =</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="fl">1.538</span>, <span class="fl">1.323</span>, <span class="fl">0.01</span>),</span>
<span id="cb64-13"><a href="3.7-infer.html#cb64-13" tabindex="-1"></a>                          <span class="at">y =</span> y9),</span>
<span id="cb64-14"><a href="3.7-infer.html#cb64-14" tabindex="-1"></a>            <span class="at">color =</span> <span class="st">&quot;darkred&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>Конструювання моделі, що є занадто складною для вхідних даних і, відтак, майже ідеально підбудовується до неї, є практикою що часто називають <strong>пере-пристосуванням</strong>, або перенавчанням (<em>overfitting</em>). Пере-пристосовані моделі, завдяки своїй складності, разом із реальним взаємозв’язком між змінними описують випадковий шум (який є <a href="3.7-infer.html#inference">незменшуваною помилкою</a>), і відтак не можуть бути використані для адекватного умомиводу та передбачення.</p>
<p>Отже, якщо взяти інший набір даних із тієї ж дослідної системи, то складна пере-пристосована модель, побудована на тих же даних, матиме зовсім інший вигляд (наприклад, коефіцієнти поліноміального рівняння значно зміняться). Властивість моделі змінюватися значною мірою залежно від даних, використаних для її побудування, називають <strong>варіацією</strong> (<em>variance</em>). Модель із високою варіацією буде значно змінюватись за незначних змін у вхідному наборі даних, і високопластичні методи (на кшталт тих же поліномальних моделей високого порядку) мають, зазвичай, високу варіацію. Ідеально, ми намагаємось зменшити варіацію методу, аби побудована модель не сильно залежала від випадкового шуму в даних і не була <a href="https://en.wikipedia.org/wiki/Idiosyncrasy">ідіосинкратичною</a> до конкретного використаного набору даних.</p>
<p>З іншого боку, <strong>упередження</strong> (<em>bias</em>) методу відповідає тій похибці, яку ми вводимо коли намагаємось описати складні взаємозв’язки реального життя простими моделями. Наприклад, проста лінійна регресія є доволі потужним і легко інтерпретованим методом, однак в екологічних системах рідко коли можна спостерігати чіткий лінійний зв’язок. Всякий алгоритм в аналізі даних може містити системну помилку і невірно інтерпретувати дані, і подальше використання цього алгоритму вводитиме упередження. Значне упередження моделі викликатиме <em>недо-пристосування</em>, коли нестача гнучкості моделі змушує модель ігнорувати релевантну інформацію (Рис. <a href="3.7-infer.html#fig:fig-high-bias">3.15</a>).</p>
<div class="figure"><span style="display:block;" id="fig:fig-high-bias"></span>
<img src="bookdown-demo_files/figure-html/fig-high-bias-1.png" alt="Використання простого методу для опису складної нелінійної залежності, в принципі, є можливим, однак призводитиме до хибного умовиводу та передбачення. В цьому випадку існує куполоподібна залежність між змінними, однак проста лінійна регресія не вказує на будь-який статистично значущий взаємозв'язок. Приклад описує зв'язок між кількістю відкладених яєць самками американського кліща собачого (*Dermacentor variabilis*) та температурою середовища в період перед відкладанням яєць ([Mount &amp; Haile 1989](https://doi.org/10.1093/jmedent/26.1.60))." width="672" />
<p class="caption">
Рис. 3.15: Використання простого методу для опису складної нелінійної залежності, в принципі, є можливим, однак призводитиме до хибного умовиводу та передбачення. В цьому випадку існує куполоподібна залежність між змінними, однак проста лінійна регресія не вказує на будь-який статистично значущий взаємозв’язок. Приклад описує зв’язок між кількістю відкладених яєць самками американського кліща собачого (<em>Dermacentor variabilis</em>) та температурою середовища в період перед відкладанням яєць (<a href="https://doi.org/10.1093/jmedent/26.1.60">Mount &amp; Haile 1989</a>).
</p>
</div>
<p>Найкращим методом для статистичного умовиводу та передбачення є такий метод, який має найменше упередження і найменшу варіацію. Однак, на практиці ці дві властивості пов’язані між собою: пластичні методи мають високу варіацію та низьке упередження. Балансування варіації та упередження в аналізі даних часто називають <strong>компромісом упередження та варіації</strong> (<em>bias-variance trade-off</em>), і пошук найкращої моделі зводиться до обережного налаштування параметрів складності моделі, точності її передбачень, та можливості генералізації моделі і її використання на даних, що не були використані на етапі тренування моделі.</p>
</div>
</div>
<!-- </div> -->
<div class="footnotes">
<hr />
<ol start="41">
<li id="fn41"><p>Наприклад, дистанційне зондування поверхні планети – мультиспектральні супутникові знімки із програм Sentinel та Landsat є у вільному доступі й оновлюються кожні 10–16 днів.<a href="3.7-infer.html#fnref41" class="footnote-back">↩︎</a></p></li>
<li id="fn42"><p>Іншими словами, нам в принципі все одно яку модель використовувати, нехай це буде хоч <a href="https://uk.wikipedia.org/wiki/%D0%A7%D0%BE%D1%80%D0%BD%D0%B8%D0%B9_%D1%8F%D1%89%D0%B8%D0%BA">чорна скринька</a>, аби тільки на виході із моделі виходило хороше передбачення.<a href="3.7-infer.html#fnref42" class="footnote-back">↩︎</a></p></li>
<li id="fn43"><p>… а також гомоскедастичними і нормально розподіленими – дуже раджу читати про припущення методів перед їх використанням!<a href="3.7-infer.html#fnref43" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3.6-stat-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-popeco.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/index.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
