<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Розділ 3 Базові математичні підходи в екології | Вступ до Екології Угруповань</title>
<meta name="author" content="Олексій Дубовик">
<meta name="description" content="“Зазвичай, доволі проблематично пояснити для широкої публіки чим займаються статистики.” — Хоґґ, Танніс, Циммерман, “Ймовірність та Статистичний Умовивід” (9-те видання)  “Цілком можливо, що жодна...">
<meta name="generator" content="bookdown 0.41 with bs4_book()">
<meta property="og:title" content="Розділ 3 Базові математичні підходи в екології | Вступ до Екології Угруповань">
<meta property="og:type" content="book">
<meta property="og:description" content="“Зазвичай, доволі проблематично пояснити для широкої публіки чим займаються статистики.” — Хоґґ, Танніс, Циммерман, “Ймовірність та Статистичний Умовивід” (9-те видання)  “Цілком можливо, що жодна...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Розділ 3 Базові математичні підходи в екології | Вступ до Екології Угруповань">
<meta name="twitter:description" content="“Зазвичай, доволі проблематично пояснити для широкої публіки чим займаються статистики.” — Хоґґ, Танніс, Циммерман, “Ймовірність та Статистичний Умовивід” (9-те видання)  “Цілком можливо, що жодна...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.8.0/transition.js"></script><script src="libs/bs3compat-0.8.0/tabs.js"></script><script src="libs/bs3compat-0.8.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Вступ до Екології Угруповань</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Вітання</a></li>
<li><a class="" href="%D0%BF%D0%B5%D1%80%D0%B5%D0%B4%D0%BC%D0%BE%D0%B2%D0%B0.html">Передмова</a></li>
<li><a class="" href="%D0%BF%D0%BE%D0%B4%D1%8F%D0%BA%D0%B8.html">Подяки</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Вступ</a></li>
<li><a class="" href="about-book.html"><span class="header-section-number">2</span> Про книгу та Зміст</a></li>
<li><a class="active" href="numerical-ecology.html"><span class="header-section-number">3</span> Базові математичні підходи в екології</a></li>
<li><a class="" href="popeco.html"><span class="header-section-number">4</span> Початки популяційної екології</a></li>
<li><a class="" href="foundations.html"><span class="header-section-number">5</span> Фундаментальні поняття екології</a></li>
<li><a class="" href="interspecific.html"><span class="header-section-number">6</span> Міжвидові взаємодії</a></li>
<li><a class="" href="comecol.html"><span class="header-section-number">7</span> Екологічні угруповання</a></li>
<li><a class="" href="%D0%BF%D1%96%D1%81%D0%BB%D1%8F%D1%81%D0%BB%D0%BE%D0%B2%D0%BE.html">Післяслово</a></li>
<li><a class="" href="%D0%BA%D0%BE%D0%BD%D1%82%D0%B0%D0%BA%D1%82%D0%B8-%D0%B0%D0%B2%D1%82%D0%BE%D1%80%D0%B0.html">Контакти автора</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="numerical-ecology" class="section level1" number="3">
<h1>
<span class="header-section-number">Розділ 3</span> Базові математичні підходи в екології<a class="anchor" aria-label="anchor" href="#numerical-ecology"><i class="fas fa-link"></i></a>
</h1>
<blockquote>
<p>“Зазвичай, доволі проблематично пояснити для широкої публіки
чим займаються статистики.”</p>
<p>— Хоґґ, Танніс, Циммерман, “Ймовірність та Статистичний Умовивід” (9-те видання)</p>
</blockquote>
<blockquote>
<p>“Цілком можливо, що жодна зі змінних, котрі ми маємо,
не вносить нічого значущого до нашого розуміння
варіації в залежній змінній.
Нормально, це не те що ви хочете побачити
під кінець трирічного дослідницького проекту.”</p>
<p>“Тест Коломогорова-Смірнова: люди знають його за його відоме ім’я,
радше ніж за те що він, власне, робить.”
— М. Кровлі, “Книга R” (1-ше видання)</p>
</blockquote>
<p>Цей розділ можна розділити на три основні теми: елементи перед-вишмату (<em>pre-calculus</em>, базова математика), лінійна алгебра (теорія матриць), та початки статистики, які виявились настільки об’ємними, що їх довелося розбити на декілька секцій.</p>
<p>Перед-вишмат є дуже поверхневим нагадуванням щодо основних математичних законів, котрі знаходять застосунок не тільки в екології, а й в повсякденному житті. Цю частину варто вважати швидким довідником, до якого варто звертатись лише якщо є потреба щось пригадати.</p>
<p>Матриці мають широке застосування в статистичному аналізі (певною мірою, будь-який набір даних є матрицею), і читач може отримати певну вигоду від ознайомлення зі специфічною термінологією, котра також застосовується і в класичних методах екології (<a href="popeco.html#Leslie-matrix">наприклад, тут</a>).</p>
<p>Нарешті, початки статистичного аналізу, мабуть, є найбільш значущими для розуміння, адже методи екології угруповань часто ґрунтуються на класичних поняттях статистики, як-то нульова гіпотеза, оцінка параметрів, випадкова змінна із певним розподілом, ймовірність. Відповідно, для розуміння екологічної складової варто мати певне уявлення щодо статистичних понять, на котрих екологія базується.</p>
<p><strong><span style="color: #8B0000;">Зверніть увагу</span></strong>, щодо цілого розділу варто ще раз зазначити, що його не варто вважати самодостатнім джерелом інформації зі статистичного аналізу. Найкращим використанням цього розділу буде раз в іноді підглянути сюди коли щось забулось. Розуміння статистичного аналізу вимагає тривалої підготовки, а математичний базис для статистики – відповідної математичної освіти. Відтак, чимало тем в цьому розділу описані лише поверхнево (екологічній статистиці варто приділити цілий окремий підручник, наприклад, <a href="https://global.oup.com/academic/product/a-primer-of-ecological-statistics-9781605350646?cc=us&amp;lang=en&amp;">Gotelli &amp; Ellison “A Primer of Ecological Statistics” (2012)</a>), і на всілякі питання читача “а чому так?” найдоцільнішою відповіддю буде “просто повірте на слово”. Загальною порадою для цього розділу буде гуглити всі незрозумілі поняття, благо наведені англомовні відповідники.</p>
<div id="algebra" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Математична пам’ятка<a class="anchor" aria-label="anchor" href="#algebra"><i class="fas fa-link"></i></a>
</h2>
<div id="дроби" class="section level3" number="3.1.1">
<h3>
<span class="header-section-number">3.1.1</span> Дроби<a class="anchor" aria-label="anchor" href="#%D0%B4%D1%80%D0%BE%D0%B1%D0%B8"><i class="fas fa-link"></i></a>
</h3>
<p>Серед дробів можна виділити наступні закономірності:</p>
<p><span class="math display">\[a+\frac{c}{d} = \frac{ad+c}{d}\]</span></p>
<p><span class="math display">\[\frac{a}{b}+\frac{c}{d} = \frac{ad+cb}{bd}\]</span></p>
<p><span class="math display">\[\frac{a}{b} - \frac{c}{d} = \frac{ad-cb}{bd}\]</span></p>
<p><span class="math display">\[\frac{a}{b} \cdot \frac{c}{d} = \frac{ac}{bd}\]</span></p>
<p><span class="math display">\[\frac{a/b}{c/d} = \frac{a}{b} \cdot \frac{d}{c}\]</span></p>
<p><span class="math display">\[\text{якщо } \frac{a}{b} = \frac{c}{d} \text{, то } \frac{a}{b} = \frac{c}{d} = \frac{a+b}{c+d}\]</span></p>
</div>
<div id="математичні-символи" class="section level3" number="3.1.2">
<h3>
<span class="header-section-number">3.1.2</span> Математичні символи<a class="anchor" aria-label="anchor" href="#%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%BD%D1%96-%D1%81%D0%B8%D0%BC%D0%B2%D0%BE%D0%BB%D0%B8"><i class="fas fa-link"></i></a>
</h3>
<p>На цьому етапі варто ввести також декілька позначень, які можуть бути маловідомими, наприклад,</p>
<ul>
<li>
<span class="math inline">\(&gt;, \geq, &lt;, \leq\)</span> : більше, більше або дорівнює, менше, менше або дорівнює;</li>
<li>
<span class="math inline">\(\subset, \not\subset\)</span> : є або не є підмножиною;</li>
<li>
<span class="math inline">\(\in, \notin\)</span> : в або не в;</li>
<li>
<span class="math inline">\(\{x : x &gt; a\}\)</span> : множина таких <span class="math inline">\(x\)</span>, що для кожного <span class="math inline">\(x\)</span> в цій множині <span class="math inline">\(x&gt;a\)</span>;</li>
<li>
<span class="math inline">\(A^c\)</span> - комплемент до множини <span class="math inline">\(A\)</span>, сукупність елементів які не належать до <span class="math inline">\(A\)</span>: <span class="math inline">\(\{x: x \notin A\}\)</span>;</li>
<li>
<span class="math inline">\(\emptyset\)</span> - порожня множина, в якій нічого нема.</li>
<li>
<span class="math inline">\((a, b) = \{x:a&lt;x&lt;b\}\)</span>,</li>
<li>
<span class="math inline">\([a, b] = \{x:a \leq x \leq b\}\)</span>;</li>
<li>
<span class="math inline">\(\propto\)</span> : пропорційно до;</li>
<li>
<span class="math inline">\(\simeq\)</span> : приблизно дорівнює;</li>
<li>
<span class="math inline">\(\sim\)</span> : подібно до, однак часто позначає змінну, що розподілена відповідно до певного розподілу;</li>
<li>
<span class="math inline">\(\cup\)</span> : об’єднання множин;</li>
<li>
<span class="math inline">\(\cap\)</span> : перетин множин;</li>
<li>
<span class="math inline">\(\exists\)</span> : існує;</li>
<li>
<span class="math inline">\(\Rightarrow\)</span> : відтак;</li>
<li>
<span class="math inline">\(\iff\)</span> : рівнозначно;</li>
<li>
<span class="math inline">\(\forall\)</span> : для всіх;</li>
<li>
<span class="math inline">\(\sum\limits_{i=j}^{n}x_i\)</span> : сума всіх елементів <span class="math inline">\(x_i\)</span> де <span class="math inline">\(i = j, j+1, j+2, \cdots, n-2, n-1, n\)</span>;</li>
<li>
<span class="math inline">\(\prod\limits_{i=j}^{n}x_i\)</span> : добуток всіх елементів <span class="math inline">\(x_i\)</span> де <span class="math inline">\(i = j, j+1, j+2, \cdots, n-2, n-1, n\)</span>;</li>
<li>
<span class="math inline">\(a!\)</span> є факторіалом, де <span class="math inline">\(a! = 1 \cdot 2 \cdot 3 \cdot \cdots \cdot (a-2) \cdot (a-1) \cdot a\)</span>;</li>
<li>
<span class="math inline">\(\binom{a}{b}\)</span> є біноміальним коефіцієнтом, де <span class="math inline">\(\binom{a}{b} = \frac{a!}{b!(a-b)!}\)</span>.</li>
</ul>
</div>
<div id="нерівності" class="section level3" number="3.1.3">
<h3>
<span class="header-section-number">3.1.3</span> Нерівності<a class="anchor" aria-label="anchor" href="#%D0%BD%D0%B5%D1%80%D1%96%D0%B2%D0%BD%D0%BE%D1%81%D1%82%D1%96"><i class="fas fa-link"></i></a>
</h3>
<p>Перелічені символи можна використати для пояснення нерівностей, зокрема,</p>
<p><span class="math display">\[a &lt; b, c \leq d \Rightarrow \begin{cases}
a + c &lt; b + d\\
a + c &lt; b + c\\
a - c &lt; b - c
\end{cases}\]</span></p>
<p><span class="math display">\[a &lt; b, c &gt; 0 \Rightarrow \begin{cases}
ac &lt; bc\\
\frac{a}{c} &lt; \frac{b}{c}
\end{cases}\]</span></p>
<p><span class="math display">\[a &lt; b, c &lt; 0 \Rightarrow \begin{cases}
ac &gt; bc\\
\frac{a}{c} &gt; \frac{b}{c}\\
-a &gt; -b
\end{cases}\]</span></p>
<p><span class="math display">\[0 &lt; a &lt; b\Rightarrow \frac{1}{a} &gt; \frac{1}{b} &gt; 0\]</span></p>
<p><span class="math display">\[a &lt; b\Rightarrow \begin{cases}
\frac{a+x}{b+x} &gt; \frac{a}{b} \\
\frac{a-x}{b-x} &lt; \frac{a}{b}
\end{cases}\]</span></p>
<p><span class="math display">\[a &gt; b\Rightarrow \begin{cases}
\frac{a+x}{b+x} &lt; \frac{a}{b} \\
\frac{a-x}{b-x} &gt; \frac{a}{b}
\end{cases}\]</span></p>
</div>
<div id="ступені" class="section level3" number="3.1.4">
<h3>
<span class="header-section-number">3.1.4</span> Ступені<a class="anchor" aria-label="anchor" href="#%D1%81%D1%82%D1%83%D0%BF%D0%B5%D0%BD%D1%96"><i class="fas fa-link"></i></a>
</h3>
<p>Класичне визначення ступеню наступне:</p>
<p><span class="math display">\[a^b = \underbrace{a \cdot a \cdot a \cdot \cdots \cdot a}_b\]</span>
або, в нотації добутку<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Варто помітити, що хоча й індекс &lt;span class="math inline"&gt;\(і\)&lt;/span&gt; присутній в діапазоні добутку, значення &lt;span class="math inline"&gt;\(a\)&lt;/span&gt; не має індексу і залишається незмінним.&lt;/p&gt;'><sup>13</sup></a>,</p>
<p><span class="math display">\[a^b = \prod\limits_{i=1}^b a\]</span></p>
<p>Для ступенів властиво наступне:</p>
<p><span class="math display">\[a^0 = 1\]</span></p>
<p><span class="math display">\[a^1 = a\]</span></p>
<p><span class="math display">\[a^{-m} = \frac{1}{a^m}\]</span></p>
<p><span class="math display">\[a^m \times a^n = a^{m+n}\]</span></p>
<p><span class="math display">\[\frac{a^m}{a^n} = a^{m-n}\]</span></p>
<p><span class="math display">\[(a^m)^n = a^{m \cdot n}\]</span></p>
<p><span class="math display">\[a^n \cdot b^n = (ab)^n\]</span></p>
<p><span class="math display">\[\frac{a^n}{b^n} = \left( \frac{a}{b}\right)^n\]</span></p>
<p><span class="math display">\[a^{\frac{m}{n}} = \sqrt[\leftroot{5} \uproot{10} n]{a^m} \Rightarrow a^{1/2} = \sqrt{a}\]</span></p>
<p><span class="math display">\[\frac{\sqrt[\leftroot{5} \uproot{10} n]{a}}{\sqrt[\leftroot{5} \uproot{10} n]{b}} = \sqrt[\leftroot{5} \uproot{10} n]{\frac{a}{b}} \iff \frac{\sqrt[\leftroot{5} \uproot{10} n]{a}}{\sqrt[\leftroot{5} \uproot{10} n]{b}} = \frac{a^{1/n}}{b^{1/n}} = \left( \frac{a}{b} \right)^{1/n}\]</span></p>
<p>Зведення константи в змінний ступінь (експонента) іноді може позначатись дещо незвично:</p>
<p><span class="math display">\[\exp (x_i) = e^{x_i}\]</span></p>
<p>де <span class="math inline">\(e\)</span> – число Ейлера.</p>
<p>В комп’ютерних системах можна також зустріти позначення на кшталт <code>1e-07</code>. В цьому випадку мається на увазі експоненціювання за основою <span class="math inline">\(10\)</span>:</p>
<p><span class="math display">\[\text{1e-07} = 1 \cdot 10^{-7}\]</span></p>
</div>
<div id="ряди-чисел" class="section level3" number="3.1.5">
<h3>
<span class="header-section-number">3.1.5</span> Ряди чисел<a class="anchor" aria-label="anchor" href="#%D1%80%D1%8F%D0%B4%D0%B8-%D1%87%D0%B8%D1%81%D0%B5%D0%BB"><i class="fas fa-link"></i></a>
</h3>
<p>Прогресії є рядами чисел, котрі підпорядковуються певним законам. Наприклад,</p>
<ul>
<li>
<strong><em>арифметична прогресія</em></strong> має вигляд <span class="math inline">\(a, a+d, a+2d, a+3d, \cdots\)</span>, де <span class="math inline">\(n\)</span>-ний член має значення <span class="math inline">\(T_n = a + (n-1)d\)</span>, а сума перших <span class="math inline">\(n\)</span> членів складає</li>
</ul>
<p><span class="math display">\[S_n = \sum\limits_{i=1}^n T_n = \sum\limits_{i=1}^n \left[ a + (i-1)d \right] = \frac{n}{2}[2a+(n-1)d]\]</span></p>
<ul>
<li>
<strong><em>геометрична прогресія</em></strong> має вигляд <span class="math inline">\(a, ar, ar^2, ar^3, \cdots\)</span>, де <span class="math inline">\(n\)</span>-ний член має значення <span class="math inline">\(T_n = ar^{n-1}\)</span>, а сума перших <span class="math inline">\(n\)</span> членів складає</li>
</ul>
<p><span class="math display">\[S_n = \sum\limits_{i=1}^n ar^{i-1} = \begin{cases}
a \left( \frac{r^n - 1}{r-1} \right) \text{ якщо } r&lt;1 \\
a \left( \frac{1-r^n}{1-r} \right) \text{ якщо } r&gt;1
\end{cases}\]</span></p>
<p>Суму перших <span class="math inline">\(n\)</span> натуральних чисел можна розрахувати як</p>
<p><span class="math display">\[1+2+3+\cdots + n = \sum\limits_{i=1}^n i = \frac{n(n+1)}{2}\]</span></p>
<p>Суму квадратів перших <span class="math inline">\(n\)</span> натуральних чисел можна розрахувати як</p>
<p><span class="math display">\[1^2+2^2+3^2+\cdots + n^2 = \sum\limits_{i=1}^n i^2 = \frac{n(n+1)(2n+1)}{6}\]</span></p>
<p>Суму кубів перших <span class="math inline">\(n\)</span> натуральних чисел можна розрахувати як</p>
<p><span class="math display">\[1^3+2^3+3^3+\cdots + n^3 = \sum\limits_{i=1}^n i^3 = \left( \frac{n(n+1)}{2} \right)^2\]</span></p>
</div>
<div id="ступені-арифметичних-операцій" class="section level3" number="3.1.6">
<h3>
<span class="header-section-number">3.1.6</span> Ступені арифметичних операцій<a class="anchor" aria-label="anchor" href="#%D1%81%D1%82%D1%83%D0%BF%D0%B5%D0%BD%D1%96-%D0%B0%D1%80%D0%B8%D1%84%D0%BC%D0%B5%D1%82%D0%B8%D1%87%D0%BD%D0%B8%D1%85-%D0%BE%D0%BF%D0%B5%D1%80%D0%B0%D1%86%D1%96%D0%B9"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[(a+b)^2 = a^2 + 2ab + b^2\]</span></p>
<p><span class="math display">\[(a-b)^2 = a^2 - 2ab + b^2\]</span></p>
<p><span class="math display">\[(a+b)^3 = a^3 + 3a^2b + 3ab^2 + b^3\]</span></p>
<p><span class="math display">\[(a-b)^3 = a^3 - 3a^2b + 3ab^2 - b^3\]</span></p>
<p><span class="math display">\[a^2 - b^2 = (a^2 - b^2)(a^2 + b^2)\]</span></p>
<p><span class="math display">\[a^3 + b^3 = (a+b)(a^2 - ab + b^2)\]</span></p>
<p><span class="math display">\[a^3 - b^3 = (a-b)(a^2 + ab + b^2)\]</span></p>
</div>
<div id="лінійні-та-поліноміальні-функції" class="section level3" number="3.1.7">
<h3>
<span class="header-section-number">3.1.7</span> Лінійні та поліноміальні функції<a class="anchor" aria-label="anchor" href="#%D0%BB%D1%96%D0%BD%D1%96%D0%B9%D0%BD%D1%96-%D1%82%D0%B0-%D0%BF%D0%BE%D0%BB%D1%96%D0%BD%D0%BE%D0%BC%D1%96%D0%B0%D0%BB%D1%8C%D0%BD%D1%96-%D1%84%D1%83%D0%BD%D0%BA%D1%86%D1%96%D1%97"><i class="fas fa-link"></i></a>
</h3>
<p>Лінійні рівняння мають чимале значення в популярних статистичних методах, зокрема, лінійній регресії. Найпростішу лінійну функцію можна розглядати як пряму горизонтальну лінію, котра ніяк не залежить від предиктора <span class="math inline">\(x\)</span> (Рис. <a href="numerical-ecology.html#fig:fig-3-1">3.1</a>):</p>
<p><span class="math display">\[y = f(x) = a\]</span></p>
<div class="figure">
<span style="display:block;" id="fig:fig-3-1"></span>
<img src="bookdown-demo_files/figure-html/fig-3-1-1.png" alt="Лінійні та поліноміальні функції $y = f(x)$ де $f(x) = a$, $f(x) = a + bx$, $f(x) = a + bx + cx^2$, $f(x) = a + bx + cx^2 + dx^3$ для $a=2, b = 3, c = -4, d = -2$." width="672"><p class="caption">
Рис. 3.1: Лінійні та поліноміальні функції <span class="math inline">\(y = f(x)\)</span> де <span class="math inline">\(f(x) = a\)</span>, <span class="math inline">\(f(x) = a + bx\)</span>, <span class="math inline">\(f(x) = a + bx + cx^2\)</span>, <span class="math inline">\(f(x) = a + bx + cx^2 + dx^3\)</span> для <span class="math inline">\(a=2, b = 3, c = -4, d = -2\)</span>.
</p>
</div>
<p>Більш поширеними є рівняння прямих ліній, які залежать від змінної <span class="math inline">\(x\)</span>. Найпростішим прикладом буде лінійне рівняння вигляду</p>
<p><span class="math display">\[y = f(x) = a + bx\]</span></p>
<p>де коефіцієнт <span class="math inline">\(a\)</span> відповідає значенню <span class="math inline">\(y\)</span> за <span class="math inline">\(x = 0\)</span> та коефіцієнт <span class="math inline">\(b\)</span> відповідає нахилу прямої, тобто відповідає на питання “на скільки одиниць змінюється <span class="math inline">\(y\)</span> за зміни <span class="math inline">\(x\)</span> на одну одиницю”.</p>
<p>Наприклад, розгляньмо детальніше функцію <span class="math inline">\(y = f(x) = a + bx\)</span>, в якій <span class="math inline">\(a = 2, b = 3\)</span> (Рис. <a href="numerical-ecology.html#fig:fig-3-2">3.2</a>):</p>
<div class="figure">
<span style="display:block;" id="fig:fig-3-2"></span>
<img src="bookdown-demo_files/figure-html/fig-3-2-1.png" alt="Зміст параметрів лінійної функції $y = f(x) = a + bx$ для $a=2, b = 3$." width="672"><p class="caption">
Рис. 3.2: Зміст параметрів лінійної функції <span class="math inline">\(y = f(x) = a + bx\)</span> для <span class="math inline">\(a=2, b = 3\)</span>.
</p>
</div>
<p>Лінійна функція є окремим випадком поліноміальної функції, в якій ми буквенні коефіцієнти (<span class="math inline">\(a, b, c, \cdots\)</span>) позначимо через індексовані коефіцієнти (<span class="math inline">\(\beta_0, \beta_1, \beta_2, \cdots\)</span>):</p>
<p><span class="math display">\[p(x, m) = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_m x^m\]</span></p>
<p>де <span class="math inline">\(m\)</span> позначає ступінь полінома. Відтак, найпростіша функція перетину є поліноміальною функцією ступеню <span class="math inline">\(m = 0\)</span>, де <span class="math inline">\(p(x, m = 0) = \beta_0\)</span>, функція прямої є поліноміальною функцією ступеню <span class="math inline">\(m = 1\)</span>, де <span class="math inline">\(p(x, m = 1) = \beta_0 + \beta_1 x\)</span>, квадратична функція є поліноміальною ступеню <span class="math inline">\(m = 2\)</span>, де <span class="math inline">\(p(x, m = 2) = \beta_0 + \beta_1 x + \beta_2 x^2\)</span> тощо.</p>
<p>Варто зазначити, що <span class="math inline">\(y\)</span> може бути не лише лінійною функцією однієї змінної, скажімо, <span class="math inline">\(x\)</span>, а й комбінації <span class="math inline">\(m\)</span> змінних <span class="math inline">\((x_1, x_2, x_3, \cdots, x_m)\)</span>, наприклад,</p>
<p><span class="math display">\[y = f(x_1, x_2, \cdots, x_m) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_m x_m\]</span></p>
<p>В статистичному методі лінійної регресії одна зі змінних може являти собою трансформовану іншу змінну. Наприклад, якщо ми визначимо <span class="math inline">\(x_2\)</span> як <span class="math inline">\(x_2 = (x_1)^2\)</span>, то квадратне рівняння можна визначити як лінійну комбінацію:</p>
<p><span class="math display">\[y = f(x_1, x_2) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 = \beta_0 + \beta_1 x_1 + \beta_2 (x_1^2) = p(x_1, 2)\]</span></p>
<p>Таким чином, лінійна регресія може бути легко використана для моделювання нелінійних поліноміальних взаємозв’язків за рахунок трансформації змінних і їх використання в лінійних рівняннях.</p>
</div>
<div id="logs" class="section level3" number="3.1.8">
<h3>
<span class="header-section-number">3.1.8</span> Логарифми<a class="anchor" aria-label="anchor" href="#logs"><i class="fas fa-link"></i></a>
</h3>
<p>Логарифмування є зворотним процесом то зведення в ступінь. Логарифм числа <span class="math inline">\(x\)</span> із основою <span class="math inline">\(a\)</span> є таким числом, зведення якого до ступеню <span class="math inline">\(a\)</span> поверне число <span class="math inline">\(x\)</span>, тобто,</p>
<p><span class="math display">\[\log_a x = b\iff a^b = x\]</span></p>
<p>Найбільш поширеними є десятковий логарифм <span class="math inline">\(\log_{10}\)</span> та натуральний логарифм <span class="math inline">\(\log_e\)</span> де <span class="math inline">\(e\)</span> – число Ейлера <span class="math inline">\(e \approx 2.718\)</span>, константа, визначена як <span class="math inline">\(e = \lim\limits_{n \rightarrow \infty} (1 + \frac{1}{n})^n\)</span>.</p>
<p>В екології особливо поширений модифікований десятковий логарифм, оскільки чисельності організмів часто мають або низькі значення на кшталт <span class="math inline">\(0, 1, 2\)</span>, або дуже високі значення порядку сотень та тисяч, на рівні яких можна знехтувати одиничними особами<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;На рівні однієї, двох, трьох особин плюс-мінус одна особина багато чого змінює, в той час якщо є значення, скажімо, &lt;span class="math inline"&gt;\(1234\)&lt;/span&gt;, то плюс-мінус одна особина не вносить значної інформації; порівняйте &lt;span class="math inline"&gt;\(\log_{10} 1 = 0\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\log_{10} 2 = 0.301\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\log_{10} 3 = 0.477\)&lt;/span&gt;, і &lt;span class="math inline"&gt;\(\log_{10} 1233 = 3.091\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\log_{10} 1234 = 3.091\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\log_{10} 1235 = 3.092\)&lt;/span&gt;.&lt;/p&gt;'><sup>14</sup></a>.</p>
<p>Модифікація логарифмічної трансформації в екології побудована таким чином, аби <span class="math inline">\(0\)</span> відповідало <span class="math inline">\(0\)</span> особин, <span class="math inline">\(1\)</span> відповідало <span class="math inline">\(1\)</span> особині, <span class="math inline">\(2\)</span> відповідало <span class="math inline">\(10\)</span> особинам, <span class="math inline">\(3\)</span> відповідало <span class="math inline">\(100\)</span> особинам, і так далі (<a href="https://doi.org/10.1111/j.1461-0248.2006.00926.x">Anderson et al. 2006</a>). Такої трансформації легко досягнути використовуючи функцію котра враховує факт, що логарифмування від’ємних значень та нуля неможливе (<span class="math inline">\(\log0 = -\infty\)</span>):</p>
<p><span class="math display">\[f(x) = \begin{cases}
[\log_{10}(x)+1] \times \mathbb{I}_x(x &gt; 0)\\
0 \times \mathbb{I}_x(x = 0)
\end{cases}\]</span></p>
<p>де <span class="math inline">\(\mathbb{I}_x(\cdot)\)</span> є індикаторною функцією, котра приймає значення <span class="math inline">\(1\)</span> якщо логічна умова <span class="math inline">\((\cdot)\)</span> (тут, що <span class="math inline">\(x &gt; 0\)</span>) справджується.</p>
<p>Варто мати на увазі, що, в загальному, логарифмічне трансформування чисельностей не є оптимальною практикою, адже різниця між <span class="math inline">\(\log_{10}(0)\)</span> та <span class="math inline">\(\log_{10}(1)\)</span> така ж, як, наприклад, між <span class="math inline">\(\log_{10}(1000)\)</span> та <span class="math inline">\(\log_{10}(10000)\)</span>, в той час як відсутність виду, котра призводить до отримання нульової чисельності, може передбачати набагато важливіші екологічні механізми порівняно із присутністю виду, котра призводить до не-нульової чисельності (<a href="https://doi.org/10.1111/j.2041-210X.2010.00021.x">O’Hara and Kotze 2010</a>).</p>
<p>Логарифми мають наступні властивості:</p>
<p><span class="math display">\[\log_a(xy) = \log_a(x) + \log_a(y)\]</span></p>
<p><span class="math display">\[\log_a(\frac{x}{y}) = \log_a(x) - \log_a(y)\]</span></p>
<p><span class="math display">\[\log_a(x^b) = b \log_a (x)\]</span></p>
<p><span class="math display">\[\log_a(x) = \frac{\log_b (x)}{\log_b (a)} \forall b\]</span></p>
</div>
<div id="поширені-математичні-функції" class="section level3" number="3.1.9">
<h3>
<span class="header-section-number">3.1.9</span> Поширені математичні функції<a class="anchor" aria-label="anchor" href="#%D0%BF%D0%BE%D1%88%D0%B8%D1%80%D0%B5%D0%BD%D1%96-%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%BD%D1%96-%D1%84%D1%83%D0%BD%D0%BA%D1%86%D1%96%D1%97"><i class="fas fa-link"></i></a>
</h3>
<p>Кількість найпростіших математичних функцій доволі обмежена, однак, їх використання для трансформації змінних може бути кардинально різним. Трансформація даних може знадобитись пізніше, наприклад, для задоволення передбачень певних статистичних методів. Наприклад, лінійна регресія передбачає, що змінні розподілені нормально. В разі, якщо це не відповідає дійсності, одним із варіантів подальших дій є трансформування змінних певною функцією (наприклад, логарифмічно), за чого результуюча змінна може мати ближчий до нормального розподіл. Форми таких функцій наведені на Рис. <a href="numerical-ecology.html#fig:fig-3-3">3.3</a>.</p>
<div class="figure">
<span style="display:block;" id="fig:fig-3-3"></span>
<img src="bookdown-demo_files/figure-html/fig-3-3-1.png" alt="Графіки поширених функцій." width="672"><p class="caption">
Рис. 3.3: Графіки поширених функцій.
</p>
</div>
<p>Варто також мати на увазі зворотні функції:</p>
<ul>
<li>поліноми та корені <span class="math inline">\(f(x) = x^2 \iff f^{-1}(y) = y^{1/2}\)</span>;</li>
<li>експоненти та логарифми <span class="math inline">\(f(x) = e^x \iff f^{-1}(y) = \log_e(y)\)</span>;</li>
<li>зворотні функції <span class="math inline">\(f(x) = 1/x \iff f^{-1}(y) = 1/y\)</span>.</li>
</ul>
</div>
<div id="властивості-сум" class="section level3" number="3.1.10">
<h3>
<span class="header-section-number">3.1.10</span> Властивості сум<a class="anchor" aria-label="anchor" href="#%D0%B2%D0%BB%D0%B0%D1%81%D1%82%D0%B8%D0%B2%D0%BE%D1%81%D1%82%D1%96-%D1%81%D1%83%D0%BC"><i class="fas fa-link"></i></a>
</h3>
<p>Суму позначають як <span class="math inline">\(\sum\limits_{i=m}^{n}f(x_i)\)</span> де <span class="math inline">\(i\)</span> є індексом сумації, <span class="math inline">\(x_i\)</span> – індексованою змінною, <span class="math inline">\(m\)</span> – нижня межа сумації, <span class="math inline">\(n\)</span> – верхня межа сумації. В програмуванні таку нотацію можна пояснити через цикл:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="va">m</span><span class="op">:</span><span class="va">n</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="fu">f</span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<p>Суми мають наступні властивості</p>
<p><span class="math display">\[\sum\limits_{i=m}^{n}c\cdot f(x_i) = c \cdot \sum\limits_{i=m}^{n} f(x_i) \text{ } \forall \text{ } c : \text{const}\]</span></p>
<p><span class="math display">\[\sum\limits_{i=m}^{n} \left[ f(x_i) + g(x_i)\right] = \sum\limits_{i=m}^{n} f(x_i) + \sum\limits_{i=m}^{n} g(x_i)\]</span></p>
<p><span class="math display">\[\sum\limits_{i=m}^{n} f(x_i) = \sum\limits_{i=m}^{a} f(x_i) + \sum\limits_{i=a+1}^{n} f(x_i)\]</span></p>
<p><span class="math display">\[\left( \sum\limits_{i = m}^n x_i \right)\left( \sum\limits_{j = m}^n y_j \right) = \sum\limits_{i = m}^n \sum\limits_{j = m}^n (x_i y_j)\]</span></p>
<p><span class="math display">\[\sum \limits_{i=1}^n c = nc\]</span></p>
<p><span class="math display">\[\sum \limits_{i=0}^n \log i = \log n!\]</span></p>
<p><span class="math display">\[\sum \limits_{i=0}^n \binom{n}{i} = 2^n\]</span></p>
<p><span class="math display">\[\sum \limits_{i=0}^n \binom{n}{i} a^{n-i} b^i = (a+b)^n\]</span></p>
</div>
<div id="властивості-добутків" class="section level3" number="3.1.11">
<h3>
<span class="header-section-number">3.1.11</span> Властивості добутків<a class="anchor" aria-label="anchor" href="#%D0%B2%D0%BB%D0%B0%D1%81%D1%82%D0%B8%D0%B2%D0%BE%D1%81%D1%82%D1%96-%D0%B4%D0%BE%D0%B1%D1%83%D1%82%D0%BA%D1%96%D0%B2"><i class="fas fa-link"></i></a>
</h3>
<p>Зміст нотації добутків подібний до суми. Суму позначають як <span class="math inline">\(\prod\limits_{i=m}^{n}f(x_i)\)</span> де <span class="math inline">\(i\)</span> є індексом добутку, <span class="math inline">\(x_i\)</span> – індексованою змінною, <span class="math inline">\(m\)</span> – нижня межа добутку, <span class="math inline">\(n\)</span> – верхня межа добутку. Для добутків притаманні наступні властивості:</p>
<p><span class="math display">\[\prod\limits_{i=1}^n x = x^n\]</span></p>
<p><span class="math display">\[\prod\limits_{i=1}^n x_i y_i = \left( \prod\limits_{i=1}^n x_i\right)\left( \prod\limits_{i=1}^n y_i\right)\]</span></p>
<p><span class="math display">\[\left( \prod\limits_{i=1}^n x_i \right)^a = \prod\limits_{i=1}^n x_i^a\]</span></p>
<p><span class="math display">\[\log_b \left[ \prod\limits_{i = m}^n f(x_i) \right] = \sum\limits_{i=m}^n \left[ \log_b f(x_i) \right]\]</span></p>
<p><span class="math display">\[\prod\limits_{i = m}^n [c \cdot f(x_i)] = c^{\sum_{i=m}^n f(x_i)}\]</span></p>
</div>
<div id="диференціювання" class="section level3" number="3.1.12">
<h3>
<span class="header-section-number">3.1.12</span> Диференціювання<a class="anchor" aria-label="anchor" href="#%D0%B4%D0%B8%D1%84%D0%B5%D1%80%D0%B5%D0%BD%D1%86%D1%96%D1%8E%D0%B2%D0%B0%D0%BD%D0%BD%D1%8F"><i class="fas fa-link"></i></a>
</h3>
<p>Диференціювання функції – це процес знаходження похідної цієї функції. Похідна функції <span class="math inline">\(f(x)\)</span> є такою функцією <span class="math inline">\(f'(x)\)</span>, котра описує зміну значення <span class="math inline">\(f(x)\)</span> за зміни значення аргументу <span class="math inline">\(x\)</span>.</p>
<p>Наприклад, на Рис. <a href="numerical-ecology.html#fig:fig-3-2">3.2</a> зображено функцію прямої лінії <span class="math inline">\(f(x) = 2 + 3x\)</span>. В цьому випадку, приріст функції становить 3 одиниці <span class="math inline">\(y\)</span> на одну одиницю <span class="math inline">\(x\)</span>, і цей приріст залишається незмінним за будь-якого значення <span class="math inline">\(x\)</span>, оскільки функція є прямою. Цей приріст і є похідною, отже, <span class="math inline">\(f'(x) = 3\)</span>.</p>
<p>А що щодо випадків, коли <span class="math inline">\(f(x)\)</span> не є лінійною, що трапляється набагато частіше? В такому випадку, приріст <span class="math inline">\(f(x)\)</span> залежить від значення <span class="math inline">\(x\)</span>, тобто <span class="math inline">\(f'(x)\)</span> теж є функцією із аргументом <span class="math inline">\(x\)</span>. Власне, знаходження цієї функції і є метою диференціювання. Формально, визначення похідної являє собою зміну <span class="math inline">\(f(x)\)</span> за найменшої різниці <span class="math inline">\(x\)</span>.</p>
<p>Наприклад, визначмо функцію <span class="math inline">\(f(x) = -5x + x^3\)</span> (Рис. <a href="numerical-ecology.html#fig:fig-3-4">3.4</a>):</p>
<div class="figure">
<span style="display:block;" id="fig:fig-3-4"></span>
<img src="bookdown-demo_files/figure-html/fig-3-4-1.png" alt="Функція $f(x) = -5x + x^3$. Приріст цієї функції між $x_1$ та $x_2 = x_1 + h$ складає $\frac{f(x_1 + h) - f(x_1)}{h}$." width="672"><p class="caption">
Рис. 3.4: Функція <span class="math inline">\(f(x) = -5x + x^3\)</span>. Приріст цієї функції між <span class="math inline">\(x_1\)</span> та <span class="math inline">\(x_2 = x_1 + h\)</span> складає <span class="math inline">\(\frac{f(x_1 + h) - f(x_1)}{h}\)</span>.
</p>
</div>
<p>Для цієї функції, похідна буде такою функцією із аргументом <span class="math inline">\(x\)</span>, котра описуватиме <span class="math inline">\(f(x_2) - f(x_1)\)</span> на зміну <span class="math inline">\((x_2 - x_1)\)</span>. На Рис. <a href="numerical-ecology.html#fig:fig-3-4">3.4</a> показано зміну <span class="math inline">\(f(x)\)</span> для <span class="math inline">\(x_2 = 0, x_1 = -1.5\)</span>, тобто <span class="math inline">\(h = x_2 - x_1 = 1.5\)</span>, де значення функції становить <span class="math inline">\(f(x_2) = -5 \cdot 0 + 0^3 = 0\)</span>, <span class="math inline">\(f(x_1) = -5 \cdot (-1.5) + (-1.5)^3 = 4.125\)</span>, отже, <span class="math inline">\(f(x_2) - f(x_1) = f(x_1+h) - f(x_1) = -4.125\)</span>, а темп цієї зміни на одиницю <span class="math inline">\(x\)</span> становить <span class="math inline">\(\frac{f(x_1 + h) - f(x_1)}{h} = \frac{-4.125}{1.5} = -2.75\)</span>.</p>
<p>На тій частині функції, котра відповідає <span class="math inline">\(x_1, x_1+h\)</span>, отримане значення темпу зміни може не цілком відповідати реальній картині. Ми бачимо, що функція трішки зростає після <span class="math inline">\(x_1\)</span>, потім спадає, і взагалі є нелінійною, але обчислене значення <span class="math inline">\(\frac{f(x_1 + h) - f(x_1)}{h} = \frac{-4.125}{1.5} = 2.75\)</span> відповідає ситуації, коли розглянута функція є прямою лінією (зображено зеленим). Аби відобразити справжню природу зміни значення функції, необхідно зменшити значення <span class="math inline">\(h\)</span> до найменшого можливого значення. Відтак, класичне визначення похідної – це така функція із аргументом <span class="math inline">\(x\)</span>, що описує темп зміни <span class="math inline">\(\frac{f(x+h) - f(x)}{h}\)</span> за найменшої зміни <span class="math inline">\(h\)</span>:</p>
<p><span class="math display">\[f'(x) = \frac{df(x)}{dx} = \lim\limits_{h \rightarrow 0}\frac{f(x+h) - f(x)}{h}\]</span></p>
<p>де позначення похідної через <span class="math inline">\(\frac{df(x)}{dx}\)</span> інтуїтивно вказує на її природу, якщо позначити зміну значення змінної через <span class="math inline">\(d\)</span>: зміна значення <span class="math inline">\(f(x)\)</span> поділена на зміну значення <span class="math inline">\(x\)</span>. Похідну варто сприймати як приріст функції, котрий буде позитивним коли функція зростає, негативним коли функція спадає, і дорівнює нулю коли функція залишається сталою (таке можливе або якщо функція є прямою горизонтальною лінією, або коли в точках перегину, коли функція перестає зростати і починає спадати або навпаки).</p>
<p>Похідні мають наступні властивості, котрі допомагають розрахувати їх для будь-якої неперервної функції:</p>
<p><span class="math display">\[\frac{d}{dx}a = 0\]</span></p>
<p><span class="math display">\[\frac{d}{dx}ax = a\]</span></p>
<p><span class="math display">\[\frac{d}{dx} x^a = ax^{a-1}\]</span></p>
<p><span class="math display">\[\frac{d}{dx} e^x = e^x\]</span></p>
<p><span class="math display">\[\frac{d}{dx} a^x = a^x \ln (a) \text{ } \forall \text{ } a&gt;0\]</span></p>
<p><span class="math display">\[\frac{d}{dx} \ln (x) = \frac{1}{x} \text{ } \forall \text{ } x&gt;0\]</span></p>
<p><span class="math display">\[\frac{d}{dx} \log_a(x) = \frac{1}{x \ln(a)}\]</span></p>
<p>В диференціюванні складних функцій (функцій, котрі складаються з інших функцій, наприклад, <span class="math inline">\(f(\cdot)\)</span> і <span class="math inline">\(g(\cdot)\)</span>) керуються наступними правилами:</p>
<p><span class="math display">\[\frac{d}{dx} \left( af(x) + b g(x) \right) = a \frac{d}{dx}f(x) + b \frac{d}{dx}g(x) \iff (af(x) + bg(x))' = af'(x) + bg'(x)\]</span></p>
<p><span class="math display">\[\frac{d}{dx}(f(x)g(x)) = \frac{df(x)}{dx} g(x) + f(x) \frac{dg(x)}{dx} \iff (f(x)g(x))' = f'(x)g(x) + f(x)g'(x)\]</span></p>
<p><span class="math display">\[\frac{d}{dx} \left( \frac{f(x)}{g(x)}\right) = \frac{\frac{df(x)}{dx}g(x) - f(x) \frac{dg(x)}{dx}}{g(g(x))} \iff \left( \frac{f(x)}{g(x)} \right)' = \frac{f'(x)g(x) - f(x)g'(x)}{g^2(x)}\]</span></p>
<p>і для функції функції <span class="math inline">\(f(x) = h(g(x))\)</span>,</p>
<p><span class="math display">\[\frac{df(x)}{dx} = \frac{dh(g(x))}{dg(x)} \cdot \frac{dg(x)}{dx} \iff f'(x) = h'(g(x)) \cdot g'(x)\]</span></p>
<p>Оскільки похідна є функцією сама по собі, її також можна диференціювати (тобто знайти <span class="math inline">\(n\)</span>-ну похідну, або похідну <span class="math inline">\(n\)</span>-ного порядку). Наприклад, для функції <span class="math inline">\(f(x) = -5x + x^3\)</span>,</p>
<p><span class="math display">\[\begin{cases}
\frac{d}{dx}f(x) = -5 + 3x^2 \\
\frac{d^2}{dx}f(x) = 6x \\
\frac{d^3}{dx}f(x) = 6 \\
\frac{d^4}{dx}f(x) = 0
\end{cases}\]</span></p>
<div class="figure">
<span style="display:block;" id="fig:fig-3-5"></span>
<img src="bookdown-demo_files/figure-html/fig-3-5-1.png" alt="Функція $f(x) = -5x + x^3$ та її похідні." width="672"><p class="caption">
Рис. 3.5: Функція <span class="math inline">\(f(x) = -5x + x^3\)</span> та її похідні.
</p>
</div>
</div>
<div id="інтегрування" class="section level3" number="3.1.13">
<h3>
<span class="header-section-number">3.1.13</span> Інтегрування<a class="anchor" aria-label="anchor" href="#%D1%96%D0%BD%D1%82%D0%B5%D0%B3%D1%80%D1%83%D0%B2%D0%B0%D0%BD%D0%BD%D1%8F"><i class="fas fa-link"></i></a>
</h3>
<p>Інтегрування функції <span class="math inline">\(f(x)\)</span> – це процес знаходження такої функції <span class="math inline">\(F(x)\)</span>, похідна якої являє собою функцію <span class="math inline">\(f(x)\)</span>:</p>
<p><span class="math display">\[\frac{d}{dx}F(x) = f(x) \iff \int f(x) dx = F(x) + C\]</span></p>
<p>де <span class="math inline">\(C\)</span> – це будь-яка константа (оскільки похідна константи дорівнює нулю, ця константа не впливає на результат диференціювання).</p>
<p>Інтеграл є, певною мірою, континуальний аналог суми: якщо означення суми оперує дискретними значеннями <span class="math inline">\(i: i = 1, 2, 3, \cdots, n\)</span>, то інтеграл, подібно до похідних, розраховується для найменших можливих інтервалів аргументу функції <span class="math inline">\(x\)</span>.</p>
<p><strong><em>Невизначений інтеграл</em></strong> функції <span class="math inline">\(f(x)\)</span> <span class="math inline">\(\int f(x)dx = F(x)\)</span> має зміст як функція, похідна якої дорівнює вихідній функції <span class="math inline">\(f(x)\)</span>. Водночас, <strong><em>визначений інтеграл</em></strong> <span class="math inline">\(\int\limits_a^b f(x) dx = F(b) - F(a)\)</span> часто використовується для знаходження площі під кривою <span class="math inline">\(f(x)\)</span>, що обмежена значеннями <span class="math inline">\(a\)</span> і <span class="math inline">\(b\)</span>.</p>
<p>В разі, якщо аналітичне знаходження невизначеного інтегралу складне або неможливе, корисним може видатись Рейманівське визначення визначеного інтегралу:</p>
<p><span class="math display">\[\int\limits_a^b f(x) dx = F(a) - F(b) = \sum\limits_{i=1}^n [F(x_i) - F(x_{i-1})]\]</span></p>
<p>де <span class="math inline">\([F(x_i) - F(x_{i-1})]\)</span> є площею прямокутника, обмеженого функцією <span class="math inline">\(f(x)\)</span> і дуже маленьким інтервалом <span class="math inline">\([x_{i-1}, x_i)\)</span> (Рис. <a href="numerical-ecology.html#fig:fig-3-6">3.6</a>).</p>
<div class="figure">
<span style="display:block;" id="fig:fig-3-6"></span>
<img src="bookdown-demo_files/figure-html/fig-3-6-1.png" alt="Знаходження визначеного інтеграла як Рейманівську суму функції $f(x) = -5 + 3x^2$." width="672"><p class="caption">
Рис. 3.6: Знаходження визначеного інтеграла як Рейманівську суму функції <span class="math inline">\(f(x) = -5 + 3x^2\)</span>.
</p>
</div>
<p>Якщо, наприклад, шукати площу під кривою як на Рис. <a href="numerical-ecology.html#fig:fig-3-6">3.6</a> поділом інтервалу між <span class="math inline">\(a = 1.5\)</span>, <span class="math inline">\(b = 2.5\)</span> на <span class="math inline">\(10\)</span> прямокутників (хоча їх може бути скільки завгодно), то це нескладно обчислити за допомогою R:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">f</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">-</span><span class="fl">5</span> <span class="op">+</span> <span class="fl">3</span><span class="op">*</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">1.5</span>, <span class="fl">2.4</span>, <span class="fl">0.1</span><span class="op">)</span><span class="op">+</span><span class="fl">0.05</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fl">0.1</span><span class="op">*</span><span class="fu">f</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 7.2475</code></pre>
<p>Із прикладу на Рис. <a href="numerical-ecology.html#fig:fig-3-5">3.5</a> ми знаємо, що функція <span class="math inline">\(f(x) = -5 + 3x^2\)</span> є першою похідною функції <span class="math inline">\(F(x) = -5x + x^3\)</span>, яка, за визначенням, є невизначеним інтегралом <span class="math inline">\(\int f(x)dx\)</span>. Відповідно,</p>
<p><span class="math display">\[
\begin{aligned}
  \int\limits_{1.5}^{2.5}-5 + 3x^2 dx = F(2.5) - F(1.5) = \left[ -5x + x^3 \right]_{1.5}^{2.5} = \\ [-5 \cdot 2.5 + 2.5^3] - [-5 \cdot 1.5 + 1.5^3] = 3.125 - (-4.125) = 7.25 \simeq 7.2475
\end{aligned}
\]</span></p>
<p>Типові інтеграли мають наступні значення:</p>
<p><span class="math display">\[\int x^a dx = \frac{x^{a+1}}{a+1} + C \text{ } \forall \text{ } a \neq -1\]</span></p>
<p><span class="math display">\[\int x^{-1} dx = \int \frac{dx}{x} = \ln(|x|) + C\]</span></p>
<p><span class="math display">\[\int axdx = ax + C\]</span></p>
<p><span class="math display">\[\int \frac{1}{ax + b}dx = \frac{1}{a} \ln(|ax+b|) + C\]</span></p>
<p><span class="math display">\[\int \ln(x) dx = x \ln(x) -x + C\]</span></p>
<p><span class="math display">\[\int e^x dx = e^x + C\]</span></p>
<p>Інтегралам притаманні наступні властивості:</p>
<p><span class="math display">\[\int \limits_a^b c f(x) dx = c \int \limits_a^b f(x) dx\]</span></p>
<p><span class="math display">\[\int \limits_a^b f(x) + g(x) dx = \int \limits_a^b f(x) dx + \int \limits_a^b g(x) dx\]</span></p>
<p><span class="math display">\[\int \limits_a^a f(x)dx = 0\]</span></p>
<p><span class="math display">\[\int \limits_a^b f(x)dx = -\int \limits_b^a f(x)dx\]</span></p>
<p><span class="math display">\[\int \limits_a^c f(x)dx = \int \limits_a^b f(x)dx + \int \limits_b^c f(x)dx \text{ } \forall \text{ } a &lt; b &lt; c\]</span></p>
<p>Для інтегрування складних функцій використовують наступні техніки:</p>
<ul>
<li>інтегрування підстановкою</li>
</ul>
<p><span class="math display">\[\int \limits_a^b f(g(x)) g'(x)dx = \int \limits_{g(a)}^{g(b)} f(u) du\]</span></p>
<p>де <span class="math inline">\(u = g(x)\)</span>, <span class="math inline">\(du = g'(x)dx\)</span></p>
<ul>
<li>інтегрування частинами</li>
</ul>
<p><span class="math display">\[\int u (dv) = uv - \int v (du) \iff \int f(x)g'(x)dx = f(x)g(x) - \int f'(x) g(x) dx\]</span></p>
<p>де <span class="math inline">\(v = \int dv\)</span>.</p>
<p>В цілому, в екології рідко коли потрібно аналітично вирішити інтеграл чи похідну, однак, розуміння <em>змісту</em> цих операцій необхідне для подальшого розуміння статистичних підходів. В рідкісних випадках, коли необхідно вирішити певний інтеграл, має сенс скористатися онлайн-сервісами на кшталт <a href="https://www.integral-calculator.com/">integral-calculator.com</a> або розрахувати значення визначеного інтегралу ітеративно за допомогою Рейманівської суми, як показано вище. Варто також мати на увазі, що деякі функції неможливо або дуже складно аналітично проінтегрувати.</p>
</div>
</div>
<div id="matrices" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Лінійна алгебра<a class="anchor" aria-label="anchor" href="#matrices"><i class="fas fa-link"></i></a>
</h2>
<div id="визначення-матриці" class="section level3" number="3.2.1">
<h3>
<span class="header-section-number">3.2.1</span> Визначення матриці<a class="anchor" aria-label="anchor" href="#%D0%B2%D0%B8%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%BD%D1%8F-%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D1%96"><i class="fas fa-link"></i></a>
</h3>
<p>Матриця є двовимірним набором значень, що позначається як</p>
<p><span class="math display">\[\textbf{A} = \begin{bmatrix}
a_{1, 1} &amp; a_{1,2} &amp; \cdots &amp; a_{1, n}\\
a_{2, 1} &amp; a_{2,2} &amp; \cdots &amp; a_{2, n}\\
\vdots &amp; \vdots &amp; a_{i,j} &amp; \vdots \\
a_{m, 1} &amp; a_{m,2} &amp; \cdots &amp; a_{m, n}
\end{bmatrix}\]</span></p>
<p>в якій <span class="math inline">\(i: 1, 2, \cdots, m-1, m\)</span> позначає індекс рядка елементу і <span class="math inline">\(j: 1, 2, \cdots, n-1, n\)</span> позначає індекс колонки елементу <span class="math inline">\(a_{i, j}\)</span>. В цій книзі матриці як об’єкти позначатимуться жирними великими літерами і визначатимуться як матриці в квадратних дужках, але варто мати на увазі, що позначення іноді варіюють в різних джерелах.</p>
<p>Розмірність матриці визначається кількістю рядків <span class="math inline">\(m\)</span> та кількістю колонок <span class="math inline">\(n\)</span>. Якщо один із цих вимірів дорівнює одиниці, матриця являє собою <strong><em>вектор</em></strong> – послідовність значень. Поняття вектору важливе для розуміння оперування даними, адже спостереження (рядки) та параметри (колонки) в масиві даних є векторами розмірністю <span class="math inline">\(m \times 1\)</span> або <span class="math inline">\(1 \times n\)</span>. Вектори позначаються як <span class="math inline">\(\vec{a}\)</span>.</p>
<p>В подальшому матриці визначені як жирні літери (напр., <span class="math inline">\(\mathbf{A}\)</span>), а елементи матриці – як індексовані літери <span class="math inline">\(a_{i,j}\)</span>, або <span class="math inline">\([\mathbf{A}]_{i,j}\)</span>.</p>
</div>
<div id="трансформації-матриць" class="section level3" number="3.2.2">
<h3>
<span class="header-section-number">3.2.2</span> Трансформації матриць<a class="anchor" aria-label="anchor" href="#%D1%82%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D1%96%D1%97-%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D1%8C"><i class="fas fa-link"></i></a>
</h3>
<p><strong><em>Додавання матриць</em></strong>, якщо обидві матриці мають ідентичну розмірність, є доволі очевидним, адже додаються елементи із ідентичними індексами. Наприклад, <span class="math inline">\(\mathbf{A}+\mathbf{B} = a_{i,j}+b_{i, j}\)</span>.</p>
<p>Наприклад, якщо</p>
<p><span class="math display">\[\mathbf{A} = \begin{bmatrix}
a_{1, 1} &amp; a_{1,2} &amp; \cdots &amp; a_{1, n}\\
a_{2, 1} &amp; a_{2,2} &amp; \cdots &amp; a_{2, n}\\
\vdots &amp; \vdots &amp; a_{i,j} &amp; \vdots \\
a_{m, 1} &amp; a_{m,2} &amp; \cdots &amp; a_{m, n}
\end{bmatrix},
\mathbf{B} = \begin{bmatrix}
b_{1, 1} &amp; b_{1,2} &amp; \cdots &amp; b_{1, n}\\
b_{2, 1} &amp; b_{2,2} &amp; \cdots &amp; b_{2, n}\\
\vdots &amp; \vdots &amp; b_{i,j} &amp; \vdots \\
b_{m, 1} &amp; b_{m,2} &amp; \cdots &amp; b_{m, n}
\end{bmatrix}\]</span></p>
<p>тоді</p>
<p><span class="math display">\[\mathbf{A} + \mathbf{B} = \begin{bmatrix}
a_{1, 1}+b_{1, 1} &amp; a_{1,2}+b_{1,2} &amp; \cdots &amp; a_{1, n}+b_{1, n}\\
a_{2, 1}+b_{2, 1} &amp; a_{2,2}+b_{2,2} &amp; \cdots &amp; a_{2, n}+b_{2, n}\\
\vdots &amp; \vdots &amp; a_{i,j}+b_{i,j} &amp; \vdots \\
a_{m, 1}+b_{m, 1} &amp; a_{m,2}+b_{m,2} &amp; \cdots &amp; a_{m, n}+b_{m, n}
\end{bmatrix}\]</span></p>
<p>Подібно до додавання, <strong><em>скалярне множення</em></strong> матриць полягає в отриманні добутку константи із кожним елементом матриці:</p>
<p><span class="math display">\[c \cdot \mathbf{A} = c \cdot a_{i, j} = \begin{bmatrix}
c \cdot a_{1, 1} &amp; c \cdot a_{1,2} &amp; \cdots &amp; c \cdot a_{1, n}\\
c \cdot a_{2, 1} &amp; c \cdot  a_{2,2} &amp; \cdots &amp; c \cdot a_{2, n}\\
\vdots &amp; \vdots &amp; c \cdot a_{i,j} &amp; \vdots \\
c \cdot a_{m, 1} &amp; c \cdot a_{m,2} &amp; \cdots &amp; c \cdot a_{m, n}
\end{bmatrix}\]</span></p>
<p>Нарешті, <strong><em>транспонування</em></strong> матриці полягає в заміні індексування рядків та колонки і навпаки, <span class="math inline">\(i \rightarrow j, j \rightarrow i\)</span>, тобто,</p>
<p><span class="math display">\[\mathbf{A'} = \mathbf{A^T} = a'_{j, i} \text{ } \forall \text{ } \mathbf{A} = a_{i, j}\]</span></p>
</div>
<div id="операції-над-матрицями" class="section level3" number="3.2.3">
<h3>
<span class="header-section-number">3.2.3</span> Операції над матрицями<a class="anchor" aria-label="anchor" href="#%D0%BE%D0%BF%D0%B5%D1%80%D0%B0%D1%86%D1%96%D1%97-%D0%BD%D0%B0%D0%B4-%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D1%8F%D0%BC%D0%B8"><i class="fas fa-link"></i></a>
</h3>
<p><strong><em>Множення матриць</em></strong> є дещо складнішим, і загальним правилом є те, що для матриць <span class="math inline">\(A\)</span> розміром <span class="math inline">\(m \times n\)</span> і <span class="math inline">\(B\)</span> розміром <span class="math inline">\(n \times p\)</span> добуток складатиме матрицю розміром <span class="math inline">\(m \times p\)</span> (відповідно, <em>кількість колонок в першій матриці повинна дорівнювати кількості рядків в другій матриці</em>) із елементами, що відповідають сумі добутків рядків <span class="math inline">\(A\)</span> та колонок <span class="math inline">\(B\)</span>, тобто елемент добутку матриць із індексами <span class="math inline">\(i, j\)</span> складатиме (<a href="https://books.google.com/books/about/Linear_Algebra.html?id=P8BZzAEACAAJ">Bogacki 2019</a>) (Рис. <a href="numerical-ecology.html#fig:fig-3-7">3.7</a>)</p>
<p><span class="math display">\[[\mathbf{AB}]_{i, j} = \sum \limits_{r=1}^n (a_{i,r} b_{r, j}) = a_{i,1} b_{1, j} + a_{i, 2}b_{2, j} + \cdots + a_{i, n}b_{n, j}\]</span></p>
<div class="figure">
<span style="display:block;" id="fig:fig-3-7"></span>
<img src="images/matrices.png" alt="Знаходження значення елементу добутку матриць $\mathbf{A}$ і $\mathbf{B}$." width="1200"><p class="caption">
Рис. 3.7: Знаходження значення елементу добутку матриць <span class="math inline">\(\mathbf{A}\)</span> і <span class="math inline">\(\mathbf{B}\)</span>.
</p>
</div>
<p>За додавання матриць завжди потрібно враховувати розмірність матриць. Якщо матриця <span class="math inline">\(\mathbf{A}\)</span> має розмір <span class="math inline">\(m \times n\)</span>, друга матриця <span class="math inline">\(\mathbf{B}\)</span> має розмір <span class="math inline">\(n \times p\)</span>, то їх добуток <span class="math inline">\(\mathbf{AB}\)</span> матиме розмір <span class="math inline">\(m \times p\)</span>. Відтак, для множення матриць необхідно, аби кількість колонок першої матриці дорівнювала кількості рядків другої матриці.</p>
<p>Для добутків матриць властиво, що</p>
<p><span class="math display">\[\mathbf{AB} \neq \mathbf{BA}\]</span></p>
<p><span class="math display">\[(\mathbf{AB})\mathbf{C} = \mathbf{A}(\mathbf{BC})\]</span></p>
<p><span class="math display">\[(\mathbf{A+B})\mathbf{C} = \mathbf{AC} + \mathbf{BC}\]</span></p>
<p><span class="math display">\[\mathbf{C}(\mathbf{A+B}) = \mathbf{CA} + \mathbf{CB}\]</span></p>
<p>Очевидно, що якщо множення матриць можливе, то можливо також і звести матрицю в ступінь, наприклад,</p>
<p><span class="math display">\[\mathbf{A}^3 = (\mathbf{AA})\mathbf{A}\]</span></p>
<p>Уявімо квадратну <strong><em>одиничну матрицю</em></strong> <span class="math inline">\(\mathbf{I}_n\)</span> розміром <span class="math inline">\(n \times n\)</span>, де</p>
<p><span class="math display">\[\mathbf{I}_n = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots  &amp;\vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1
\end{bmatrix}\]</span></p>
<p>тобто всі, крім діагональних, елементи матриці дорівнюють нулю. Тоді</p>
<p><span class="math display">\[\mathbf{AB} = \mathbf{I}_n = \mathbf{BA}\]</span></p>
<p>отже, <span class="math inline">\(\mathbf{B}\)</span> є <strong><em>оберненою матрицею</em></strong> <span class="math inline">\(\mathbf{A}\)</span>:</p>
<p><span class="math display">\[\mathbf{B} = \mathbf{A}^{-1}\]</span></p>
<p><span class="math display">\[\mathbf{AA^{-1}} = \mathbf{I}_n\]</span></p>
<p><span class="math display">\[\mathbf{A}^0 = \mathbf{I}_n\]</span></p>
<p><strong>Не</strong> для будь-якої матриці може існувати обернена матриця.</p>
<p>Подібно до математичних функцій, <strong><em>лінійні трансформації</em></strong> можуть приймати вектори чи матриці в якості аргументу:</p>
<p><span class="math display">\[F: \mathbb{R}^n \rightarrow \mathbb{R}^m \text{ якщо}\]</span></p>
<p><span class="math display">\[F(\vec{u} + \vec{v}) = F(\vec{u}) + F(\vec{v}) \text{ } \forall \text{ } \vec{u}, \vec{v}\]</span></p>
<p><span class="math display">\[F(c \vec{u}) = c F(\vec{u}) \text{ } \forall \text{ } \vec{u}, c\]</span></p>
<p><span class="math display">\[F(c_1 \vec{u_1} + \cdots + c_k \vec{u_k}) = c_1 F(\vec{u_1}) + \cdots + c_k F(\vec{u_k})\]</span></p>
<p><span class="math display">\[F: \mathbb{R}^n \rightarrow \mathbb{R}^m \iff \exists \mathbf{A}: F(\vec{v}) = \mathbf{A} \vec{v}, \mathbf{A} = a_{i, j}, 1 &lt; i&lt;m, 1 &lt; j &lt; n\]</span></p>
</div>
<div id="детермінант-власні-вектори-та-власне-значення" class="section level3" number="3.2.4">
<h3>
<span class="header-section-number">3.2.4</span> Детермінант, власні вектори, та власне значення<a class="anchor" aria-label="anchor" href="#%D0%B4%D0%B5%D1%82%D0%B5%D1%80%D0%BC%D1%96%D0%BD%D0%B0%D0%BD%D1%82-%D0%B2%D0%BB%D0%B0%D1%81%D0%BD%D1%96-%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%B8-%D1%82%D0%B0-%D0%B2%D0%BB%D0%B0%D1%81%D0%BD%D0%B5-%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%BD%D1%8F"><i class="fas fa-link"></i></a>
</h3>
<p><strong><em>Детермінант</em></strong> визначається (<a href="https://books.google.com/books/about/Linear_Algebra.html?id=P8BZzAEACAAJ">Bogacki 2019</a>) для матриці <span class="math inline">\(1 \times 1\)</span> <span class="math inline">\(\mathbf{A} = [a_{1, 1}]\)</span> як</p>
<p><span class="math display">\[\det \mathbf{A} = a_{1, 1}\]</span></p>
<p>і для більших квадратних матриць розміром <span class="math inline">\(n \times n\)</span> рекурсивно</p>
<p><span class="math display">\[\det \mathbf{A} = \sum \limits_{j=1}^n (-1)^{1+j} a_{1, j} \det \mathbf{M}_{1,j}\]</span></p>
<p>де <span class="math inline">\(\mathbf{M}_{i,j}\)</span> є підматрицею від <span class="math inline">\(\mathbf{A}\)</span> розміром <span class="math inline">\((n - 1) \times (m - 1)\)</span> із видаленими рядком <span class="math inline">\(i\)</span> і колонкою <span class="math inline">\(j\)</span>, тобто,</p>
<p><span class="math display">\[\det \mathbf{A} = a_{1,1} \det \mathbf{M}_{1,1} - a_{1,2} \det \mathbf{M}_{1,2} + a_{1,3} \det \mathbf{M}_{1,3} - \cdots + (-1)^{1+n} a_{1, n} \det \mathbf{M}_{1,n}\]</span></p>
<p>Для матриці <span class="math inline">\(\mathbf{A}\)</span> розміром <span class="math inline">\(2 \times 2\)</span>, наприклад,</p>
<p><span class="math display">\[\det \mathbf{A} = \det \begin{bmatrix}
a_{1, 1} &amp; a_{1, 2} \\
a_{2, 1} &amp; a_{2, 2}
\end{bmatrix} = a_{1, 1} \cdot a_{2, 2} - a_{1, 2} \cdot a_{2, 1}\]</span></p>
</div>
<div id="matrices_art" class="section level3" number="3.2.5">
<h3>
<span class="header-section-number">3.2.5</span> Геометричний зміст матриць<a class="anchor" aria-label="anchor" href="#matrices_art"><i class="fas fa-link"></i></a>
</h3>
<p>Технічні визначення операцій над матрицями можуть видаватись надмірно деталізованими і неінтуїтивними правилами, котрі просто необхідно завчити. Однак, всі ці правила мають очевидний геометричний зміст, котрий рідко використовують в поясненнях базової теорії матриць <a href="https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">3Blue1Brown 2023</a>.</p>
<p>В першу чергу, необхідно зрозуміти поняття вектора. В багатьох чисельних методах і, зокрема, в програмному середовищі R, векторами звуться скінченні ряди чисел, наприклад, змінна в наборі даних. Однак, зі шкільної програми математики можна пригадати, що векторами є і спрямовані відрізки. Наприклад, якщо позначити вектор <span class="math inline">\(\vec{a}\)</span> як <span class="math inline">\(\vec{a} = (1, 2)\)</span>, то ряд чисел <span class="math inline">\((1, 2)\)</span> можна зобразити як такий вектор, що починається з координат <span class="math inline">\((x_0 = 0, y_0 = 0)\)</span> і закінчується координатами <span class="math inline">\((x_1 = 1, y_1 = 2)\)</span>.</p>
<p>Уявімо простір таких векторів. З попереднього параграфу можна помітити, що хоча й, формально, вектор має дві точки (в двовимірному просторі це <span class="math inline">\((x_0, y_0)\)</span> та <span class="math inline">\((x_1, y_1)\)</span>), для кожного вектору одна з точок буде однаковою: <span class="math inline">\((x_0 = 0, y_0 = 0)\)</span>. Відтак, ми не втратимо ніякої інформації, якщо для багатьох векторів ми забудемо про початкові точки і визначимо лише кінцеві точки. У двовимірному просторі можна визначити скільки завгодно векторів/точок <span class="math inline">\(\vec{v_1} = (x_{1, 1}, y_{1, 1}), \vec{v_2} = (x_{1, 2}, y_{1, 2}), \cdots, \vec{v_i} = (x_{1, i}, y_{1, i})\)</span>, але, звісно, це справджуватиметься і для одного виміру (<span class="math inline">\(\vec{v_1} = (x_{1, 1}), \vec{v_2} = (x_{1, 2}), \cdots, \vec{v_i} = (x_{1, i})\)</span>), і для трьох вимірів (<span class="math inline">\(\vec{v_1} = (x_{1, 1}, y_{1, 1}, z_{1, 1}), \vec{v_2} = (x_{1, 2}, y_{1, 2}, z_{1, 2}), \cdots, \vec{v_i} = (x_{1, i}, y_{1, i}, z_{1, i})\)</span>), і для будь-якої скінченної кількості вимірів. В структурі даних, відтак, хоча й ряди чисел є векторами, ми можемо уявити кожне спостереження як точку у просторі параметрів цього спостереження (т. зв. точка даних, або data point, що ж еквівалентним одиночному спостереженню).</p>
<p>Для простору векторів, матриця є лінійною трансформацією. Тобто, якщо помножити координати точки (або вектору, що є тим самим) на матрицю, то на виході ми отримаємо інший лінійний вектор. Подібно до математичної функції, ми можемо застосувати лінійну трансформацію до всього простору, і отримаємо інший простір, в котрому точки знаходяться та пропорційній до вихідного простору відстані одна від одної, а прямі між точками залишаються прямими<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Очевидно, ці дві умови не завжди можуть справджуватися, у випадку чого трансформація не є лінійною.&lt;/p&gt;"><sup>15</sup></a>.</p>
<p>Пригадаймо правила множення матриць. Для цього потрібно кожен вектор уявити у вигляді матриці. Наприклад, для вектору <span class="math display">\[\vec{a} = (x_1 = 1, y_1 = 2) = \begin{bmatrix}
x_1 = 1 \\
y_1 = 2 \end{bmatrix} = \begin{bmatrix}
1 \\
2 \end{bmatrix}\]</span></p>
<p>застосуймо матрицю</p>
<p><span class="math display">\[\mathbf{A} = \begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}\]</span></p>
<p><span class="math display">\[
\mathbf{A} \vec{a} = \begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix} \cdot \begin{bmatrix}
1 \\ 2
\end{bmatrix}= \begin{bmatrix}
1 \cdot 1 + 2 \cdot 2 \\
1 \cdot 3 + 2 \cdot 4
\end{bmatrix} = \begin{bmatrix}
5 \\ 11
\end{bmatrix}
\]</span></p>
<p>Тобто після застосування лінійної трансформації до вектору ми отримуємо вектор такої ж розмірності, при чому ця лінійна трансформація може бути застосована для будь-якого вектору такої розмірності. Тож матриця <span class="math inline">\(\mathbf{A}\)</span> відповідає лінійній трансформації, зображеній на Рис. <a href="numerical-ecology.html#fig:fig-3-8">3.8</a>.</p>
<div class="figure">
<span style="display:block;" id="fig:fig-3-8"></span>
<img src="bookdown-demo_files/figure-html/fig-3-8-1.png" alt="Матриця $\mathbf{A} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}$ як лінійна трансформація простору. Вектор $\vec{a} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$ є лише частиною цього простору." width="672"><p class="caption">
Рис. 3.8: Матриця <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span> як лінійна трансформація простору. Вектор <span class="math inline">\(\vec{a} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}\)</span> є лише частиною цього простору.
</p>
</div>
<p>Загалом, для всякого двовимірного простору можна уявити два незалежні базові вектори: <span class="math inline">\(\hat{i} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}\)</span> й <span class="math inline">\(\hat{j} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}\)</span>. Тоді будь-який вектор <span class="math inline">\(\vec{v}\)</span> є сумою векторів, що відображають добутки певної константи <span class="math inline">\(a\)</span> та <span class="math inline">\(\hat{i}\)</span> й <span class="math inline">\(b\)</span> та <span class="math inline">\(\hat{j}\)</span>: <span class="math inline">\(\vec{v} = a \hat{i} + b \hat{j}\)</span>, а, відтак, всяку лінійну трансформацію <span class="math inline">\(L(\cdot)\)</span> можна виразити у вигляді <span class="math display">\[L(\vec{v}) = a L(\hat{i}) + b L(\hat{j})\]</span></p>
<p>відтак, лінійна трансформація вектору <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix} \rightarrow L \left( \begin{bmatrix} x \\ y \end{bmatrix} \right)\)</span> є тотожною лінійній трансформації базових векторів, шкалованих на певні константи для базових векторів:</p>
<p><span class="math display">\[L \left( \begin{bmatrix} x \\ y \end{bmatrix} \right) = x[L (\hat{i})] + y[L(\hat{j})] = \begin{bmatrix} x L(1) &amp; xL(0) \\ y L(0) &amp; y L(1) \end{bmatrix}\]</span></p>
<p>Тобто сутність квадратної матриці як репрезентації лінійної трансформації – це сукупність координат, на які припадають базові вектори після лінійної трансформації. Для попереднього прикладу <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span>, зокрема, <span class="math inline">\(L(\hat{i})\)</span> матиме координати <span class="math inline">\((1, 3)\)</span>, а <span class="math inline">\(L(\hat{j})\)</span> – <span class="math inline">\((2, 4)\)</span>.</p>
<p>Якщо розглядати матрицю як носій інформації про лінійну трансформацію, то множення матриць є нічим іншим, як застосуванням лінійної трансформації до лінійної трансформації. Тобто, якщо уявити дві лінійні трансформації <span class="math inline">\(L_1\)</span> та <span class="math inline">\(L_2\)</span> такі що</p>
<p><span class="math display">\[L_2 \left( L_1 \left( \begin{bmatrix}
x \\
y
\end{bmatrix} \right) \right) =
\begin{bmatrix}
a &amp; c \\
b &amp; d
\end{bmatrix}
\left(
\begin{bmatrix}
e &amp; g \\
f &amp; h
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}
\right) =
\left(
\begin{bmatrix}
a &amp; c \\
b &amp; d
\end{bmatrix}
\begin{bmatrix}
e &amp; g \\
f &amp; h
\end{bmatrix}
\right)
\begin{bmatrix}
x \\
y
\end{bmatrix} =
\begin{bmatrix}
ae + bg &amp; af + bh \\
ce + dg &amp; cf + dh
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}\]</span></p>
<p>Із рисунку <a href="numerical-ecology.html#fig:fig-3-8">3.8</a> можна помітити, що площа фігур в просторі змінюється в результаті лінійної трансформації. Детермінант матриці відображає зміну площі квадрата зі сторонами <span class="math inline">\(\hat{i}\)</span> та <span class="math inline">\(\hat{j}\)</span> таким чином, що <span class="math inline">\(A[L(\hat{i} \times \hat{j})] = c \cdot A[(\hat{i} \times \hat{j})]\)</span>. Якщо <span class="math inline">\(c = 0\)</span>, то матриця з таким детермінантом позначає лінійну трансформацію, що призводить до зменшення розмірності вектора, а якщо <span class="math inline">\(c&lt;0\)</span>, то (подібно до того, як від’ємна площа має сенс в інтегруванні) змінюється орієнтація <span class="math inline">\(\hat{i}\)</span> відносно <span class="math inline">\(\hat{j}\)</span>.</p>
<p>Іншим поширеним застосуванням матриць є вирішення квадратних рівнянь. Наприклад, систему вигляду</p>
<p><span class="math display">\[\begin{cases}
a_1 x + b_1 y + c_1 z = d_1 \\
a_2 x + b_2 y + c_2 z = d_2 \\
a_3 x + b_3 y + c_3 z = d_3
\end{cases}\]</span></p>
<p>можна зобразити у вигляді матриць:</p>
<p><span class="math display">\[\begin{bmatrix}
a_1 &amp; b_1 &amp; c_1 \\
a_2 &amp; b_2 &amp; c_2 \\
a_3 &amp; b_3 &amp; c_3
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix} =
\begin{bmatrix}
d_1 \\
d_2 \\
d_3
\end{bmatrix} \iff \mathbf{A} \vec{x} = \vec{v}\]</span></p>
<p>і систему можна вирішити, якщо існує така <span class="math inline">\(\mathbf{A^{-1}}\)</span>, що <span class="math inline">\((\mathbf{AA^{-1}}) \vec{w} = \vec{w}\)</span> (тобто добуток цих двох матриць відображає таку лінійну трансформацію, яка не змінює вихідні вектори – таку трансформацію і кодує одинична матриця). Тоді <span class="math inline">\(\mathbf{A^{-1} A} \vec{x} = \mathbf{A^{-1}} \vec{v}\)</span>, отже, <span class="math inline">\(\vec{x} = \mathbf{A^{-1}} \vec{v}\)</span>.</p>
</div>
</div>
<div id="stats" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Ймовірність у статистиці<a class="anchor" aria-label="anchor" href="#stats"><i class="fas fa-link"></i></a>
</h2>
<p>Наступні розділи не стосуватимуться різноманітних статистичних тестів, адже їх існує безліч. Варто розуміти, що не існує універсального рецепту до статистичного аналізу даних, а формулювання на кшталт “зробити якусь статистику для моїх даних” є ґрунтовно помилковим. Підходи до статистичного аналізу завжди випливають від дослідницького питання і адекватно поставлених гіпотез, а недалеким від правди є твердження, що для кожного дослідження є свій аналіз.</p>
<p>Критичним є розуміння понять, котрими оперує статистичний аналіз і котрі використовують всілякі статистичні тести. В наступних розділах буде описано ймовірність як підґрунтя статистичного аналізу (цей розділ), <a href="numerical-ecology.html#pdf-pmf">розподіли ймовірності</a>, <a href="numerical-ecology.html#basic-hypotheses">тестування гіпотез</a>, <a href="numerical-ecology.html#stat-models">поняття статистичних моделей</a>, та <a href="numerical-ecology.html#infer">використання статистики для умовиводу та передбачення</a>.</p>
<div id="prob" class="section level3" number="3.3.1">
<h3>
<span class="header-section-number">3.3.1</span> Ймовірність<a class="anchor" aria-label="anchor" href="#prob"><i class="fas fa-link"></i></a>
</h3>
<p>Теорія ймовірності може видатись інтуїтивно зрозумілою до певної міри. Центральним поняттям її є, звісно, <strong>ймовірність</strong> (<em>probability</em>), для розуміння котрої необхідно окреслити поняття <strong>випадкового експерименту</strong> (<em>trial</em>) і <strong>випадкової події</strong> (<em>event</em>).</p>
<p>Випадковий експеримент є передмовою випадкової події. Наприклад, аби випав аверс, монету необхідно підкинути. Підкидання монети є випадковим експериментом, котрий може призвести до однієї із двох можливих випадкових подій: <strong>(1)</strong> випадає аверс, або <strong>(2)</strong> випадає реверс. Якщо ж монету не підкинути, то не станеться й випадкова подія.</p>
<p>Приклад монети завжди є доволі зручним, адже він інтуїтивний, простий, і зрозумілий. Очевидно, випадкові експерименти можуть бути набагато складнішими, а кількість альтернативних результуючих подій може бути незліченною.</p>
<p>У прикладі із монетою питання полягає в тому, яка є ймовірність події <strong>(1)</strong>, тобто випадання аверсу, або події <strong>(2)</strong>, себто випадання реверсу. Інтуїтивною відповіддю буде “50-на-50”, але це не є правильною відповіддю, адже ми не можемо знати це непевне. Що, наприклад, якщо вага монети незбалансована? Аби знайти відповідь на це питання, найпростішим підходом буде підкинути монетку безкінечну кількість разів і порахувати частоту випадання, скажімо, аверсу. Ця частота і буде ймовірністю.</p>
<p>Звісно, в реальності неможливо підкинути монету безліч разів, тому таке чисельне визначення ймовірності є суто теоретичним. Однак, якщо провести експеримент багато разів, це дозволить знайти приблизне значення шуканої ймовірності. Скоріш за все, воно буде близьким до <span class="math inline">\(P(аверс) \approx 0.5\)</span>. А якщо читач має добре підґрунтя в статистиці, то навіть знайдеться тест для перевірки чесності монети: звісно, що після багатьох підкидань спостережена частота аверсу може відрізнятись від <span class="math inline">\(0.5\)</span> і становити, скажімо, <span class="math inline">\(0.498\)</span>. Так от, різниця <span class="math inline">\(0.5 - 0.498 = 0.002\)</span> за певного розміру вибірки буде значущою (тобто монета нечесна) або ні.</p>
<p>Очевидно, що ймовірність не може бути від’ємною, а найменше її значення становить <span class="math inline">\(0\)</span>. В такому випадку (<span class="math inline">\(P = 0\)</span>), випадкова подія не станеться навіть якщо випадковий експеримент буде відтворено безкінечну кількість разів. З іншого боку, ймовірність <span class="math inline">\(1\)</span> вказує на те, що випадкова подія станеться за кожного експерименту. Зазвичай, значення ймовірності знаходиться десь в інтервалі між цими двома екстремальними значеннями.</p>
<p>В багатьох випадках, не потрібно мати монету в руках аби зрозуміти ймовірності подій. Щоправда, системи таких подій часто є набагато складнішими. Наприклад, що якщо є дві монетки? Простір можливих подій тоді стає більшим, адже тепер може випасти два аверса, два реверса, або аверс і реверс. Якими є ймовірності цих подій, якщо підкидання монети є незалежним від підкидання іншої монети, і обидві монети є чесними (тобто очікувана ймовірність випадіння аверсу дорівнює <span class="math inline">\(0.5\)</span>)?</p>
<p>Оскільки монет є дві, існує декілька сценаріїв розвитку подій: <strong>(1)</strong> монета 1 випадає на аверс і монета 2 випадає на аверс, <strong>(2)</strong> монета 1 випадає на аверс і монета 2 випадає на реверс, <strong>(3)</strong> монета 1 випадає на реверс і монета 2 випадає на аверс, або <strong>(4)</strong> монета 1 випадає на реверс і монета 2 випадає на реверс. То якими є ймовірності трьох (аверс-аверс, реверс-реверс, аверс-реверс) випадкових подій згаданих вище?<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Позначимо ймовірність випадання аверсу (A) або реверсу (R) на першій монеті як &lt;span class="math inline"&gt;\(P(C_1 = A) = P(C_1 = R) = 0.5\)&lt;/span&gt; і на другій монеті як &lt;span class="math inline"&gt;\(P(C_2 = A) = P(C_2 = R) = 0.5\)&lt;/span&gt;. Тоді &lt;span class="math inline"&gt;\(P(A, A) = P(C_1 = A) \cdot P(C_2 =  A) = 0.5 \cdot 0.5 = 0.25\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(P(R, R) = P(C_1 = R) \cdot P(C_2 =  R) = 0.5 \cdot 0.5 = 0.25\)&lt;/span&gt;, і &lt;span class="math inline"&gt;\(P(A, R) = P(C_1 = A) \cdot P(C_2 = R) + P(C_1 = R) \cdot P(C_2 = A) = 0.25 + 0.25 = 0.5\)&lt;/span&gt;.&lt;/p&gt;'><sup>16</sup></a></p>
<p>Ми бачимо як проста монетка може генерувати доволі складні ймовірнісні ситуації – а що ж тоді буде зі звичайними гральними кубиками? А якщо ми візьмемо до уваги щось складніше на кшталт набору кубиків до Підземелля й Драконів із їх 4-, 6-, 10-, 12-, і 20-гранними костями? В таких випадках <em>простори ймовірності</em> стають дедалі складнішими. І всі ці випадки є <em>дискретними</em> (<em>discrete</em>), в яких будь-яку подію можна описати неподільним одиничним значенням (з підкидання монетки може випасти або аверс, або реверс – ми маємо тільки два можливих значення), на відміну від <em>неперевних, або континуальних</em> (<em>continuous</em>) змінних (які можна описати дійсними числами).</p>
<p>Що таке ймовірнісний простір? Строго кажучи, <strong>ймовірнісний простір</strong> (<em>probability space</em>) – це формальна модель випадкового експерименту. У випадку з одним підкиданням монетки, його можна поділити на наступні елементи:</p>
<ul>
<li><p><strong>простір елементарних подій</strong> (<span class="math inline">\(\Omega\)</span>, <em>sample space</em>) – множина, яка описує всі можливі варіанти випадкової події: <span class="math inline">\(\{аверс, реверс\}\)</span>;</p></li>
<li><p>асоційована <em>сигма-алгебра</em> (<span class="math inline">\(\sigma\)</span>, <em>event space</em>) – якщо простими словами, то це така множина, яка включає в себе всі можливі підмножини <span class="math inline">\(\Omega\)</span>;</p></li>
<li><p><strong>ймовірності подій</strong> (<span class="math inline">\(P\)</span>, <em>probability</em>) – визначені для елементарних подій значення ймовірностей, наприклад: <span class="math inline">\(P(аверс) = 0.5, P(реверс) = 0.5\)</span>.</p></li>
</ul>
<p>Для прикладу з монеткою асоційована сигма-алгебра <span class="math inline">\(\sigma = \{\{аверс\}, \{реверс\}, \{аверс, реверс\}, \{\emptyset\}\}\)</span>, і в повному вигляді ймовірності подій складатимуть <span class="math inline">\(P(аверс) = 0.5, P(реверс) = 0.5, P({аверс, реверс}) = 0, P(\emptyset) = 0\)</span>.</p>
<p><strong><em>Аксіоматично</em></strong>, ймовірність можна визначити наступним чином: для простору елементарних подій <span class="math inline">\(S\)</span> й асоційованої сигма-алгебри множин <span class="math inline">\(\mathbb{B}\)</span>, функція ймовірності <span class="math inline">\(P\)</span> із доменом <span class="math inline">\(\mathbb{B}\)</span> задовільняє наступні вимоги</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(P(A) \geq 0 \text{ } \forall \text{ } A \in \mathbb{B}\)</span> (тобто ймовірність будь-якої події <span class="math inline">\(A\)</span> в просторі елементарних подій більше або дорівнює нулю),</p></li>
<li><p><span class="math inline">\(P(S) = 1\)</span> (тобто ймовірність цілого простору подій дорівнює одиниці),</p></li>
<li><p>якщо cкінченні події <span class="math inline">\(A_1, A_2, A_3, \cdots \in \mathbb{B}\)</span> є взаємовиключними (<span class="math inline">\(A_i \cap A_j = \emptyset \forall i \neq j\)</span>), тоді <span class="math inline">\(P(\cup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} P(A_i)\)</span> (тобто ймовірність всіх цих подій дорівнює сумі ймовірностей цих окремих подій).</p></li>
</ol>
<p>В такому випадку, уявімо наступне: (1) <span class="math inline">\(S = \{S_1, S_2, \cdots, \S_n\}\)</span>, (2) <span class="math inline">\(\mathbb{B}\)</span> – асоційована із <span class="math inline">\(S\)</span> сигма-алгебра, (3) <span class="math inline">\(p_1, p_2, \cdots, p_n\)</span> – не-негативні числа із сумою <span class="math inline">\(\sum_{i=1}^n p_i = 1\)</span>, і (4) для всякої події <span class="math inline">\(A \in \mathbb{B}\)</span> визначимо <span class="math inline">\(P(A) = \sum_{i:S_i \in A}(p_i)\)</span>. Тоді <span class="math inline">\(P\)</span> можна назвати ймовірнісною функцією визначеною в <span class="math inline">\(\mathbb{B}\)</span> якщо вона відповідає вимогам аксіоматичного визначення ймовірності (див. вище). Із такого визначення випливають наступні наслідки:</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(P(\emptyset) = 0\)</span>: ймовірність нульової множини (тобто відсутності події) становить нуль, якщо монетку вже підкинуто, то станеться або аверс, або реверс;</li>
<li>
<span class="math inline">\(P(A) \leq 1\)</span>: ймовірність події не може бути більшою за одиницю;</li>
<li>
<span class="math inline">\(P(A^c) = 1 - P(A) \Leftrightarrow P(A) + P(A^c) = 1 \Leftrightarrow A \cup A^c = S\)</span>: ймовірність комплементу події зворотно пов’язана із ймовірністю цієї події (якщо ймовірність викинути аверс становить <span class="math inline">\(0.3\)</span>, то ймовірність комплементу – тобто не викинути аверс – становить <span class="math inline">\(1-0.3\)</span>);</li>
<li>
<span class="math inline">\(P(B \cap A^c) = P(B) - P(B\cap A)\)</span> (з цього моменту пояснювати вербально стає складніше, читачу варто побавитись із колами Ейлера аби уявити про що йдеться);</li>
<li>
<span class="math inline">\(P(B \cup A) = P(B) + P(A) - P(B\cap A)\)</span>;</li>
<li>
<span class="math inline">\(A \subset B\)</span>, <span class="math inline">\(P(A) \leq P(B)\)</span>;</li>
<li>
<span class="math inline">\(P(B \cap A) \geq P(B) + P(A) - 1\)</span>.</li>
</ol>
<p>Коли йдеться про ймовірності, дуже важливим моментом є <strong>незалежність подій</strong> (<em>independence</em>) і пов’язані поняття. Дві події, <span class="math inline">\(A\)</span> і <span class="math inline">\(B\)</span>, вважаються незалежними якщо <span class="math inline">\(P(A \cap B) = P(A) P(B)\)</span>. Якщо <span class="math inline">\(A \cap B = \emptyset\)</span>, тобто ці події не мають жодних спільних елементів в просторі елементарних подій, то такі події можна описати як <strong>взаємовиключні</strong> (<em>mutually exclusive</em>; наприклад, одне підкидання монетки). Скінченна множина подій є <strong>попарно незалежною</strong> (<em>pairwise independence</em>) якщо для всіх пар справджується наступне: <span class="math inline">\(P(A_i \cap A_j) = P(A_i)P(A_j)\)</span>. Якщо ж кожна подія в множині незалежна від будь-яких перетинів всіх інших подій: <span class="math inline">\(P(\cap_{j=1}^k A_{i_j}) = \prod_{j=1}^k P(A_{i_j})\)</span>, тоді такі події можна назвати <strong>взаємонезалежними</strong> (<em>mutually independent</em>).</p>
</div>
<div id="bayes" class="section level3" number="3.3.2">
<h3>
<span class="header-section-number">3.3.2</span> Теорема Баєса<a class="anchor" aria-label="anchor" href="#bayes"><i class="fas fa-link"></i></a>
</h3>
<p>Уявімо дві події, <span class="math inline">\(A\)</span> і <span class="math inline">\(B\)</span>, котрі належать до <span class="math inline">\(S\)</span> (<span class="math inline">\(\{A, B\} \in S\)</span>), і, скажімо, <span class="math inline">\(P(B) &gt; 0\)</span>. Тоді ми можемо означити <strong>умовну ймовірність</strong> (<em>conditional probability</em>) події <span class="math inline">\(A\)</span> за того, що подія <span class="math inline">\(B\)</span> відбулась: <span class="math inline">\(P(A|B) = \frac{P(A \cap B)}{P(B)}\)</span>. Це доволі нескладно осягнути інтуїтивно. Скажімо, ми підкидаємо дві чесні монетки по черзі: <span class="math inline">\(B\)</span> позначає випадіння аверса з першою монеткою, <span class="math inline">\(A\)</span> позначає другий аверс. В цілому експерименті може статись чотири різні варіанти: аверс-аверс, аверс-реверс, реверс-аверс, і реверс-реверс. Ймовірність пари “аверс-аверс” складає <span class="math inline">\(P(A \cap B) = 1/4\)</span>. Ймовірність просто викинути реверс із першою монеткою становить <span class="math inline">\(P(B) = 1/2\)</span>. Тоді якщо ми припустимо, що перша монетка поверне аверс, ймовірність того що й друга монетка випаде на аверс становить <span class="math inline">\(P(A | B) = \frac{1/4}{1/2} = 1/2\)</span>.</p>
<p>Якщо ми знаємо, що <span class="math inline">\(P(A)&gt;0\)</span>, тоді можна побачити що <span class="math inline">\(P(B|A) = \frac{P(B \cap A)}{P(A)} = \frac{P(A | B)P(B)}{P(A)}\)</span>. Отже, <span class="math inline">\(P(B|A)P(A)=P(A|B)P(B)=P(A \cap B)\)</span>. Якщо продовжувати бавитись із підстановками в цих рівняннях, то вийде що <span class="math inline">\(P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B|A) P(A)}{P(B)} = \frac{P(B|A)P(A}{P(B \cap A) + P(B \cap A^c)} = \frac{P(B|A)P(A)}{P(B|A) P(A) + P(B|A^c)P(A^c)}\)</span>, що зветься Баєсівським правилом умовних ймовірностей і призводить до <strong>теореми Баєса</strong> (<em>Bayes theorem</em>).</p>
<p>Уявімо що <span class="math inline">\(\{A_1, A_2, A_3, \cdots\}\)</span> є поділом простору <span class="math inline">\(S\)</span> (<span class="math inline">\(A_i \cap A_j = \emptyset \forall i \neq j\)</span>, <span class="math inline">\(\cup_{k=1}^{\infty} A_k = S\)</span>). Уявіть будь-яку множину <span class="math inline">\(B\)</span>. Тоді</p>
<p><span class="math display">\[P(A_i|B) = \frac{P(B|A_i)P(A_i)}{\sum_{k=1}^{\infty}[P(B|A_k)P(A_k)]}\]</span></p>
<p>Певною мірою, цю теорему нескладно зрозуміти інтуїтивно, але іноді може видаватись навпаки. Для простого прикладу, уявімо що ми маємо список студентів з двох різних груп. В групі <span class="math inline">\(A\)</span> сумарно 80 студентів: 60 жінок і 20 чоловіків; в групі <span class="math inline">\(B\)</span> – 20 студентів, десятеро жінок і десятеро чоловіків. Ви обираєте випадкову особу із цих двох груп і бачите, що це чоловік. Які ймовірності того, що цей студент походить із певної групи? Ми бачимо що ймовірність обрати чоловіка з групи <span class="math inline">\(A\)</span> складає <span class="math inline">\(P(male|A) =  20/(60+20) = 1/4\)</span>, в той час як в групі <span class="math inline">\(B\)</span> – <span class="math inline">\(P(male|B) = 10/(20+20) = 1/2\)</span>. Але в той же час ймовірність обрати випадкову особу із групи <span class="math inline">\(A\)</span> становить <span class="math inline">\(P(A) = 80/(80+20) = 4/5\)</span>, в той час як з групи <span class="math inline">\(B\)</span> – <span class="math inline">\(P(B) = 20/(80+20) = 1/5\)</span>, і ми маємо врахувати ці ймовірності коли оцінюємо шукану ймовірність того, що наш студент походить із групи <span class="math inline">\(A\)</span>. Так, в цій групі небагато чоловіків, але й розмір групи великий, тож випадкова особа набагато ймовірніше потрапила із групи <span class="math inline">\(A\)</span>! Але навіть без оцінки всіх цих дрібних ймовірностей, у цілій вибірці сумарно <span class="math inline">\(30\)</span> чоловіків: <span class="math inline">\(20\)</span> походять із групи <span class="math inline">\(A\)</span>, <span class="math inline">\(10\)</span> – із групи <span class="math inline">\(B\)</span>. Отже, ймовірність обрати чоловіка із групи <span class="math inline">\(A\)</span> становитиме <span class="math inline">\(20/30\)</span>, із групи <span class="math inline">\(B\)</span> – <span class="math inline">\(10/30\)</span>. Ймовірності так само співпадають, наприклад, <span class="math inline">\(P(A|male) = \frac{P(male|A)P(A)}{P(male|A)P(A) + P(male|B)P(B)} = \frac{(20/80) \cdot (80/100)}{(20/80) \cdot (80/100) + (10/20) \cdot (20/100)} = \frac{0.2}{0.2+0.1}=2/3\)</span>.</p>
<p>Хоча й ця теорема не здається надто складною, вона надає цікавий погляд на процеси пізнання світу й <a href="numerical-ecology.html#paradigms">статистичний умовивід</a>. Одним із знаменитих прикладів є наступний уявний експеримент. Пацієнт здав кров на аналіз на якусь відносно непоширену хворобу (на неї хворіють, скажімо, <span class="math inline">\(1\%\)</span> популяції), і, на жаль, отримав позитивний результат. Чи це означає що наш пацієнт дійсно хворий на цю хворобу? Адже тести можуть помилятися. Відповідь, звісно, залежить від конкретних чисел, але, в цілому, нашому пацієнтові рано хнюпити носа. Скажімо, наш тест має точність <span class="math inline">\(95\%\)</span> в позитивних випадках (тобто із <span class="math inline">\(20\)</span> хворих пацієнтів один отримає негативний тест – отже, частота хибно-негативних результатів складає <span class="math inline">\(5\%\)</span>). Мало того, зрідка тест може помилятись і з негативними пацієнтами: наприклад, кожен десятий здоровий пацієнт отримає хибно-позитивний результат (частота хибно-позитивних результатів становить <span class="math inline">\(10\%\)</span>). Відтак, як би ситуація погано не виглядала для нашого пацієнта, яка ситуація є більш ймовірною: <strong><em>(1)</em></strong> пацієнт належить до <span class="math inline">\(1\%\)</span> всієї популяції і дійсно хворіє, тож тест надав істинний результат (ймовірність якого <span class="math inline">\(95\%\)</span>), або <strong><em>(2)</em></strong> пацієнт належить до <span class="math inline">\(99\%\)</span> популяції і є здоровим, а тест надав хибний результат (ймовірність чого <span class="math inline">\(10\%\)</span>)? Спробуйте застосувати теорему Баєса для оцінки цих постеріорних<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Поширеними термінами в цій сфері є &lt;strong&gt;пріорна, або апріорна, ймовірність&lt;/strong&gt; &lt;span class="math inline"&gt;\(P(A)\)&lt;/span&gt; (&lt;em&gt;prior&lt;/em&gt;) – така, яку можна спостерігати до оновлення нашого знання (все одно які результати тесту, ми і так знаємо скільки хворих в популяції), та &lt;strong&gt;постеріорна, або апостеріорна, ймовірність&lt;/strong&gt; &lt;span class="math inline"&gt;\(P(A|B)\)&lt;/span&gt; (&lt;em&gt;posterior&lt;/em&gt;) – оновлена ймовірність події за умови нашого пріорного знання.&lt;/p&gt;'><sup>17</sup></a> ймовірностей цих двох сценаріїв. (Іноді мені й самому потрібно задуматись із тим, куди у формулі підставляти котрі числа, тож, мабуть, вирішення подібних задач вимагає практики). А життєвий урок із цієї задачі наступний: якою б не видавалась ймовірність події, потрібно завжди враховувати наявне пріорне знання.</p>
<p>Розберемо теорему на запчастини із цим прикладом. Уявімо весь наш простір ймовірностей, який представляє велику популяцію пацієнтів (наприклад, <span class="math inline">\(10000\)</span> людей). У ньому апріорна ймовірність того, що випадковий пацієнт здоровий, складає <span class="math inline">\(P(\text{здоровий}) = 0.99\)</span>, й, відповідно, <span class="math inline">\(P(\text{хворий}) = 0.01\)</span>. Із опису якості тесту на захворювання ми знаємо, що із сотні хворих людей п’ятеро отримають хибно-негативний результат: <span class="math inline">\(P(\text{негативний|хворий}) = 0.05 \Leftrightarrow P(\text{позитивний|хворий}) = 0.95\)</span>. Водночас, коли ми перевіряємо здорових людей із цим не надто якісним тестом, то <span class="math inline">\(P(\text{негативний|здоровий}) = 0.9 \Leftrightarrow P(\text{позитивний|здоровий}) = 0.1\)</span>. Ми можемо прикинути розподіл пацієнтів за класами в такій десятитисячній популяції:</p>
<ul>
<li>
<p><span class="math inline">\(10000 \times P(\text{здоровий}) = 10000 \times 0.99 = 9900\)</span> здорових людей, з яких</p>
<ul>
<li><p><span class="math inline">\(9900 \times P(\text{позитивний|здоровий}) = 9900 \times 0.1 = 990\)</span> отримало позитивний тест,</p></li>
<li><p><span class="math inline">\(9900 \times P(\text{негативний|здоровий}) = 9900 \times 0.9 = 8910\)</span> отримало негативний тест,</p></li>
</ul>
</li>
<li>
<p><span class="math inline">\(10000 \times P(\text{хворий}) = 10000 \times 0.01 = 100\)</span> хворих людей, з яких</p>
<ul>
<li><p><span class="math inline">\(100 \times P(\text{позитивний|хворий}) = 100 \times 0.95 = 95\)</span> отримало позитивний тест,</p></li>
<li><p><span class="math inline">\(100 \times P(\text{негативний|хворий}) = 100 \times 0.05 = 5\)</span> отримало негативний тест.</p></li>
</ul>
</li>
</ul>
<p>В цій популяції <span class="math inline">\(990 + 95 = 1080\)</span> отримало позитивний тест, але ми явно бачимо що із людей з позитивним тестом більше здорових, аніж хворих! Отож і нашому пацієнту є над чим задуматись коли навіть із позитивним тестом він набагато ймовірніше є здоровим, аніж хворим.</p>
<p>Отже, яка ймовірність того, що пацієнт із позитивним тестом є хворим? Запросто, <span class="math inline">\(95/1080 \approx 0.088\)</span>, в той час ймовірність що він здоровий становить <span class="math inline">\(990/1080 \approx 0.912\)</span>. Як бачимо, набагато простіше оперувати одиницями пацієнтів, хоча із канонічним застосуванням Баєсівської формули вийде ідентичний результат, ніби пацієнти поскорочувались в рівняннях. Наприклад,</p>
<p><span class="math display">\[
\begin{aligned}
  P(\text{здоровий|позитивний}) = \\
  \frac{[P(позитивний|здоровий)] P(здоровий)}{[P(позитивний|здоровий) P(здоровий) + P(позитивний|хворий) P(хворий)]} = \\
  \frac{[0.1] \cdot 0.99}{[0.1 \cdot 0.99 + 0.95 \cdot 0.01]} = 0.099/0.1085 \approx 0.912
\end{aligned}
\]</span></p>
</div>
<div id="mle" class="section level3" number="3.3.3">
<h3>
<span class="header-section-number">3.3.3</span> Правдоподібність<a class="anchor" aria-label="anchor" href="#mle"><i class="fas fa-link"></i></a>
</h3>
<p>Іноді Баєсівське правило уявляють наступним чином:</p>
<p><span class="math display">\[\text{(постеріорна ймовірність)} = \frac{\text{[правдоподібність]} \times \text{(пріорна ймовірність)}}{\text{[свідчення]}}\]</span>
З попереднього розділу можна здогадатись що <strong>постеріорна ймовірність</strong> (<em>posterior</em>) – це ота шукана ймовірність події за наявного <strong>пріорного</strong> (<em>prior</em>) знання. В прикладі із попереднього підрозділу пріорною ймовірністю була ймовірність того, що пацієнт здоровий. Ця ймовірність відображала об’єктивну реальність незалежно від результатів тесту. <strong>Свідчення</strong> ще називають відособленою правдоподібністю (<em>marginal likelihood</em><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Переклад українською суто з Вікіпедії, мені загалом не звучить.&lt;/p&gt;"><sup>18</sup></a>), і воно відповідає оцьому дивному значенню <span class="math inline">\(0.1085\)</span> із попереднього прикладу – найпростіший шлях то думати про це як про якесь нормалізуюче значення яке просто <em>треба</em>. У відображенні того прикладу із кількостями людей же те значення відповідало <span class="math inline">\(1080\)</span>-тьом нещасним, котрі отримали позитивний результат тесту. Але що таке <strong>правдоподібність</strong> (<em>likelihood</em>)?</p>
<p>В попередньому прикладі на питання “отримав позитивний тест, чи пора вмирать?” можна відповісти без повного вирішення через Баєсівське рівняння. В ситуації <strong>(1)</strong> можна просто перемножити <span class="math inline">\(P(\text{позитивний|здоровий})P(\text{здоровий}) = 0.1 \cdot 0.99 = 0.099\)</span>, а в <strong>(2)</strong> – <span class="math inline">\(P(\text{позитивний|хворий})P(\text{хворий}) = 0.95 \cdot 0.01 = 0.0095\)</span>. Оці два результуючі значення є правдоподібностями, які відповідають певним ситуаціям. В технічному сенсі, правдоподібність є ймовірністю, але ця ймовірність має зміст лише у визначених обмежених підпросторах події, тож про правдоподібність простіше думати як про якесь безрозмірне значення яке пропорційне ймовірності якоїсь події. У цьому прикладі, <span class="math inline">\(0.099 &gt; 0.0095\)</span>, отже, ситуація <strong>(1)</strong> майже вдесятеро більш правдоподібна за ситуацію <strong>(2)</strong>. Отже, шановний пацієнте, ні, ваш результат тесту за даного контексту ще не кінець світу.</p>
<p>Поняття правдоподібності дуже корисне в підборі параметрів моделі. Зазвичай, в статистиці задача аналізу вибірки полягає в оцінці якогось параметру, однак на цю задачу можна дивитись і з протилежного боку: як оцінити наскільки вибірка правдоподібна за певного параметру? Наприклад, уявіть результат багаторазового (скажімо, <span class="math inline">\(n = 10\)</span>) підкидання монетки як якийсь вектор (наприклад, <span class="math inline">\(X = \{H, T, H, T, T, T, H, H, T, T\}\)</span>, або ж якщо випадіння аверсу позначити як одиницю, то <span class="math inline">\(X = \{1, 0, 1, 0, 0, 0, 1, 1, 0, 0\}\)</span>). Окреме випадіння аверсу є подією із розподілу Бернулі (див. <a href="numerical-ecology.html#pdfs">наступний розділ</a>), яке описується ймовірністю одиночної події <span class="math inline">\(p\)</span> (у нашому випадку – яка ймовірність випадіння аверсу за одного підкидання?). Приймемо функцію розподілу ймовірності (знову ж, дивись нижче що то таке) за <span class="math inline">\(f(x) = p^x (1-p)^{1-x}\)</span> (<span class="math inline">\(x \in X\)</span>, тобто тут ми позначаємо окреме спостереження як <span class="math inline">\(x\)</span> і вибірку як <span class="math inline">\(X\)</span>), тоді функція правдоподібності відповідатиме виразу</p>
<p><span class="math display">\[\mathcal{L}(p|X) = \prod \limits_{i = 1}^n p^{x_i} (1 - p)^{1-x_i}\]</span>
Очевидно, що ця функція приймає на вхід якусь фіксовану вибірку <span class="math inline">\(X\)</span> і пробігає по всіх можливим значенням параметру <span class="math inline">\(p\)</span>. Помічаєте як змінився підхід? Параметр вибірки не виглядає як якесь фіксоване значення, а радше як рухома ціль. Наше ж завдання – це знайти таке значення <span class="math inline">\(p\)</span>, <span class="math inline">\(\hat{p}\)</span>, за якого функція правдоподібності буде мати найбільше значення. Тоді ми можемо вважати оцінку (<em>estimate</em>) <span class="math inline">\(\hat{p}\)</span> такою, за якої отримати нашу вибірку <span class="math inline">\(X\)</span> видається най-<em>правдоподібніше</em>. Це іноді може бути непростим завданням для вирішення рівнянь, й до того ж значення правдоподібності дуже маленькі, особливо для малих вибірок (тому що ми множимо якісь значення ймовірностей <span class="math inline">\(p \leq 1\)</span> знову і знову). Тут можна використати один простий трюк: взяти логарифм цілої функції. Це несуттєво вплине на пошук максимального значення функції, адже приріст логарифмованої функції завжди відбувається в тому ж напрямку, що й вихідної функції. І отож, ми можемо визначити функцію лог-правдоподібності:</p>
<p><span class="math display">\[\ln \mathcal{L}(p|X) = \ln \left( \prod \limits_{i = 1}^n p^{x_i} (1 - p)^{1-x_i} \right) = \ln p \sum \limits_{i=1}^n x_i + \ln (1-p) \sum \limits_{i=1}^n (1 - x_i)\]</span></p>
<p>Аби знайти максимум цієї функції, можна спробувати знайти таке значення, за якого похідна функції лог-правдоподібності становитиме нуль (тобто відсутність приросту функції – це має бути або її максимум, або її мінімум). Наразі немає необхідності влазити в подальші деталі розрахунків<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;…але якщо дуже треба, то похідну можна знайти як &lt;span class="math inline"&gt;\(\frac{d \ln \mathcal{L} (p)}{d p} = \frac{\sum_{i=1}^n x_i}{p} - \frac{\sum_{i=1}^n (1 - x_i)}{1-p}\)&lt;/span&gt;, яку прирівнюємо до нуля і розв’язуємо для &lt;span class="math inline"&gt;\(p\)&lt;/span&gt;: &lt;span class="math inline"&gt;\(\frac{\sum_{i=1}^n x_i}{\hat{p}} = \frac{\sum_{i=1}^n (1 - x_i)}{1-\hat{p}}\)&lt;/span&gt; &lt;span class="math inline"&gt;\(\Rightarrow\)&lt;/span&gt; &lt;span class="math inline"&gt;\(\frac{\hat{p}}{1 - p} = \frac{\sum_{i=1}^n x_i}{\sum_{i=1}^n (1 - x_i)}\)&lt;/span&gt; &lt;span class="math inline"&gt;\(\Rightarrow\)&lt;/span&gt; &lt;span class="math inline"&gt;\(\hat{p} = (1 - \hat{p}) \frac{\sum_{i=1}^n x_i}{n - \sum_{i=1}^n x_i}\)&lt;/span&gt; &lt;span class="math inline"&gt;\(\Rightarrow\)&lt;/span&gt; &lt;span class="math inline"&gt;\(\hat{p} (n - \sum_{i=1}^n x_i) = (1 - \hat{p}) \sum_{i=1}^n x_i = \sum_{i=1}^n x_i - \hat{p} \sum_{i=1}^n x_i\)&lt;/span&gt; &lt;span class="math inline"&gt;\(\Rightarrow\)&lt;/span&gt; &lt;span class="math inline"&gt;\(\hat{p} (n - \sum_{i=1}^n x_i) + \hat{p} \sum_{i=1}^n x_i = \sum_{i=1}^n x_i\)&lt;/span&gt; &lt;span class="math inline"&gt;\(\Rightarrow\)&lt;/span&gt; &lt;span class="math inline"&gt;\(n\hat{p} = \sum_{i=1}^n x_i\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\hat{p} = \frac{\sum_{i=1}^n x_i}{n}\)&lt;/span&gt;. Краса та й годі!&lt;/p&gt;'><sup>19</sup></a>, але після прирівняння похідної до нуля можна перебудувати рівняння таким чином, аби з одного боку рівняння було лише <span class="math inline">\(p\)</span>. Це рівняння відповідатиме оцінці максимальної правдоподібності, у нашому випадку,</p>
<p><span class="math display">\[\hat{p} = \frac{1}{n} \sum \limits_{i=1}^n x_i\]</span></p>
<p>Іронічно, у випадку розподілу Бернулі оцінка максимальної правдоподібності дорівнює середньому арифметичному, тож <span class="math inline">\(\hat{p} = 0.4\)</span> для нашої вибірки <span class="math inline">\(X = \{1, 0, 1, 0, 0, 0, 1, 1, 0, 0\}\)</span>.</p>
<p>Найпростіше застосування методу пошуку оцінщика (<em>estimator</em>) максимальної правдоподібності застосовується в подібних ситуаціях, коли існує припущення щодо функції розподілу ймовірності вибірки, який дозволяє розписати функцію правдоподібності і шукати її максимум. В таких випадках моделлю є вибірка, і ми шукаємо параметр розподілу, за якого ця вибірка є найбільш правдоподібною. В складніших ситуаціях моделлю може бути власне щось, що ми розуміємо під поняттям <a href="numerical-ecology.html#stat-models">математичної моделі</a>, на кшталт регресії, а параметрів моделі може бути більш ніж один – і метод максимальної правдоподібності все одно працює… принаймні до моменту поки не вдасться знайти математика, котрий зможе аналітично знайти функцію лог-правдоподібності, взяти її похідну, і так далі.</p>
<p>Чи є правдоподібність ймовірністю? І так, і ні, і, скоріше, ні аніж так. Правдоподібність є <em>пропорційною</em> до ймовірності спостерігати змінну за певного значення параметру і є функцією цього параметру. Одна із вимог аксіоматичного визначення ймовірності каже, що інтеграл функції ймовірності повинен дорівнювати одиниці; у випадку правдоподібності як функції певного параметру, інтегрування цієї функції не завжди дорівнюватиме одиниці, відтак, за визначенням правдоподібність не є ймовірністю в таких випадках.</p>
</div>
</div>
<div id="pdf-pmf" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> Розподіли ймовірності<a class="anchor" aria-label="anchor" href="#pdf-pmf"><i class="fas fa-link"></i></a>
</h2>
<div id="pdfs" class="section level3" number="3.4.1">
<h3>
<span class="header-section-number">3.4.1</span> Функції розподілу ймовірності<a class="anchor" aria-label="anchor" href="#pdfs"><i class="fas fa-link"></i></a>
</h3>
<p>У прикладі із підкиданням монетки можливі два варіанти розвитку подій (іншими словами, із чого складається простір елементарних подій): аверс чи реверс. Відповідно, існує дві ймовірності, асоційовані із цими варіантами. Ці дві ймовірності утворюють <strong>розподіл ймовірності</strong> (<em>probability distribution</em>): певну функцію <span class="math inline">\(f\)</span> від кожного елементу <span class="math inline">\(x\)</span> простору елементарних подій (<span class="math inline">\(\Omega\)</span>, який виступає в якості домену функції <span class="math inline">\(f\)</span> – в цьому підрозділі <span class="math inline">\(\Omega\)</span> позначимо як <span class="math inline">\(\mathcal{X}\)</span>), яка повертає значення ймовірності. Коли уявити підкидання монетки як випадковий експеримент Бернулі, де <span class="math inline">\(1\)</span> відповідає випадінню аверсу (отже, <span class="math inline">\(\mathcal{X} = \{0, 1\}\)</span>), то функцію розподілу ймовірності можна розписати як <span class="math inline">\(f(x) = p^x (1-p)^{1-x}\)</span>, що легко скорочується в <span class="math inline">\(f(1) = p\)</span> і <span class="math inline">\(f(0) = 1 - p\)</span>.</p>
<p>Покликання функцій розподілу ймовірності – це фундаментальний опис ідеальних випадкових змінних <span class="math inline">\(X\)</span> за <span class="math inline">\(n \rightarrow \infty\)</span>. Очевидно, існує чимало розподілів окрім Бернулі, в яких змінні можна поділити на дискретні й континуальні. Фізичний зміст функції розподілу ймовірності дещо відрізняється в цих двох випадках.</p>
<ul>
<li><p><strong>Дискретний розподіл</strong> описує змінну, яка може набувати лише дискретних значень: наприклад, <span class="math inline">\(x \in \mathcal{X}: \mathcal{X} = \{0, 1, 2, 3, \cdots\}\)</span>, й, відповідно, функція розподілу ймовірності обчислюється як ймовірність випадково повернути конкретне значення у випадковій змінній. Функції розподілу ймовірності для дискретних змінних називають <strong>функціями маси ймовірності</strong> (<em>probability mass function</em>, <strong>pmf</strong>). Прикладами таких змінних може виступати кількість особин, кількість подій тощо (скільки не старатись, а уявити <span class="math inline">\(1.26\)</span> людей важко - це завжди має бути <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span>, або якесь інше ціле число). Відтак, pmf повертає ймовірність того, що випадкове значення із випадкової змінної <span class="math inline">\(X\)</span> дорівнюватиме певному значенню <span class="math inline">\(x\)</span>: <span class="math inline">\(P(X = x) = f(x)\)</span>.</p></li>
<li><p><strong>Неперервний, або континуальний розподіл</strong> описує неперервні змінні (вага, зріст тощо). Особливістю таких змінних є те, що їх точність завжди не є кінечною. Наприклад, якщо пацієнт зважується і отримує результат 70 кг, то чи це значить що вага становить рівно 70,000 г? Можливо, можна взяти точніший зважувальний апарат, який покаже вагу 70,130 г. Але і цей показник неточний: що якщо справжня вага дорівнює <span class="math inline">\(70131.76378 \ldots\)</span> г? І це ми ще не враховуємо калібрування та похибку інструментів. Подібно до цього прикладу, в неперервній змінній <span class="math inline">\(X\)</span> ймовірність повернути будь-яке конкретне значення неможливо оцінити точно, адже воно наближається до нуля (яка ймовірність обрати людину із багатомільярдного населення планети, вага якої становитиме рівно 70,131.76378 г? Мабуть, що ця ймовірність нікчемна). Відповідно, для опису розподілів таких змінних має зміст описувати радше ймовірність отримати випадкове значення, що менше або дорівнює до певного значення: а отже, яка частка значень із випадкової змінної <span class="math inline">\(X\)</span> менше або дорівнює певному значенню <span class="math inline">\(x\)</span>? В цих випадках <strong>функції густини ймовірності</strong> (<em>probability density function</em>, <strong>pdf</strong>) мають вигляд неперервних кривих, а шукані ймовірності знаходять інтегруванням (тобто шуканням площі під кривою) цих функцій: <span class="math inline">\(P(X \leq x) = \int \limits_{-\infty}^x f(x) dx\)</span>.</p></li>
</ul>
<p>В обох випадках для всякої випадкової змінної <span class="math inline">\(X\)</span> можна визначити функцію розподілу ймовірності як <strong>функцію кумулятивного розподілу</strong> (<em>cumulative distribution function</em>, <strong>cdf</strong>) <span class="math inline">\(F(x) = P(X \leq x) \forall x\)</span>. Цікавою особливістю cdf є те, що її можна визначити навіть для <span class="math inline">\(x \not\subset \mathcal{X}\)</span>: наприклад, для випадку із процесом Бернулі pmf може оцінити <em>тільки</em> ймовірність того, що <span class="math inline">\(x = 0\)</span> або <span class="math inline">\(x = 1\)</span>. Яка ймовірність того, що випаде <span class="math inline">\(0.7\)</span>? Доволі абсурдне питання, адже монетка не може випасти на <span class="math inline">\(70\%\)</span> аверсу і <span class="math inline">\(30\%\)</span> реверсу. Відтак, pmf неможливо визначити, однак cdf цілком можна: <span class="math inline">\(P (X \leq 0.7) = 1-p\)</span> (за віссю значення змінної <span class="math inline">\(X\)</span> кумулятивна ймовірність становитиме <span class="math inline">\(0\)</span> до моменту поки не <span class="math inline">\(X = 0\)</span>; <span class="math inline">\((1-p)\)</span> до моменту поки не <span class="math inline">\(X = 1\)</span>; з того ж моменту, всяке значення <span class="math inline">\(x \geq 1\)</span> матиме ймовірність того що <span class="math inline">\(X \leq x\)</span> становитиме <span class="math inline">\((1 - p) + p = 1\)</span>) (Рис. <a href="numerical-ecology.html#fig:fig-3-9">3.9</a>.). Функція <span class="math inline">\(F(x)\)</span> є cmf якщо вона відповідає наступним вимогам:</p>
<ul>
<li><p><span class="math inline">\(\lim \limits_{x \rightarrow - \infty} F(x) = 0\)</span>, <span class="math inline">\(\lim \limits_{x \rightarrow  \infty} F(x) = 1\)</span>,</p></li>
<li><p>приріст <span class="math inline">\(F(x)\)</span> не зменшується із <span class="math inline">\(x\)</span> (тобто для всяких <span class="math inline">\(u &gt; v\)</span> <span class="math inline">\(F(u) \geq F(v)\)</span>),</p></li>
<li><p><span class="math inline">\(F(x)\)</span> є право-неперервною, тобто <span class="math inline">\(\lim \limits_{h \downarrow 0} F(x + h) = F(x)\)</span>.</p></li>
</ul>
<div class="figure">
<span style="display:block;" id="fig:fig-3-9"></span>
<img src="bookdown-demo_files/figure-html/fig-3-9-1.png" alt="Функція маси ймовірності і функція кумулятивного розподілу для процесу Бернулі із $p = 0.7$: елементарного підкидання монетки із $70 \%$ ймовірністю випадіння аверсу." width="1056"><p class="caption">
Рис. 3.9: Функція маси ймовірності і функція кумулятивного розподілу для процесу Бернулі із <span class="math inline">\(p = 0.7\)</span>: елементарного підкидання монетки із <span class="math inline">\(70 \%\)</span> ймовірністю випадіння аверсу.
</p>
</div>
<p>Для дискретних змінних, pmf задана як <span class="math inline">\(f(x) = P(X = x) \forall x\)</span> і <span class="math inline">\(P(X = u) = 0\)</span> якщо <span class="math inline">\(u \notin \mathcal{X}\)</span>. Для трансформації pmf в cdf можна обчислити <span class="math inline">\(F(x) = P(X \leq x) = \sum \limits_{u: u \leq x} P(X = u) = \sum \limits_{u: u \leq x} f(u)\)</span>, а для протилежної трансформації cdf в pmf можна обчислити <span class="math inline">\(f(x) = F(u) - \lim \limits_{h \downarrow 0} F(u - h)\)</span>. Для неперервної ж змінної, <span class="math inline">\(f(u) = F(X = u) - \lim \limits_{h \downarrow 0} F(u - h) = 0\)</span>, адже <span class="math inline">\(P(X = u) = 0 \forall u\)</span>; cdf можна перевести в pdf як <span class="math inline">\(f(x) = \frac{d F(x)}{d x}\)</span>, і зворотньо pdf у cdf як <span class="math inline">\(F(x) = \int \limits_{u: u \leq x} f(u) du\)</span>.</p>
<p>Важливим параметром для опису будь-якої функції розподілу ймовірності є <strong>математичне очікування, або математичне сподівання</strong> (<em>expectation</em>). Для всякої функції <span class="math inline">\(f(x)\)</span>, математичне очікування можна знайти як середнє усіх можливих значень <span class="math inline">\(x: x \in \mathcal{X}\)</span> зважене за ймовірністю цих значень <span class="math inline">\(f(x)\)</span>: відтак, для дискретних розподілів <span class="math inline">\(X\)</span> математичне очікування змінної оцінюється як <span class="math inline">\(\mathbb{E}[X] = \sum \limits_{x \in \mathcal{X}} x f(x)\)</span>, а для неперервних – як <span class="math inline">\(\mathbb{E}[X] = \int \limits_{x \in \mathcal{X}} x f(x) dx\)</span>. Подібно до очікування змінної, можна шукати й очікування функції змінної <span class="math inline">\(g(x)\)</span>: <span class="math inline">\(\mathbb{E}[g(X)] = \sum \limits_{x \in \mathcal{X}} g(x) f(x)\)</span> або <span class="math inline">\(\mathbb{E}[g(X)] = \int \limits_{x \in \mathcal{X}} g(x) f(x) dx\)</span>. Якщо очікування існує (є такі функції, для яких неможливо аналітично вирішити суми чи інтеграли), воно матиме наступні властивості:</p>
<ul>
<li><p>якщо <span class="math inline">\(c\)</span> константа, то <span class="math inline">\(\mathbb{E}[c] = c\)</span>,</p></li>
<li><p>якщо <span class="math inline">\(c\)</span> константа і <span class="math inline">\(g()\)</span> – функція, то <span class="math inline">\(\mathbb{E}[c g(X)] = c \mathbb{E} [g(X)]\)</span>,</p></li>
<li><p>якщо <span class="math inline">\(c_1\)</span> та <span class="math inline">\(c_2\)</span> є константами і <span class="math inline">\(g_1()\)</span>, <span class="math inline">\(g_2()\)</span> – функціями, то <span class="math inline">\(\mathbb{E}[c_1 g_1 (X) + c_2 g_2(X)] = c_1 \mathbb{E}[g_1(X)] + c_2 \mathbb{E}[g_2(X)]\)</span>.</p></li>
</ul>
<p>Математичне очікування змінної <span class="math inline">\(\mathbb{E}[X]\)</span> ще часто називають <strong>середнім</strong> (не плутати із середнім арифметичним, яке є середнім для нормального розподілу і деяких інших розподілів). Іншим важливим окремим випадком математичного очікування функції розподілу її ймовірності є її <strong>варіація</strong> <span class="math inline">\(Var[X]\)</span> – очікування середньоквадратичного відхилення випадкової змінної від її середнього:</p>
<p><span class="math display">\[
\begin{aligned}
  Var[X] = \mathbb{E} \left[ (X - \mathbb{E}[X])^2 \right] = \mathbb{E} \left[ X^2 - 2X \mathbb{E} [X] + (\mathbb{E} [X])^2 \right] = \\
  \mathbb{E} [X^2] - 2 \mathbb{E}[X] \mathbb{E}[X] + (\mathbb{E}[X])^2 = \mathbb{E} [X^2] - 2 \mathbb{E}[X] \mathbb{E}[X] + \mathbb{E}[X] \mathbb{E}[X] = \\
  \mathbb{E}[X^2] - \mathbb{E}[X] \mathbb{E}[X] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
\end{aligned}
\]</span></p>
<p>Функції розподілу ймовірності можуть приймати будь-який вигляд<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;…особливо pdf: часто значення функції перевищує одиницю, що викликає справедливе питання “а як ймовірність може бути більша за одиницю?”. Не може, але й ця функція не повертає ймовірність. Її &lt;em&gt;інтеграл&lt;/em&gt; повертає ймовірність.&lt;/p&gt;"><sup>20</sup></a>. Розгляньмо окремі випадки поширених розподілів ймовірності. В усіх випадках ми кажемо що випадкова змінна <span class="math inline">\(X\)</span> походить із певного розподілу <span class="math inline">\(f(x)\)</span> позначенням <span class="math inline">\(X \sim \mathcal{A}(\theta, \cdots)\)</span> де <span class="math inline">\(\theta\)</span> є параметром розподілу <span class="math inline">\(\mathcal{A}\)</span>.</p>
<p>Нижче наведено формули розподілів ймовірності і, якщо доцільно, кумулятивних розподілів для окремих поширених статистичних розподілів. Також наведено значення середнього та варіації цих розподілів з точки зору параметрів і наявні шляхи обчислення <em>оцінки</em> параметрів (позначені як <span class="math inline">\(\hat{\theta}\)</span> для параметру <span class="math inline">\(\theta\)</span>) для вибірки <span class="math inline">\(x_i \in X\)</span> розміром <span class="math inline">\(n\)</span> <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Варто зауважити, що формули оцінщиків взяті із відкритих джерел, включно із постами блогів. Кожен розподіл має своєрідну ситуацію із оцінщиками, і виведення оцінщиків не завжди проводиться методом максимальної правдоподібності чи за допомогою момент-генеруючих функцій (ми не торкаємось цього методу), ба того, такі оцінщики іноді є упередженими (biased). Використовувати оцінщики параметрів необхідно із застереженнями!&lt;/p&gt;"><sup>21</sup></a>.</p>
<div id="дискретні-розподіли" class="section level4" number="3.4.1.1">
<h4>
<span class="header-section-number">3.4.1.1</span> Дискретні розподіли<a class="anchor" aria-label="anchor" href="#%D0%B4%D0%B8%D1%81%D0%BA%D1%80%D0%B5%D1%82%D0%BD%D1%96-%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB%D0%B8"><i class="fas fa-link"></i></a>
</h4>
<div id="розподіл-бернулі" class="section level5" number="3.4.1.1.1">
<h5>
<span class="header-section-number">3.4.1.1.1</span> Розподіл Бернулі<a class="anchor" aria-label="anchor" href="#%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB-%D0%B1%D0%B5%D1%80%D0%BD%D1%83%D0%BB%D1%96"><i class="fas fa-link"></i></a>
</h5>
<p>Розподіл Бернулі описує дискретну змінну із лише двома класами, відтак, яку можна представити як бінарну змінну: <span class="math inline">\(x \in \mathcal{X}: \mathcal{X} = \{0, 1\}\)</span>. Прикладом слугує підкидання монетки.</p>
<ul>
<li><p>Позначення: <span class="math inline">\(X \sim \mathcal{Bernoulli}(p)\)</span></p></li>
<li><p>pmf: <span class="math inline">\(f(x) = P(X = x) = p^x (1-p)^{x-1}\)</span>,</p></li>
<li><p>cdf: <span class="math inline">\(F(x) = P(X \leq x) = 0 \mathbb{I}_x(x &lt; 0)\)</span>, <span class="math inline">\(P(X \leq x) = (1 - p) \mathbb{I}_x(0 \leq x &lt; 1)\)</span>, <span class="math inline">\(P(X \leq x) = 1 \mathbb{I}_x(1 &lt; x)\)</span>.</p></li>
<li><p>середнє <span class="math inline">\(\mathbb{E} [X] = \frac{N+1}{2} = p\)</span>,</p></li>
<li><p>варіація <span class="math inline">\(Var[X] = p(1 - p)\)</span>,</p></li>
<li><p>оцінщик <span class="math inline">\(\hat{p} = \frac{1}{n} \sum \limits_{i=1}^{n}x_i\)</span>.</p></li>
</ul>
</div>
<div id="дискретний-рівномірний-розподіл" class="section level5" number="3.4.1.1.2">
<h5>
<span class="header-section-number">3.4.1.1.2</span> Дискретний рівномірний розподіл<a class="anchor" aria-label="anchor" href="#%D0%B4%D0%B8%D1%81%D0%BA%D1%80%D0%B5%D1%82%D0%BD%D0%B8%D0%B9-%D1%80%D1%96%D0%B2%D0%BD%D0%BE%D0%BC%D1%96%D1%80%D0%BD%D0%B8%D0%B9-%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB"><i class="fas fa-link"></i></a>
</h5>
<p>Дискретний рівномірний розподіл описує ситуацію, в якій кожна із дискретних величин (<span class="math inline">\(\mathcal{X} = \{1, 2, 3, \cdots, N\}\)</span>) має однакову ймовірність потрапити у вибірку. Прикладом можуть слугувати гральні кісточки.</p>
<ul>
<li><p>Позначення: <span class="math inline">\(X \sim \mathcal{DU}(N)\)</span> або <span class="math inline">\(X \sim \mathcal{DU}(a, b)\)</span> де <span class="math inline">\(N = b - a + 1 \text { } \forall b \geq a\)</span>,</p></li>
<li><p>pmf: <span class="math inline">\(f(x) = P(X = x) = \frac{1}{N} \mathbb{I}_x (x \in \{0, 1, 2, 3, \cdots, N\})\)</span>,</p></li>
<li><p>cdf: <span class="math inline">\(F(x) = P(X \leq x) = \frac{x - a + 1}{N}\)</span>,</p></li>
<li><p>середнє <span class="math inline">\(\mathbb{E} [X] = \frac{N+1}{2} = \frac{a+b}{2}\)</span>,</p></li>
<li><p>варіація <span class="math inline">\(Var[X] = \frac{N^2 - 1}{12}\)</span>,</p></li>
<li><p>оцінщик <span class="math inline">\(\hat{N} = \frac{n+1}{n} \max (x_i)\)</span>.</p></li>
</ul>
</div>
<div id="біноміальний-розподіл" class="section level5" number="3.4.1.1.3">
<h5>
<span class="header-section-number">3.4.1.1.3</span> Біноміальний розподіл<a class="anchor" aria-label="anchor" href="#%D0%B1%D1%96%D0%BD%D0%BE%D0%BC%D1%96%D0%B0%D0%BB%D1%8C%D0%BD%D0%B8%D0%B9-%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB"><i class="fas fa-link"></i></a>
</h5>
<p>Проведіть <span class="math inline">\(n\)</span> незалежних випадкових експериментів Бернулі: <span class="math inline">\(X \sim \mathcal{Bernoulli}(p)\)</span>. Якщо позначити <span class="math inline">\(y\)</span> як кількість успішних експериментів в <span class="math inline">\(X\)</span> із <span class="math inline">\(n\)</span> спроб, то <span class="math inline">\(Y\)</span> описуватиметься біноміальним розподілом.</p>
<ul>
<li><p>Позначення: <span class="math inline">\(Y \sim \mathcal{Binomial}(n, p)\)</span>,</p></li>
<li><p>pmf: <span class="math inline">\(f(Y) = P(Y = y) = \binom{n}{y} p^y (1-p)^{n - y} \mathbb{I}_y (y \in \{0, 1, 2, \cdots, n\})\)</span><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;…cdf біноміального розподілу існує, але його складно вивести і він все одно мало що скаже; варіацію біноміального розподілу також шукати відносно непросто.&lt;/p&gt;"><sup>22</sup></a>,</p></li>
<li><p>середнє <span class="math inline">\(\mathbb{E} [Y] = np\)</span> <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Обчислення очікування із біноміальними коефіцієнтами включає розкладання біному: &lt;span class="math inline"&gt;\((a + b)^n = \sum \limits_{x = 0}^n \binom{n}{x} a^x b^{n - x}\)&lt;/span&gt;.&lt;/p&gt;'><sup>23</sup></a>,</p></li>
<li><p>варіація <span class="math inline">\(Var[Y] = np(1-p)\)</span>,</p></li>
<li><p>оцінщик <span class="math inline">\(\hat{p} = \frac{y}{n}\)</span>.</p></li>
</ul>
</div>
<div id="геометричний-розподіл" class="section level5" number="3.4.1.1.4">
<h5>
<span class="header-section-number">3.4.1.1.4</span> Геометричний розподіл<a class="anchor" aria-label="anchor" href="#%D0%B3%D0%B5%D0%BE%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D1%87%D0%BD%D0%B8%D0%B9-%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB"><i class="fas fa-link"></i></a>
</h5>
<p>Уявіть повторення випадкового експерименту Бернулі із параметром <span class="math inline">\(p\)</span> до того, поки не випаде перший успіх. В такому випадку, можна розрахувати кількість безуспішних спроб <span class="math inline">\(x\)</span> до першого успіху та кількість спроб <span class="math inline">\(y\)</span> потрібних для першого успіху. Обидві змінні описуються геометричним розподілом.</p>
<ul>
<li>pmf:</li>
</ul>
<p><span class="math display">\[f(x) = P(X = x) = p(1 - p)^x \mathbb{I}_x (0, 1, 2, \cdots, \infty)\]</span></p>
<p><span class="math display">\[f(y) = P(Y = y) = p(1-p)^{y-1} \mathbb{I}_y (0, 1, 2, \cdots, \infty)\]</span></p>
<ul>
<li>середні</li>
</ul>
<p><span class="math display">\[\mathbb{E} [X] = \frac{1-p}{p}\]</span></p>
<p><span class="math display">\[\mathbb{E} [Y] = \frac{1}{p}\]</span></p>
<ul>
<li>варіації</li>
</ul>
<p><span class="math display">\[Var[X] = \frac{1-p}{p^2}\]</span></p>
<p><span class="math display">\[Var[Y] = \frac{1-p}{p^2}\]</span></p>
<ul>
<li>оцінщик <span class="math inline">\(\hat{p} = \frac{1}{x}\)</span>.</li>
</ul>
</div>
<div id="негативний-біноміальний-розподіл" class="section level5" number="3.4.1.1.5">
<h5>
<span class="header-section-number">3.4.1.1.5</span> Негативний біноміальний розподіл<a class="anchor" aria-label="anchor" href="#%D0%BD%D0%B5%D0%B3%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B8%D0%B9-%D0%B1%D1%96%D0%BD%D0%BE%D0%BC%D1%96%D0%B0%D0%BB%D1%8C%D0%BD%D0%B8%D0%B9-%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB"><i class="fas fa-link"></i></a>
</h5>
<p>Негативний біноміальний розподіл описує <span class="math inline">\(X\)</span> як кількість невдач перед <span class="math inline">\(r\)</span>-тим успіхом в серії випадкових експериментів Бернулі із параметром <span class="math inline">\(p\)</span>.</p>
<ul>
<li><p>Позначення: <span class="math inline">\(X \sim \mathcal{NBinom} (r, p)\)</span>,</p></li>
<li><p>pmf: <span class="math inline">\(f(x) = P(X = x) = \binom{\text{спроби}}{\text{успіхи} + x \text{ невдач}} \cdot \text{невдачі} \cdot \text{успіхи} = \binom{x + r - 1}{r-1} (1-p) ^x p^r\)</span>,</p></li>
<li><p>середнє <span class="math inline">\(\mathbb{E} [X] = \frac{r(1-p)}{p}\)</span>,</p></li>
<li><p>варіація <span class="math inline">\(Var[X] = \frac{r(1-p)}{p^2}\)</span>,</p></li>
<li><p>оцінщик <span class="math inline">\(\hat{p} = \frac{r-1}{r + x - 1}\)</span>.</p></li>
</ul>
<p>Примітно, що геометричний розподіл є окремим випадком негативного біноміального розподілу. Якщо <span class="math inline">\(Y\)</span> – кількість спроб для отримання <span class="math inline">\(r\)</span> успіхів, то <span class="math inline">\(Y = X + r\)</span>, <span class="math inline">\(P(Y = y) = \binom{y-1}{r-1} (1-p)^{y-r} p^r\)</span>. <span class="math inline">\(\mathbb{E}[Y] = \mathbb{E}[X] + r\)</span>, <span class="math inline">\(Var[Y] = Var[X]\)</span>.</p>
</div>
<div id="розподіл-пуасона" class="section level5" number="3.4.1.1.6">
<h5>
<span class="header-section-number">3.4.1.1.6</span> Розподіл Пуасона<a class="anchor" aria-label="anchor" href="#%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB-%D0%BF%D1%83%D0%B0%D1%81%D0%BE%D0%BD%D0%B0"><i class="fas fa-link"></i></a>
</h5>
<p>Мабуть, один із найменш інтуїтивно зрозумілих але найбільш поширених розподілів. Він описує кількість подій, які відбуваються протягом визначеного вікна в просторі, при тому що всі події є незалежними один від одного. Існує чимало прикладів процесів Пуасона, наприклад, кількість людей на платформі залізничної станції в момент часу чи кількість особин популяції зареєстрованих на ділянці.</p>
<ul>
<li><p>Позначення: <span class="math inline">\(X \sim \mathcal{Poisson}(\lambda)\)</span></p></li>
<li><p>pmf: <span class="math inline">\(f(x) = P(X = x) = e^{-\lambda} \frac{\lambda^x}{x!}\)</span>, де <span class="math inline">\(x \in \{0, 1, 2, 3, \cdots, \infty \}\)</span>, <span class="math inline">\(\lambda &gt; 0\)</span>,</p></li>
<li><p>середнє <span class="math inline">\(\mathbb{E} [X] = \lambda\)</span>,</p></li>
<li><p>варіація <span class="math inline">\(Var[X] = \lambda\)</span>,</p></li>
<li><p>оцінщик <span class="math inline">\(\hat{\lambda} = \frac{1}{n} \sum \limits_{i=1}^{n}x_i\)</span>.</p></li>
</ul>
<p>Цікаво, що <span class="math inline">\(Y \sim \mathcal{Binomial}(n, p)\)</span> із не-екстремальним значенням <span class="math inline">\((np)\)</span> та дуже значним <span class="math inline">\(n\)</span> може бути апроксимована до розподілу Пуасона із <span class="math inline">\(\lambda \approx np\)</span>.</p>
</div>
<div id="гіпергеометричний-розподіл" class="section level5" number="3.4.1.1.7">
<h5>
<span class="header-section-number">3.4.1.1.7</span> Гіпергеометричний розподіл<a class="anchor" aria-label="anchor" href="#%D0%B3%D1%96%D0%BF%D0%B5%D1%80%D0%B3%D0%B5%D0%BE%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D1%87%D0%BD%D0%B8%D0%B9-%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB"><i class="fas fa-link"></i></a>
</h5>
<p>Уявіть зліченну популяцію розміром <span class="math inline">\(N\)</span> із об’єктів, що належать до різних класів, зокрема, в якій існує <span class="math inline">\(K: K \leq N\)</span> об’єктів із певною характеристикою (наприклад, особини виду, в якому ми зацікавлені, в угрупованні різних видів<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Застосування гіпергеометричного розподілу в реальних задачах екології угруповань може бути складним, адже чисельності видів можуть сягати сотень і тисяч, й обчислення біноміальних коефіцієнтів видасть дуже великі числа – іноді настільки великі, що комп’ютер не може їх обчислити і зве безкінечністю. Аби обійти обмеження стандартної комп’ютерної архітектури в нагоді можуть стати функції із бібліотеки &lt;code&gt;gmp&lt;/code&gt; для R.&lt;/p&gt;"><sup>24</sup></a>). Тоді можна очікувати, що вибірка розміром <span class="math inline">\(n\)</span> із цілої популяції міститиме <span class="math inline">\(x\)</span> об’єктів із шуканого класу.</p>
<ul>
<li><p>Позначення: <span class="math inline">\(X \sim \mathcal{HyperGeom}(N, K, n)\)</span>,</p></li>
<li><p>pmf: <span class="math inline">\(f(x) = P(X = x) = \frac{\binom{N}{K} \binom{N-K}{n - x}}{\binom{N}{n}}\)</span>,</p></li>
<li><p>середнє <span class="math inline">\(\mathbb{E} [X] = n \frac{K}{N}\)</span>,</p></li>
<li><p>оцінщик для <a href="https://math.stackexchange.com/questions/40319/maximum-likelihood-estimate-of-hypergeometric-distribution-parameter">випадків апроксимації до розподілу Пуасона</a> де <span class="math inline">\(K/N &lt;&lt; 1, n &gt;&gt; 1\)</span>: <span class="math inline">\(\hat{m} = \frac{N \sum \limits_i^T x_i}{Tn}\)</span> для вибірки <span class="math inline">\(x_i \in X\)</span> розміром <span class="math inline">\(T\)</span>.</p></li>
</ul>
</div>
<div id="нуль-упереджені-моделі" class="section level5" number="3.4.1.1.8">
<h5>
<span class="header-section-number">3.4.1.1.8</span> Нуль-упереджені моделі<a class="anchor" aria-label="anchor" href="#%D0%BD%D1%83%D0%BB%D1%8C-%D1%83%D0%BF%D0%B5%D1%80%D0%B5%D0%B4%D0%B6%D0%B5%D0%BD%D1%96-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%96"><i class="fas fa-link"></i></a>
</h5>
<p>Доволі цікавою родиною розподілів, котрі часто застосовують в екологічних дослідженнях, є “нуль-упереджені”, або “нуль-надуті” моделі (zero-inflated models). Такі моделі описують розподіли, в котрих значна частка спостережених значень припадає на нулі. Такі розподіли можуть описувати спостережені чисельності виду у вибірці спостережень якщо, зазвичай, ми не спостерігаємо вид (відповідно, чисельність дорівнює нулю), але якщо спостерігаємо, то чисельність відповідає якомусь позитивному цілому значенню. Відтак, якщо ігнорувати всі нульові спостереження, то чисельність описуватиметься якимось симпатичним дискретним розподілом, однак, нульові спостереження не можна просто так ігнорувати.</p>
<p>Насправді, нуль-упереджені моделі є комбінацією двох розподілів: один генерує нулі, в той час як інший генерує позитивні дискретні значення. Найбільш поширеним розподілом в цій родині є нуль-упереджений розподіл Пуасона (zero-inflated Poisson, ZIP): нуль-генеруюча частина цього процесу визначає чи випадкове значення дорівнюватиме нулю, і якщо ні, тоді випадкове значення отримується із звичайного розподілу Пуасона (який також може генерувати нулі, але набагато менше). Відтак, ZIP матиме два параметри: (1) ймовірність того, що нуль-генеруюча функція поверне нуль (<span class="math inline">\(p\)</span>), та (2) параметр розподілу Пуасона (<span class="math inline">\(\lambda\)</span>). Математично, такий розподіл можна визначити як <span class="math inline">\(X \sim \mathcal{ZIP}(p, \lambda)\)</span></p>
<p><span class="math display">\[
P(X = x_i) =
\begin{cases}
P(X = 0) = p + (1-p)e^{-\lambda}\\
P(X = x_i) = (1 - p) \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} \text{ } \forall \text{ } x_i = \{1, 2, 3, \cdots\}
\end{cases}
\]</span></p>
<ul>
<li><p>середнє <span class="math inline">\(\mathbb{E} [X] = (1-p) \lambda\)</span>,</p></li>
<li><p>варіація <span class="math inline">\(Var[X] = (1-p)\lambda \cdot (1+p \lambda)\)</span>,</p></li>
<li><p>оцінщики <span class="math inline">\(\hat{\lambda} = \frac{s^2 + \bar{x}^2}{\bar{x}} - 1\)</span>, <span class="math inline">\(\hat{p} = \frac{s^2 - \bar{x}}{s^2 + \bar{x}^2 - \bar{x}}\)</span> де <span class="math inline">\(\bar{x} = \frac{1}{n} \sum x_i\)</span>, <span class="math inline">\(s^2 = \frac{1}{n - 1} \sum (x_i - \bar{x})^2\)</span>.</p></li>
</ul>
</div>
</div>
<div id="континуальні-розподіли" class="section level4" number="3.4.1.2">
<h4>
<span class="header-section-number">3.4.1.2</span> Континуальні розподіли<a class="anchor" aria-label="anchor" href="#%D0%BA%D0%BE%D0%BD%D1%82%D0%B8%D0%BD%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D1%96-%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB%D0%B8"><i class="fas fa-link"></i></a>
</h4>
<div id="рівномірний-розподіл" class="section level5" number="3.4.1.2.1">
<h5>
<span class="header-section-number">3.4.1.2.1</span> Рівномірний розподіл<a class="anchor" aria-label="anchor" href="#%D1%80%D1%96%D0%B2%D0%BD%D0%BE%D0%BC%D1%96%D1%80%D0%BD%D0%B8%D0%B9-%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB"><i class="fas fa-link"></i></a>
</h5>
<p>Рівномірний розподіл описує ситуацію, коли будь-яке значення <span class="math inline">\(x\)</span> між <span class="math inline">\(a\)</span> і <span class="math inline">\(b\)</span> має рівну ймовірність: <span class="math inline">\(P(a \leq X \leq b) = 1\)</span>.</p>
<ul>
<li><p>Позначення: <span class="math inline">\(X \sim \mathcal{U}(a, b)\)</span>,</p></li>
<li><p>pdf: <span class="math inline">\(f(x) = \frac{1}{b-a} \mathbb{I}_x (a \leq x \leq b)\)</span> (примітно, що функція є константою),</p></li>
<li><p>cdf: <span class="math inline">\(F(x) = P(X \leq x) = \frac{x-a}{b-a} \text { } \forall (a \leq x \leq b)\)</span>, але <span class="math inline">\(F(X) = 0 \mathbb{I}_x(x &lt; 0)\)</span> і <span class="math inline">\(F(X) = 1 \mathbb{I}_x (1 &lt; x)\)</span>,</p></li>
<li><p>середнє <span class="math inline">\(\mathbb{E} [X] = \frac{a+b}{2}\)</span>,</p></li>
<li><p>варіація <span class="math inline">\(Var[X] = \frac{(b-a)^2}{12}\)</span>,</p></li>
<li><p>оцінщики <span class="math inline">\(\hat{a} = \min(x_i), \hat{b} = \max(x_i)\)</span>.</p></li>
</ul>
</div>
<div id="бета-розподіл" class="section level5" number="3.4.1.2.2">
<h5>
<span class="header-section-number">3.4.1.2.2</span> Бета-розподіл<a class="anchor" aria-label="anchor" href="#%D0%B1%D0%B5%D1%82%D0%B0-%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB"><i class="fas fa-link"></i></a>
</h5>
<p>Доволі різноманітна родина розподілів із формою функції, яка контролюється двома параметрами, <span class="math inline">\(a\)</span> і <span class="math inline">\(b\)</span>. Щодо цього розподілу, мабуть, варто просто знати про його існування. Його іноді застосовують в популяційній генетиці <a href="https://doi.org/10.1007%2FBF01441146">(Balding &amp; Nichols 1995)</a> та Баєсівському аналізі <a href="https://doi.org/10.1109/MFI.2016.7849531">(Jøsang 2016)</a>.</p>
<ul>
<li><p>Позначення: <span class="math inline">\(X \sim \mathcal{Beta}(a, b)\)</span>,</p></li>
<li><p>pdf: <span class="math inline">\(f(x) \propto x^{a - 1} (1-x)^{b-1} \mathbb{I}_x(0 &lt; x &lt; 1)\)</span>, <span class="math inline">\(f(x) = \frac{\Gamma (a + b)}{\Gamma (a) \Gamma (b)} x^{a - 1} (1 - x)^{b - 1}\)</span>, де <span class="math inline">\(\Gamma()\)</span> - гамма-функція <span class="math inline">\(\Gamma(\alpha) = \int \limits_0^{\infty} u^{\alpha - 1} e^{-u} du\)</span>.</p></li>
<li><p>середнє <span class="math inline">\(\mathbb{E} [X] = \frac{a}{a + b}\)</span>,</p></li>
<li><p>варіація <span class="math inline">\(Var[X] = \frac{ab}{(a+b)^2 (a+b+1)}\)</span>.</p></li>
</ul>
</div>
<div id="експоненційний-розподіл" class="section level5" number="3.4.1.2.3">
<h5>
<span class="header-section-number">3.4.1.2.3</span> Експоненційний розподіл<a class="anchor" aria-label="anchor" href="#%D0%B5%D0%BA%D1%81%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%86%D1%96%D0%B9%D0%BD%D0%B8%D0%B9-%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB"><i class="fas fa-link"></i></a>
</h5>
<p>Експоненційний розподіл є своєрідним неперервним аналогом геометричного розподілу і описує відстань між незалежними неперервними подіями, які відбуваються із постійним темпом. Розподіл темпів смертності в природних популяціях нагадує експоненційний <a href="https://doi.org/10.1016/0022-5193(79)90098-5">(Abernethy 1979)</a>.</p>
<ul>
<li><p>Позначення: <span class="math inline">\(X \sim \mathcal{Exp}(\lambda)\)</span>,</p></li>
<li><p>pdf: <span class="math inline">\(f(x) = \frac{1}{\lambda} e^{-\frac{x}{\lambda}} \mathbb{I}_x (0 \leq x \leq \infty)\)</span>,</p></li>
<li><p>cdf: <span class="math inline">\(F(x) = 1 - e^{-\frac{x}{\lambda}}\)</span>,</p></li>
<li><p>середнє <span class="math inline">\(\mathbb{E} [X] = \lambda\)</span>,</p></li>
<li><p>варіація <span class="math inline">\(Var[X] = \lambda^2\)</span>,</p></li>
<li><p>оцінщик <span class="math inline">\(\hat{(\frac{1}{\lambda})} = \frac{1}{n} \sum x_i\)</span> із упередженням, або <span class="math inline">\(\hat{(\frac{1}{\lambda})} = \frac{n-2}{\sum x_i}\)</span>.</p></li>
</ul>
<p>Цікавою властивістю експоненційного процесу є відсутність пам’яті: ймовірність вижити в наступний момент часу за умови виживання до цього моменту дорівнює ймовірності вижити в будь-який момент часу (<span class="math inline">\(P(X &gt; (s+t)|X &gt; s) = P(X &gt; t)\)</span>).</p>
<p>Особливим випадком експоненційного розподілу є двопараметричний <strong>зміщений експоненційний розподіл</strong>: <span class="math inline">\(f(x) = \frac{1}{\lambda} e^{-\frac{x - \mu}{\lambda}} \mathbb{I}_x (\mu \leq x \leq \infty)\)</span>, для якого середнє дорівнює <span class="math inline">\(\mathbb{E}[X] = \mu + \lambda\)</span>.</p>
</div>
<div id="norm-dirstr" class="section level5" number="3.4.1.2.4">
<h5>
<span class="header-section-number">3.4.1.2.4</span> Нормальний розподіл<a class="anchor" aria-label="anchor" href="#norm-dirstr"><i class="fas fa-link"></i></a>
</h5>
<p>Мабуть, найбільш знаменитий розподіл, яким можна описати чимало змінних в біології: розміри листків рослин, зріст людей певного віку тощо. Його логіка доволі проста і каже що змінна <span class="math inline">\(X\)</span> матиме значення <span class="math inline">\(x\)</span>, які концентруються навколо якогось середнього значення <span class="math inline">\(\bar{x}\)</span>, і чим сильніше <span class="math inline">\(x\)</span> відрізняються від <span class="math inline">\(\bar{x}\)</span>, тим менш поширеними вони будуть. Цьому розподілу варто приділити дещо більше уваги, аніж іншим.</p>
<ul>
<li><p>Позначення: <span class="math inline">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>, де параметр <span class="math inline">\(\mu: \{-\infty \leq \mu \leq \infty\}\)</span> відповідає середньому значенню (а також <strong><em>медіані</em></strong> <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;strong&gt;Медіана&lt;/strong&gt; – значення в сортованій послідовності випадкової змінної &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;, яке розділяє цю послідовність на дві частини однакового розміру; медіана тісно пов’язана із поняттями &lt;strong&gt;перцентилів&lt;/strong&gt; &lt;span class="math inline"&gt;\(x_{\alpha}\)&lt;/span&gt; – такими значеннями &lt;span class="math inline"&gt;\(x\)&lt;/span&gt;, які більше за частку &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt; випадкової змінної &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; (наприклад, перцентиль &lt;span class="math inline"&gt;\(x_{\alpha = 0.95}\)&lt;/span&gt; – це таке значення, що в змінній &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; &lt;span class="math inline"&gt;\(95 \%\)&lt;/span&gt; значень менше або дорівнюють &lt;span class="math inline"&gt;\(x_{\alpha = 0.95}\)&lt;/span&gt;). Медіана відповідає перцентилю із &lt;span class="math inline"&gt;\(\alpha = 0.5\)&lt;/span&gt;.&lt;/p&gt;'><sup>25</sup></a> та <strong><em>моді</em></strong> <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;&lt;strong&gt;Мода&lt;/strong&gt; – найбільш поширене значення у вибірці.&lt;/p&gt;"><sup>26</sup></a>), а параметр <span class="math inline">\(\sigma^2: \sigma &gt; 0\)</span> відповідає варіації в розподілі, що виражається в ширині характерної куполоподібної кривої.</p></li>
<li><p>pdf: <span class="math inline">\(f(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\left[ \frac{(x - \mu)^2}{2 \sigma^2} \right]}\)</span>,</p></li>
<li><p>середнє <span class="math inline">\(\mathbb{E} [X] = \mu\)</span>,</p></li>
<li><p>варіація <span class="math inline">\(Var[X] = \sigma^2\)</span>.</p></li>
</ul>
<p>Хорошою демонстрацією методу <a href="numerical-ecology.html#mle">максимальної правдоподібності</a> є пошук параметрів із такої вибірки <span class="math inline">\(X\)</span> що <span class="math inline">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>. Знайдемо функцію правдоподібності:</p>
<p><span class="math display">\[\mathcal{L}(X | \mu, \sigma^2) = \prod \limits_{i=1}^n f(x_i| \mu, \sigma^2) = \prod \limits_{i=1}^n \frac{1}{\sqrt{2 \pi} \sigma} e^{-\left[ \frac{(x - \mu)^2}{2 \sigma^2} \right]} = \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^n \exp \left[ - \frac{\sum \limits_{i=1}^n (x_i - \mu)}{2 \sigma^2} \right]\]</span></p>
<p>Оскільки бавитись з такою формулою виглядає тією ще задачею, візьмемо логарифм правдоподібності:</p>
<p><span class="math display">\[\ln \mathcal{L}(X | \mu, \sigma^2) = - \frac{n}{2} \ln{(2 \pi \sigma^2)} - \frac{\sum \limits_{i=1}^n (x_i - \mu)}{2 \sigma^2}\]</span></p>
<p>Знайдемо оцінщик <span class="math inline">\(\hat{\mu}\)</span> першим. Для цього потрібно продиференціювати попередній вираз відносно <span class="math inline">\(\mu\)</span> і прирівняти його до нуля:</p>
<p><span class="math display">\[\frac{\partial \ln \mathcal{L}(X | \mu, \sigma^2)}{\partial \mu} = \frac{\sum \limits_{i=1}^n (x_i - \mu)}{\sigma^2} = 0\]</span>
Відтак, рішення</p>
<p><span class="math display">\[\sum \limits_{i=1}^n (x_i - \mu) = 0 \Rightarrow \sum \limits_{i=1}^n x_i - n \mu = 0 \Rightarrow \hat{\mu} = \frac{\sum \limits_{i=1}^n x_i}{n}\]</span>
Що ніщо інше як середнє арифметичне. Щодо іншого параметру, <span class="math inline">\(\sigma^2\)</span>,</p>
<p><span class="math display">\[\frac{\partial \ln \mathcal{L}(X | \mu, \sigma^2)}{\partial \sigma^2} = - \frac{n}{2} \cdot \frac{2 \pi}{2 \pi \sigma^2} + \frac{\sum \limits_{i=1}^n (x_i - \mu)^2}{2 (\sigma^2)^2} = 0 \Rightarrow -n \sigma^2 + \sum \limits_{i=1}^n (x_i - \mu)^2 = 0\]</span></p>
<p>Можна підставити <span class="math inline">\(\hat{\mu} = \frac{1}{n} \sum \limits_{i=1}^n x_i = \bar{x}\)</span> і отримати</p>
<p><span class="math display">\[\hat{\sigma^2} = \frac{1}{n} \sum \limits_{i=1}^n (x_i - \bar{x})^2\]</span></p>
<p>Якщо читач дещо обізнаний в базовій статистиці, то можна помітити що цією формулою <em>не</em> користуються для оцінки дисперсії у вибірках із нормального розподілу. Все через те, що такий оцінщик не проходить перевірку на упередженість (bias: оцінщик вважається неупередженим якщо <span class="math inline">\(\mathbb{E}[\hat{\theta}] = \theta\)</span>): якщо переформулювати (подано без покрокових обчислень, можна перевірити якщо пам’ятати що <span class="math inline">\(\frac{1}{n} \sum \limits_{i=1}^n x_i \equiv \bar{x}\)</span>)</p>
<p><span class="math display">\[\hat{\sigma^2} = \frac{1}{n} \sum \limits_{i=1}^n (x_i - \bar{x})^2 = \frac{1}{n} \sum \limits_{i=1}^n x_i^2 - (\bar{x})^2\]</span></p>
<p>Тоді (пам’ятаючи що <span class="math inline">\(\sigma^2 = Var[X] = \mathbb{E} [X^2] - (\mathbb{E} [X])^2 = \mathbb{E} [X^2] - \mu^2\)</span>)</p>
<p><span class="math display">\[
\begin{aligned}
  \mathbb{E} [\hat{\sigma^2}] = \mathbb{E} \left[ \frac{1}{n} \sum \limits_{i=1}^n x_i^2 - (\bar{x})^2 \right] = \frac{1}{n} \sum \limits_{i = 1}^n \mathbb{E} [x_i^2] - \mathbb{E} [\bar{x}^2] = \\
  \frac{1}{n}  \sum \limits_{i = 1}^n (\sigma^2 + \mu^2) - (\frac{\sigma^2}{n} + \mu^2) = (1 - \frac{1}{n}) \sigma^2 \neq \sigma^2
\end{aligned}
\]</span></p>
<p>Натомість, неупередженим оцінщиком <span class="math inline">\(\hat{\sigma^2}\)</span> є щось, що називають <strong>дисперсією вибірки</strong>: <span class="math inline">\(\hat{\sigma^2} = s^2 = \frac{1}{n-1} \sum \limits_{i=1}^n (x_i - \bar{x})^2\)</span>. Аби уникнути термінологічної плутанини (а вона чомусь завжди наявна в оцінках варіації), варто визначити наступні дескриптори варіації, що часто застосовуються до вибірок із нормальним розподілом (використовуйте тест Шапіро-Вілка (Shapiro-Wilk test) для перевірки нормальності вибірки; <span class="math inline">\(p &gt; 0.05\)</span>, на відміну від більшості статистичних тестів, є підставою вважати вибірку нормально розподіленою):</p>
<ul>
<li><p><strong>дисперсія</strong> (<em>sample variance</em>) є неупередженим оцінщиком параметру <span class="math inline">\(\sigma^2\)</span> нормального розподілу у вибірці: <span class="math inline">\(s^2 = \frac{1}{n-1} \sum \limits_{i=1}^n (x_i - \bar{x})^2\)</span>, якому в R відповідає функція <code><a href="https://rdrr.io/r/stats/cor.html">var()</a></code>,</p></li>
<li><p><strong>середньоквадратичне відхилення</strong> описує варіацію вибірки незалежно від її розподілу: <span class="math inline">\(v^2 = \frac{1}{n} \sum \limits_{i=1}^n (x_i - \bar{x})^2\)</span>; однак набагато частіше під цим терміном (як і будемо ми) розуміють кориговане <strong>стандартне відхилення</strong> (<em>standard deviation</em>, <strong><em>SD</em></strong>): <span class="math inline">\(s = \sqrt{\frac{1}{n-1} \sum \limits_{i=1}^n (x_i - \bar{x})^2}\)</span>, якому в R відповідає функція <code><a href="https://rdrr.io/r/stats/sd.html">sd()</a></code>,</p></li>
<li><p><strong>стандартна помилка</strong> (<em>standard error</em>, <strong><em>SE</em></strong>) описує відхилення вибіркового середнього <span class="math inline">\(\bar{x}\)</span>: <span class="math inline">\(\frac{s}{\sqrt{n}}\)</span>,</p></li>
<li><p><strong>коефіцієнт варіації</strong> (<em>coefficient of variation</em>, <strong><em>CV</em></strong>) нормалізує стандартне відхилення на середнє арифметичне і часто позначається у відсотках: <span class="math inline">\(CV = 100 \% \cdot {s}/{\hat{\mu}}\)</span>.</p></li>
</ul>
<p>Щодо важливості нормального розподілу в статистиці, варто згадати закон великих чисел та центральну граничну теорему. <strong>Закон великих чисел</strong> стверджує, що якщо із <strong><em>генеральної сукупності</em></strong> <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;&lt;strong&gt;Генеральна сукупність&lt;/strong&gt; – поширене поняття в статистиці; якщо спостерігач обрав вибірку значень (наприклад, вага особин модельного виду), ця вибірка є лише обмеженою підмножиною генеральної сукупності, розмір якої апроксимує до безкінечності. Ми припускаємо що вибірка є репрезентативною щодо генеральної сукупності, отже, розподіл ймовірності в генеральній сукупності апроксимує до розподілу в генеральній сукупності. Неможливо набрати вибірку розміром із розмір генеральної сукупності – спостерігач завжди пропустить бодай один зразок.&lt;/p&gt;"><sup>27</sup></a> незалежно і багаторазово набирати окремі вибірки, то усереднена статистика<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Статистика – інше слово для параметру вибірки або параметру тесту. Середнє арифметичне є статистикою, дисперсія є статистикою тощо.&lt;/p&gt;"><sup>28</sup></a> цих вибірок наближається до істинного значення статистики генеральної сукупності, якщо таке існує. В контексті нормального розподілу, середні вибірок (<span class="math inline">\(\hat{\mu}\)</span>) з генеральної сукупності конвергують до середнього генеральної сукупності <span class="math inline">\(\mu\)</span>. <strong>Центральна гранична теорема</strong> ж постулює, що в множині таких незалежних <em>змінних</em> <span class="math inline">\(X_1, X_2, X_3, \cdots, X_n\)</span>, що <span class="math inline">\(\mathbb{E} [X_i] = \mu\)</span>, <span class="math inline">\(Var[X_i] = \sigma^2\)</span>, незалежно від розподілу окремих змінних <span class="math inline">\(X_i\)</span>, за <span class="math inline">\(n \rightarrow \infty\)</span> розподіл змінних <span class="math inline">\(\sqrt{n} (\frac{1}{n}\sum \limits_{i=1}^n X_i - \mu)\)</span> конвергує до <span class="math inline">\(\mathcal{N}(0,  \sigma^2)\)</span>. Іншими словами, яким би не був розподіл генеральної сукупності, розподіл середніх значень вибірок із такої генеральної сукупності буде нагадувати нормальний розподіл.</p>
<p>Корисною технікою є <strong>z-стандартизація</strong> (<em>z-scaling</em>), за допомогою якої будь-яку вибірку можна трансформувати в таку, в якої <span class="math inline">\(\mu = 0, \sigma^2 = 1\)</span> (припускаючи, що вибірка розподілена нормально, але техніка працює для будь-якого розподілу): <span class="math inline">\(z_i = \frac{x_i - \bar{x}}{s}\)</span>. Такий нормальний розподіл, що <span class="math inline">\(\mathcal{N} (\mu = 0, \sigma^2 = 1)\)</span> називається <strong>cтандартним нормальним розподілом</strong>.</p>
</div>
<div id="лог-нормальний-розподіл" class="section level5" number="3.4.1.2.5">
<h5>
<span class="header-section-number">3.4.1.2.5</span> Лог-нормальний розподіл<a class="anchor" aria-label="anchor" href="#%D0%BB%D0%BE%D0%B3-%D0%BD%D0%BE%D1%80%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%B8%D0%B9-%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB"><i class="fas fa-link"></i></a>
</h5>
<p>Змінна <span class="math inline">\(X\)</span> розподілена лог-нормально (<span class="math inline">\(X \sim L\mathcal{N}(\mu, \sigma^2)\)</span>) якщо <span class="math inline">\(\ln(X) \sim \mathcal{N}(\mu, \sigma^2)\)</span>. Відтак, якщо поглянути з іншого боку, то якщо <span class="math inline">\(Y \sim \mathcal{N}(\mu, \sigma^2)\)</span>, то <span class="math inline">\(X = e^Y \sim L\mathcal{N}(\mu, \sigma^2)\)</span>.</p>
<ul>
<li><p>Середнє <span class="math inline">\(\mathbb{E} [\ln (X)] = \mu, \mathbb{E}[X] = e^{\mu + \frac{\sigma^2}{2}}\)</span>,</p></li>
<li><p>варіація <span class="math inline">\(Var[\ln(X)] = \sigma^2, Var[X] = \exp [2(\mu - \sigma^2)] - \exp [2 \mu + \sigma^2]\)</span>.</p></li>
</ul>
</div>
<div id="розподіл-коші" class="section level5" number="3.4.1.2.6">
<h5>
<span class="header-section-number">3.4.1.2.6</span> Розподіл Коші<a class="anchor" aria-label="anchor" href="#%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB-%D0%BA%D0%BE%D1%88%D1%96"><i class="fas fa-link"></i></a>
</h5>
<p>Доволі дивний розподіл, який формою нагадує нормальний із набагато гострішим піком та товстішими хвостами.</p>
<ul>
<li><p>Позначення: <span class="math inline">\(X \sim \mathcal{Cauchy}(x_0, \gamma)\)</span>, де <span class="math inline">\(\gamma\)</span> регулює форму кривої, а <span class="math inline">\(x_0\)</span> відповідає локації піку.</p></li>
<li><p>pdf: <span class="math inline">\(f(x) = \frac{1}{\pi} \left[ \frac{\gamma}{(x - x_0)^2 + \gamma^2} \right]\)</span></p></li>
</ul>
<p>Дивність цього розподілу полягає в тому, що його середнє й варіація неможливо аналітично визначити. Оцінки середнього арифметичного і cередньоквадратичного відхилення не конвергують зі збільшенням розміру вибірки, а єдиним більш-менш точним методом оцінки параметру форми <span class="math inline">\(\hat{\gamma}\)</span> є медіана абсолютних значень вибірки.</p>
</div>
</div>
</div>
<div id="bars" class="section level3" number="3.4.2">
<h3>
<span class="header-section-number">3.4.2</span> Опис розподілу змінної (описова статистика)<a class="anchor" aria-label="anchor" href="#bars"><i class="fas fa-link"></i></a>
</h3>
<p>Уявімо вибірку змінної <span class="math inline">\(X \sim \mathcal{N}(\mu = 15, \sigma^2 = 9)\)</span> розміром <span class="math inline">\(n = 1000\)</span>:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># визначимо рандомне зерно для повторюваності коду</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># визначимо змінну як випадкову вибірку із нормального розподілу із визначеними параметрами</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">1000</span>, mean <span class="op">=</span> <span class="fl">15</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">9</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Як описати тенденції цієї вибірки одним-двома параметрами? Мабуть, більшість автоматично скажуть “давайте порахуємо середнє”, хтось додасть “і варіацію у вигляді середньоквадратичного відхилення чи дисперсії”. Натомість, навіть на цьому етапі вирішення статистики для опису змінної варто задати собі питання: а що нам ці статистики дадуть? По-перше, як мінімум, ми хочемо отримати такі значимі і продумані статистики, із якими, якщо потрібно, можна спробувати відтворити вибірку. По-друге, як ми побачили в попередньому підрозділі, існує чимало розподілів ймовірності, і <em>найгірше, що можна зробити – це спробувати описати змінну із певним розподілом параметром не цього розподілу</em> (наприклад, намагатись оцінити параметри <span class="math inline">\(a\)</span> і <span class="math inline">\(b\)</span> бета-розподілу, коли вибірка відповідає Пуасонівському процесу). Дуже важливим неписаним правилом статистичного аналізу є те, що <strong>в більшості випадків оцінщик чи статистичний тест видасть якийсь результат для даних, які в нього введені, і, може, навіть видасть якесь значення <span class="math inline">\(p\)</span> (див. <a href="numerical-ecology.html#pval">нижче</a>), але цей результат нічого не значить якщо обрано некоректну статистичну процедуру для певного набору даних</strong>. Крім того, завжди мати на увазі принцип <strong>“garbage in, garbage out”</strong> (“сміття на вході, сміття на виході”): навіть якщо логіка статистичного аналізу правильна і програма функціонує коректно, результати не можна вважати валідними якщо вхідні дані помилкові.</p>
<p>Це дуже довгий спосіб сказати, що для вибору метрики центральної тенденції у вибірці варто враховувати розподіл цієї вибірки. Не можна розраховувати, скажімо, середнє арифметичне просто тому, що так зручно. Натомість, вибір статистики повинен бути виважений і відповідати передбаченому розподілу змінної<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Наприклад, як розрахувати узагальнену кількість особин виду для повторних спостережень на певній локації? Свого часу мені казали рахувати максимальну кількість особин між спостереженнями, однак такий підхід не є коректним. Натомість, якщо розглядати спостереження особин як біноміальний процес або процес Пуасона, за обох розподілів середнє арифметичене слугує виправданим оцінщиком. Для поглибленого погляду в цю тему, див. &lt;a href="foundations.html#detectability"&gt;ймовірність детекції&lt;/a&gt;.&lt;/p&gt;'><sup>29</sup></a>. В нашому випадку, ми знаємо що <span class="math inline">\(X\)</span> згенеровано як нормальний процес, але давайте про всяк випадок перевіримо чи ця змінна дійсно розподілена нормально.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/shapiro.test.html">shapiro.test</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  X
## W = 0.99737, p-value = 0.1053</code></pre>
<p>Тест Шапіро-Вілка видає <span class="math inline">\(p = 0.105 &gt; 0.05\)</span>, що у випадку цього тесту дає підстави вважати, що змінна дійсно розподілена нормально. Відтак, центральну тенденцію можна описати середнім арифметичним, адже воно відповідає оцінщику очікування нормального розподілу.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 14.92021</code></pre>
<p>Які є альтернативи середньому арифметичному? Популярним вибором є <strong>медіана</strong>, особливо в якості <strong><em>непараметричної статистики</em></strong> – такої статистики, яка вимагає мінімальних передбачень щодо розподілу даних і може бути використана для змінних із будь-яким розподілом. В багатьох випадках змінні в даних матимуть дивний (асиметричний, бімодальний тощо) розподіл. В таких випадках можна спробувати провести тест Шапіро-Вілка, який скоріш за все не підтримає припущення про нормальний розподіл (<span class="math inline">\(p &lt; 0.05\)</span>). Якщо є припущення про якийсь інший розподіл, його можна перевірити із тестом <strong>Колмогорова-Смірнова</strong><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;&lt;strong&gt;Тест Колмогорова-Смірнова&lt;/strong&gt; використовують для перевірки гіпотези, що дві вибірки отримані із одного неперервного розподілу. Іншими словами, тестом Колмогорова-Смірнова можна перевірити розподіл змінної (навіть непараметричний розподіл, якщо наявна його адекватна pdf).&lt;/p&gt;"><sup>30</sup></a>. Порівняймо нашу вибірку із нормальним розподілом (хоча ми й так вже знаємо із тесту Шапіро-Вілка відповідь).</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># z-стандартизація вибірки потрібна, адже розподіл для порівняння є за замовчуванням стандартним нормальним розподілом</span></span>
<span><span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/scale.html">scale</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># порівняймо із ідеальним випадком, для чого наведемо назву функції кумулятивного нормального розподілу</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/ks.test.html">ks.test</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Z</span>, y <span class="op">=</span> <span class="st">"pnorm"</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
##  Asymptotic one-sample Kolmogorov-Smirnov test
## 
## data:  Z
## D = 0.021443, p-value = 0.7474
## alternative hypothesis: two-sided</code></pre>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># альтернативно, можна для порівняння використати іншу випадкову вибірку із нормальним розподілом</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/ks.test.html">ks.test</a></span><span class="op">(</span><span class="va">Z</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
##  Asymptotic two-sample Kolmogorov-Smirnov test
## 
## data:  Z and rnorm(n = length(Z))
## D = 0.026, p-value = 0.8879
## alternative hypothesis: two-sided</code></pre>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># або порівняти вихідну не-стандартизовану змінну із випадковою нормальною вибіркою із середнім та варіацією вихідної вибірки</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/ks.test.html">ks.test</a></span><span class="op">(</span><span class="va">X</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
##  Asymptotic two-sample Kolmogorov-Smirnov test
## 
## data:  X and rnorm(n = length(X), mean = mean(X), sd = sd(X))
## D = 0.036, p-value = 0.5361
## alternative hypothesis: two-sided</code></pre>
<p>Оскільки вибірка <span class="math inline">\(X \sim \mathcal{N}()\)</span>, можна перебачити що медіана приблизно дорівнюватиме середньому арифметичному, відтак, для нормальних вибірок немає змісту наводити інший параметр окрім середнього.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 14.92021</code></pre>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/median.html">median</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 14.88062</code></pre>
<p>Якби ми підходили до непараметричного аналізу вибірки із не-нормальним розподілом, варто було би використовувати лише медіану. Класичним прикладом є уявний експеримент із вибіркою з трьох людей: одним мільйонером (зарплатня <span class="math inline">\(\$1000000\)</span> на місяць) та двома бідняками (<span class="math inline">\(\$100\)</span> на місяць). Яка середня зарплатня і яка медіана в такій популяції?</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1000000</span>, <span class="fl">100</span>, <span class="fl">100</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 333400</code></pre>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/median.html">median</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1000000</span>, <span class="fl">100</span>, <span class="fl">100</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 100</code></pre>
<p>Очевидно, середнє значення <span class="math inline">\(\$330400\)</span> нічого не значить в цій значно асиметричній вибірці, адже біднякам така “середня температура по лікарні” аж ніяк не допомагає. Медіана ж є набагато більш репрезентативною статистикою.</p>
<p>Скажімо, ми хочемо описати розподіл змінної, але не маємо змоги надати значення кожного значення в змінній (особливо якщо йдеться про значний розмір вибірки). Навіть у випадку нормально розподіленої змінної середнього значення недостатньо, адже воно не надає жодної ідеї про розмах кривої, тобто її варіацію. Як тоді читач може відтворити вашу змінну? Хорошою ідеєю є надати значення оцінщика варіації. Наприклад, сказати що <span class="math inline">\(X \sim \mathcal{N}(\mu = 15, \sigma^2 = 9), n = 1000\)</span> цілком достатньо аби відтворити змінну за допомогою комп’ютера, наприклад, в R як <code>rnorm(n = 1000, mean = 15, sd = sqrt(9))</code>.</p>
<p>У випадку невідомого розподілу змінної, можна використати перцентилі – значення у вибірці, які розділяють цю сортовану вибірку на визначені частки. Наприклад, якщо ми позначимо <span class="math inline">\(\alpha\)</span>-тий перцентиль змінної <span class="math inline">\(X\)</span> як <span class="math inline">\(x_{\alpha}\)</span>, тоді <span class="math inline">\(P(X \leq x_{\alpha}) = \alpha\)</span> (тобто у вибірці <span class="math inline">\(X\)</span> <span class="math inline">\(99\%\)</span> значень <span class="math inline">\(x_i\)</span> будуть меншими або дорівнюватимуть значенню <span class="math inline">\(x_{\alpha = 0.99}\)</span>). В якості непараметричного опису розподілу можна як мінімум подати перцентилі для <span class="math inline">\(\alpha = 0.025\)</span> і <span class="math inline">\(\alpha = 0.975\)</span> – тоді <span class="math inline">\(95\%\)</span> змінної перебуватимуть в межах цих двох значень. Для більш детального опису розподілу можна подати й інші межі: <span class="math inline">\(60 \%\)</span> (<span class="math inline">\(\alpha = \{0.2, 0.8\}\)</span>), <span class="math inline">\(80 \%\)</span> (<span class="math inline">\(\alpha = \{0.1, 0.9\}\)</span>) тощо. Медіана відповідає <span class="math inline">\(\alpha = 0.5\)</span>, тобто рівно половина значень у вибірці буде менша за медіану, рівно половина – більша.</p>
<p>Якщо є вибірка із дивним розподілом і його не вдається описати жодним із перелічених вище параметричним розподілом, корисною технікою є <strong>ядрова оцінка густини розподілу</strong> (<em>kernel density estimation, KDE</em>,). Існує декілька його варіацій, але в найпростішому вигляді процедура наступна: <strong>(1)</strong> для кожного спостереження <span class="math inline">\(x_i\)</span> побудуймо елементарний нормальний розподіл із фіксованою варіацією <span class="math inline">\(\sigma^2 = a\)</span> такий що <span class="math inline">\(\mathcal{N}_i = \mathcal{N}(\mu = x_i, \sigma^2 = a)\)</span>, і <strong>(2)</strong> додаймо всі такі розподіли <span class="math inline">\(\mathcal{N}_i\)</span> в один. Якщо суму цих функцій стандартизувати таким чином, щоб її інтеграл дорівнював одиниці, на виході отримаємо валідну функції густини ймовірності, яка точно описує розподіл вихідної змінної. На практиці функція (kernel function) із кроку <strong>(1)</strong> складніша за простий нормальний розподіл, що дозволяє змінювати розмір вікна, в межах якої ця функція враховує точки спостережень, котрі впливають на розмір ядра – так званий параметр пропускної здатності (bandwidth) що може змінювати “чіткість” результуючої функції.</p>
<p>На жаль, мало хто приділяє увагу таким деталям і обмежується значеннями середнього і якогось оцінщика варіації на кшталт “<span class="math inline">\(\bar{x} = 14.920 \pm 2.992 SD\)</span>” чи “<span class="math inline">\(\bar{x} = 14.920 \pm 0.095 SE\)</span>”. В принципі, середнього і показника похибки достатньо для висновку щодо форми розподілу змінної (знову ж, якщо відомо що ця змінна розподілена нормально). Головним застереженням тут є те, що <strong>необхідно завжди зазначати використану статистику похибку</strong>. Крім стандартного відхилення (SD) та стандартної похибки (SE) таким можуть також слугувати <strong><em>довірчі інтервали</em></strong> (<em>confidence interval</em>). Довірчий інтервал статистики оцінює, в якому інтервалі лежатимуть значення статистики за багаторазового повтору експерименту із певним рівнем довіри <span class="math inline">\(\gamma\)</span> (зазвичай, <span class="math inline">\(\gamma = 0.95\)</span>, у випадку чого інтервал зветься “95% довірчим інтервалом”, “95% CI”). Цей інтервал не означає, що точність оцінки становить плюс-мінус якесь значення, особливо коли вибірка асиметрична (відтак, інтервал необхідно позначати власне як інтервал без використання знаку плюс-мінус). Натомість, ми можемо очікувати що за багаторазового незалежного повторення експерименту значення статистики лежатиме в межах довірчих інтервалів у <span class="math inline">\(100 \cdot \gamma \%\)</span> випадків.</p>
<p>В переважній більшості випадків оцінка довірчих інтервалів є параметричною процедурою, унікальною для окремої статистики і окремого розподілу. Поширеною помилкою є, наприклад, оцінка довірчих інтервалів середнього арифметичного нормального розподілу для частки (якогось значення між <span class="math inline">\(0\)</span> і <span class="math inline">\(1\)</span>): така оцінка передбачатиме, наприклад, існування значень частки <span class="math inline">\(&lt;0\)</span> і <span class="math inline">\(&gt;1\)</span>, що є абсурдом. В разі, якщо не вдається знайти адекватний оцінщик статистики в певному розподілі, адекватним варіантом може бути <a href="numerical-ecology.html#paradigms">пермутаційна</a> оцінка інтервалу як перцентилів (<span class="math inline">\(2.5\%, 97.5\%\)</span>) розподілу статистики згенерованої пермутаціями.</p>
<p>Вибір оцінки похибки видається залежним від моди в певних сферах, і, насправді, кожен із них має право на існування допоки дослідник чітко позначає, яка саме похибка використана. Найчастіше про це забувають в графіках змінних, що є доволі трагічним: від вибору метрики “розмаху” залежить весь умовивід із графіку. Нижче (Рис. <a href="numerical-ecology.html#fig:fig-3-10">3.10</a>. – код також додано, раптом комусь знадобиться) наведено потенційні візуальні відображення розподілу в одній однієї й тієї ж вибірки <span class="math inline">\(X\)</span>.</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Xdf</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>X <span class="op">=</span> <span class="va">X</span><span class="op">)</span></span>
<span></span>
<span><span class="va">f_3_10_a</span> <span class="op">&lt;-</span> <span class="va">Xdf</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, y <span class="op">=</span> <span class="va">X</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_jitter</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">xlab</span><span class="op">(</span><span class="st">""</span><span class="op">)</span> <span class="op">+</span> <span class="fu">ylab</span><span class="op">(</span><span class="st">"Значення X"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>axis.ticks.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span>,</span>
<span>        axis.text.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">f_3_10_b</span> <span class="op">&lt;-</span> <span class="va">Xdf</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">X</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_bar</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_x_binned</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">xlab</span><span class="op">(</span><span class="st">"Значення X"</span><span class="op">)</span> <span class="op">+</span> <span class="fu">ylab</span><span class="op">(</span><span class="st">"Частота"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">f_3_10_c</span> <span class="op">&lt;-</span> <span class="va">Xdf</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">X</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_density</span><span class="op">(</span>fill <span class="op">=</span> <span class="st">"gray"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">xlab</span><span class="op">(</span><span class="st">"Значення X"</span><span class="op">)</span> <span class="op">+</span> <span class="fu">ylab</span><span class="op">(</span><span class="st">"Частота"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">f_3_10_d</span> <span class="op">&lt;-</span> <span class="va">Xdf</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, y <span class="op">=</span> <span class="va">X</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_violin</span><span class="op">(</span>fill <span class="op">=</span> <span class="st">"gray"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ylim</span><span class="op">(</span><span class="fl">4</span>, <span class="fl">25</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">xlab</span><span class="op">(</span><span class="st">""</span><span class="op">)</span> <span class="op">+</span> <span class="fu">ylab</span><span class="op">(</span><span class="st">"Значення X"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>axis.ticks.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span>,</span>
<span>        axis.text.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">f_3_10_e</span> <span class="op">&lt;-</span> <span class="va">Xdf</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, y <span class="op">=</span> <span class="va">X</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_boxplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ylim</span><span class="op">(</span><span class="fl">4</span>, <span class="fl">25</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">xlab</span><span class="op">(</span><span class="st">""</span><span class="op">)</span> <span class="op">+</span> <span class="fu">ylab</span><span class="op">(</span><span class="st">"Значення X"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>axis.ticks.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span>,</span>
<span>        axis.text.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">X_sum</span> <span class="op">&lt;-</span> <span class="va">Xdf</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">summarise</span><span class="op">(</span>n <span class="op">=</span> <span class="fu">n</span><span class="op">(</span><span class="op">)</span>, mu <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, se <span class="op">=</span> <span class="va">sd</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">f_3_10_f</span> <span class="op">&lt;-</span> <span class="va">X_sum</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, y <span class="op">=</span> <span class="va">mu</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, y <span class="op">=</span> <span class="va">mu</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_errorbar</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, ymin <span class="op">=</span> <span class="va">mu</span> <span class="op">-</span> <span class="va">sd</span>, ymax <span class="op">=</span> <span class="va">mu</span> <span class="op">+</span> <span class="va">sd</span><span class="op">)</span>, </span>
<span>                width <span class="op">=</span> <span class="fl">0.4</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ylim</span><span class="op">(</span><span class="fl">4</span>, <span class="fl">25</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">xlab</span><span class="op">(</span><span class="st">""</span><span class="op">)</span> <span class="op">+</span> <span class="fu">ylab</span><span class="op">(</span><span class="st">"Середнє X ± SD"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>axis.ticks.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span>,</span>
<span>        axis.text.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">quantiles_95</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">r</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/quantile.html">quantile</a></span><span class="op">(</span><span class="va">x</span>, probs<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.05</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.95</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">r</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"ymin"</span>, <span class="st">"lower"</span>, <span class="st">"middle"</span>, <span class="st">"upper"</span>, <span class="st">"ymax"</span><span class="op">)</span></span>
<span>  <span class="va">r</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">f_3_10_g</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span><span class="va">Xdf</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, y <span class="op">=</span> <span class="va">X</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">guides</span><span class="op">(</span>fill <span class="op">=</span> <span class="cn">F</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">stat_summary</span><span class="op">(</span>fun.data <span class="op">=</span> <span class="va">quantiles_95</span>, geom <span class="op">=</span> <span class="st">"boxplot"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ylim</span><span class="op">(</span><span class="fl">4</span>, <span class="fl">25</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">xlab</span><span class="op">(</span><span class="st">""</span><span class="op">)</span> <span class="op">+</span> <span class="fu">ylab</span><span class="op">(</span><span class="st">"Медіана ± 50%, 95% CI"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>axis.ticks.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span>,</span>
<span>        axis.text.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">f_3_10_h</span> <span class="op">&lt;-</span> <span class="va">X_sum</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, y <span class="op">=</span> <span class="va">mu</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, y <span class="op">=</span> <span class="va">mu</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_errorbar</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, ymin <span class="op">=</span> <span class="va">mu</span> <span class="op">-</span> <span class="va">se</span>, ymax <span class="op">=</span> <span class="va">mu</span> <span class="op">+</span> <span class="va">se</span><span class="op">)</span>, </span>
<span>                width <span class="op">=</span> <span class="fl">0.4</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">xlab</span><span class="op">(</span><span class="st">""</span><span class="op">)</span> <span class="op">+</span> <span class="fu">ylab</span><span class="op">(</span><span class="st">"Середнє X ± SE"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>axis.ticks.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span>,</span>
<span>        axis.text.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">f_3_10_i</span> <span class="op">&lt;-</span> <span class="va">X_sum</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, y <span class="op">=</span> <span class="va">mu</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_col</span><span class="op">(</span>position <span class="op">=</span> <span class="fu">position_dodge</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, y <span class="op">=</span> <span class="va">mu</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_errorbar</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0</span>, ymin <span class="op">=</span> <span class="va">mu</span> <span class="op">-</span> <span class="va">se</span>, ymax <span class="op">=</span> <span class="va">mu</span> <span class="op">+</span> <span class="va">se</span><span class="op">)</span>, </span>
<span>                width <span class="op">=</span> <span class="fl">0.4</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">xlab</span><span class="op">(</span><span class="st">""</span><span class="op">)</span> <span class="op">+</span> <span class="fu">ylab</span><span class="op">(</span><span class="st">"Значення X ± SE"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>axis.ticks.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span>,</span>
<span>        axis.text.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">ggarrange</span><span class="op">(</span>plotlist <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>  <span class="va">f_3_10_a</span>, <span class="va">f_3_10_b</span>, <span class="va">f_3_10_c</span>, <span class="va">f_3_10_d</span>, <span class="va">f_3_10_e</span>, <span class="va">f_3_10_f</span>, <span class="va">f_3_10_g</span>, <span class="va">f_3_10_h</span>, <span class="va">f_3_10_i</span></span>
<span><span class="op">)</span>, labels <span class="op">=</span> <span class="va">letters</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">9</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:fig-3-10"></span>
<img src="bookdown-demo_files/figure-html/fig-3-10-1.png" alt="Різноманітні способи зобразити розподіл змінної: **(a)** хмара точок спостережень - найкращий спосіб зобразити сирі даних, якщо точок відносно небагато; **(b)** гістограма - колонки відповідають рівномірним діапазонам значень; **(c)** ядрова оцінка густини розподілу, своєрідна неперевна гістограма, що є кращою альтернативою гістограмі у випадку континуальних змінних; **(d)** 'скрипко-графік' (violin plot) є дзеркальним відображенням ядрової оцінки розподілу; **(e)** класичний 'коробко-графік' (boxplot) де жирна центральна лінія відповідає медіані, межі коробки - міжквартильному розмаху (між 25- та 75-тими перцентилями), межі вусів - розмаху даних без 'викидів', а окремі точки - власне викидам (спостереження, що потрапили за межі 1.5 міжквартильного розмаху від 25- чи 75-ого перцентилю); **(f)** відображення середнього арифметичного і відхилення у формі стандартного відхилення; **(g)** 2.5-, 25-, 50-, 75-, та 97.5-ті перцентилі, які можуть сприйматися як довірчі інтервали розподілу (але не його середнього); **(h)** відображення середнього арифметичного і відхилення у формі стандартної помилки - зверніть увагу на шкалу, розмах вусів набагато менший за інші за рахунок значного розміру вибірки; **(i)** відверто найгірший варіант - 'динаміто-графік' (dynamite plot), який малює колонку чогось (на відміну від гістограми, ця колонка рідко має будь-який зміст, наприклад, у вибірці нема значень нижче за 4, але зона від 0 до середнього все одно замальована) із 'вусами' стандартної помилки." width="960"><p class="caption">
Рис. 3.10: Різноманітні способи зобразити розподіл змінної: <strong>(a)</strong> хмара точок спостережень - найкращий спосіб зобразити сирі даних, якщо точок відносно небагато; <strong>(b)</strong> гістограма - колонки відповідають рівномірним діапазонам значень; <strong>(c)</strong> ядрова оцінка густини розподілу, своєрідна неперевна гістограма, що є кращою альтернативою гістограмі у випадку континуальних змінних; <strong>(d)</strong> ‘скрипко-графік’ (violin plot) є дзеркальним відображенням ядрової оцінки розподілу; <strong>(e)</strong> класичний ‘коробко-графік’ (boxplot) де жирна центральна лінія відповідає медіані, межі коробки - міжквартильному розмаху (між 25- та 75-тими перцентилями), межі вусів - розмаху даних без ‘викидів’, а окремі точки - власне викидам (спостереження, що потрапили за межі 1.5 міжквартильного розмаху від 25- чи 75-ого перцентилю); <strong>(f)</strong> відображення середнього арифметичного і відхилення у формі стандартного відхилення; <strong>(g)</strong> 2.5-, 25-, 50-, 75-, та 97.5-ті перцентилі, які можуть сприйматися як довірчі інтервали розподілу (але не його середнього); <strong>(h)</strong> відображення середнього арифметичного і відхилення у формі стандартної помилки - зверніть увагу на шкалу, розмах вусів набагато менший за інші за рахунок значного розміру вибірки; <strong>(i)</strong> відверто найгірший варіант - ‘динаміто-графік’ (dynamite plot), який малює колонку чогось (на відміну від гістограми, ця колонка рідко має будь-який зміст, наприклад, у вибірці нема значень нижче за 4, але зона від 0 до середнього все одно замальована) із ‘вусами’ стандартної помилки.
</p>
</div>
<p>Зображення розподілу вибірок часто необхідно для порівняння двох (чи більше) вибірок. Наприклад, уявіть вибірки <span class="math inline">\(A\)</span> і <span class="math inline">\(B\)</span>, які відповідають промірам певного параметру (наприклад, маси тіла) в двох групах (скажімо, субпопуляціях виду). Залежно від того, чому відповідають вуса графіків, висновок із їх візуального зображення відрізнятиметься:</p>
<ul>
<li><p>якщо вуса <strong>SD</strong> двох вибірок перекриваються між собою, це не може бути достатнім доказом статистичної відмінності між вибірками (необхідні додаткові тести, наприклад, t-тест Ст’юдента або непараметричний аналог);</p></li>
<li><p>якщо вуса <strong>SE</strong> перекриваються і дві вибірки <em>мають однакові розміри</em>, статистичної відмінності між вибірками, скоріш за все, немає; якщо вуса не перекриваються, необхідне додаткове тестування;</p></li>
<li><p>якщо вуса <strong>95% CI</strong> перекриваються, необхідне додаткове тестування; якщо ж вони не перекриваються, скоріш за все, існує істотна різниця між вибірками.</p></li>
</ul>
<p>Загалом, графічне зображення розмаху вибірок викликає чимало плутанини із висновками (<a href="https://doi.org/10.1093/jis/3.1.34">Payton et al. 2003</a>, <a href="https://doi.org/10.1037/1082-989X.10.4.389">Belia et al. 2005</a>, <a href="https://doi.org/10.1083/jcb.200611141">Cumming et al. 2007</a>). Порадою слугуватиме завжди зазначати які саме метрики розмаху зображені й проводити додаткове статистичне тестування гіпотез.</p>
</div>
</div>
<div id="basic-hypotheses" class="section level2" number="3.5">
<h2>
<span class="header-section-number">3.5</span> Тестування гіпотез<a class="anchor" aria-label="anchor" href="#basic-hypotheses"><i class="fas fa-link"></i></a>
</h2>
<div id="hypothesis" class="section level3" number="3.5.1">
<h3>
<span class="header-section-number">3.5.1</span> Статистична гіпотеза<a class="anchor" aria-label="anchor" href="#hypothesis"><i class="fas fa-link"></i></a>
</h3>
<p>Статистичне тестування гіпотез має багато спільного із філософією науки. <strong>Гіпотеза</strong> є припущенням, яке може бути істинним або ні. Будь-яке твердження може бути гіпотезою (наприклад, “гроза є виявом злості бородатого дядька на небі” є валідною гіпотезою), однак наукова гіпотеза має бути такою, яку можливо емпірично перевірити (див. <a href="https://uk.wikipedia.org/wiki/%D0%A1%D0%BF%D1%80%D0%BE%D1%81%D1%82%D0%BE%D0%B2%D1%83%D0%B2%D0%B0%D0%BD%D1%96%D1%81%D1%82%D1%8C">критерій спростовуваності Поппера</a>). В процесі наукового пізнання так чи інакше доводиться приймати чи відхиляти гіпотези залежно від наявних даних, однак варто пам’ятати що жодна гіпотеза не є істиною: навіть якщо всі попередні експерименти підтримують робочу гіпотезу, це не означає що наступний експеримент також її підтримає.</p>
<p>Статистичні гіпотези є прикладами гіпотез, особливістю яких є дуже формальний чисельний їх опис. Для кожної статистичної гіпотези має бути можливість записати її у вигляді рівняння, нерівності, чи логіки. Гіпотеза “гроза є виявом злості бородатого дядька на небі” не є валідною статистичною гіпотезою, однак її можна переформулювати в “ймовірність виникнення грози лінійно асоційована із густиною злих бородатих дядьків в об’ємі неба”. Так, методологічно таку гіпотезу все одно перевірити складно, але тепер у неї є математична складова, тож її можливо перевірити.</p>
<p>Статистичні гіпотези зазвичай є доволі простими твердженнями, аби їх можна було перевірити. На практиці, це часто означає пошук балансу між поглибленою трансформацією даних й тестуванням простої гіпотези. Застосовуючи <a href="https://uk.wikipedia.org/wiki/%D0%91%D1%80%D0%B8%D1%82%D0%B2%D0%B0_%D0%9E%D0%BA%D0%BA%D0%B0%D0%BC%D0%B0">принцип Оккама</a>, якщо декілька статистичних гіпотез відповідають ідентичній науковій гіпотезі і не мають суттєвої різниці між собою, варто обирати найпростішу статистичну гіпотезу. Наприклад, якщо ви намагаєтесь порівняти вибірки мас тіла в двох субпопуляціях виду, <span class="math inline">\(A\)</span> і <span class="math inline">\(B\)</span>, і припускаєте що між ними є істотна різниця, існує декілька способів сформулювати статистичну гіпотезу: <strong>(1)</strong> всі значення в <span class="math inline">\(A\)</span> більші/менші за всі значення в <span class="math inline">\(B\)</span> (не є хорошою гіпотезою, адже вона занадто консервативна – спрацює тільки коли дві хмари точок не перетинаються – і не є занадто простою, адже включає дві статистичні гіпотези <span class="math inline">\(a_i &gt; b_i \forall a_i, a_b\)</span> та <span class="math inline">\(a_i &lt; b_i \forall a_i, a_b\)</span>); <strong>(2)</strong> середнє значення вибірки <span class="math inline">\(A\)</span> більше/менше за середнє значення <span class="math inline">\(B\)</span> (менш консервативна, але все ще складна гіпотеза, що включатиме дві простіші гіпотези <span class="math inline">\(\bar{a} &gt; \bar{b}\)</span> і <span class="math inline">\(\bar{a} &lt; \bar{b}\)</span>); <strong>(3)</strong> середнє значення <span class="math inline">\(A\)</span> не дорівнює середньому значенню <span class="math inline">\(B\)</span> (включає лише одну елементарну гіпотезу <span class="math inline">\(\bar{a} \neq \bar{b}\)</span>); <strong>(4)</strong> різниця між середніми значеннями вибірок не дорівнює нулю (мабуть, є найпростішою гіпотезою, до якої можна звести це питання про маси тіла в субпопуляціях, <span class="math inline">\((\bar{a} - \bar{b}) \neq 0\)</span>).</p>
</div>
<div id="nulldistr" class="section level3" number="3.5.2">
<h3>
<span class="header-section-number">3.5.2</span> Нульовий розподіл<a class="anchor" aria-label="anchor" href="#nulldistr"><i class="fas fa-link"></i></a>
</h3>
<p>Тестування статистичних гіпотез є доволі цікавим і, певною мірою, контрінтуїтивним процесом. Зазвичай, тести не кажуть “так, ваша статистична гіпотеза має право на життя”, а виходять із протилежного твердження – <strong>нульової гіпотези</strong> (<em>null hypothesis</em>), і, відтак, радше кажуть “не знаю як щодо вашої статистичної гіпотези, але альтернатива їй взагалі не підтверджується наявними даними”. Відтак, для кожної статистичної гіпотези (твердження <span class="math inline">\(H\)</span>) існує певна нульова гіпотеза, яка намагається пояснити дані із припущення того, що <span class="math inline">\(H\)</span> не відповідає дійсності (позначимо нульову гіпотезу як <span class="math inline">\(H_0\)</span>). Як альтернатива нульовій гіпотезі існує <strong>альтернативна гіпотеза</strong> (<em>alternative hypothesis</em>) <span class="math inline">\(H_A\)</span>, яка робить твердження протилежне до <span class="math inline">\(H_0\)</span> і, відтак, узгоджується із <span class="math inline">\(H\)</span>.</p>
<p>Навіщо це потрібно? Візьмемо до уваги попередній приклад: нашою науковою гіпотезою є те, що особини субпопуляцій А і B відрізняються масою тіла. Для перевірки цієї наукової гіпотези ми формулюємо чітку статистичну гіпотезу <span class="math inline">\(H: (\bar{a} - \bar{b}) \neq 0\)</span>. Як перевірити цю статистичну гіпотезу? Можна, звісно, порахувати середні двох вибірок, і можна гарантувати що їх різниця не дорівнюватиме нулю навіть якщо вони не надто сильно різняться – отримати вибірки із ідентичними середніми арифметичними дуже малоймовірно. Відтак, ця різниця буде відрізнятись від нуля, і для перевірки статистичної гіпотези необхідно зрозуміти яка різниця між середніми є достатньо великою, аби не вважатись просто статистичним шумом. В цій ситуації у нас є дві взаємозаперечні статистичні гіпотези: нульова <span class="math inline">\(H_0: (\bar{a} - \bar{b}) = 0\)</span> та альтернативна <span class="math inline">\(H_A: (\bar{a} - \bar{b}) \neq 0\)</span>. Відтак, для відповіді на попереднє питання необхідно знати, який розподіл би мала різниця між середніми <span class="math inline">\((\bar{a} - \bar{b})\)</span> за умови що <span class="math inline">\(H_0\)</span> є істинною. Цей розподіл можна назвати нульовим (null distribution), із яким можна порівняти спостережене значення <span class="math inline">\((\bar{a} - \bar{b})\)</span> і вирішити чи “так, спостережена різниця набагато більша від випадкового шуму навколо нуля в нульовому розподілі” (відповідно, відхилити <span class="math inline">\(H_0\)</span> та прийняти <span class="math inline">\(H_A\)</span>) або “ні, спостережена різниця настільки незначна, що її можна було б очікувати навіть якщо насправді різниці нема” (відхилити <span class="math inline">\(H_A\)</span> та прийняти <span class="math inline">\(H_0\)</span>).</p>
<p>За усієї своєї простоти, адекватний статистичний аналіз неможливий без адекватної нульової моделі (<a href="https://www.jstor.org/stable/2096971">Harvey et al. 1983</a>) – і ця тема настільки важлива, що їй присвячені цілі книги (<a href="https://www.uvm.edu/~ngotelli/nullmodelspage.html">Gotelli &amp; Graves 1996</a>)! Готеллі визначає <strong>нульову модель</strong> як “<em>модель, що генерує тренди на підставі рандомізованих екологічних даних чи випадкової вибірки із відомого чи уявного розподілу […] створеного для утворення трендів, які можна було би очікувати за відсутності певного механізму [в якому ми зацікавлені]</em>”.</p>
<p>Цікавим екологічним прикладом є відомий факт того, що біологічне різноманіття поблизу екватору нашої планети набагато вище порівняно із різноманіттям поблизу полюсів. Існує декілька наукових гіпотез, що пояснюють це спостереження: <strong>(1)</strong> продуктивність екосистем поблизу екватору вища, відповідно, є більше ресурсів для більшої кількості особин, що означає більше видів (<a href="https://doi.org/10.1111/j.1461-0248.2004.00671.x">Currie et al. 2004</a>); <strong>(2)</strong> тропіки є найбільшим біомом, тож є більше площі для підтримки більшої кількості видів (<a href="https://doi.org/10.2307/3546528">Rosenzweig &amp; Sandlin 1997</a>); <strong>(3)</strong> тропіки є старішим біомом, відповідно, було більше часу для видоутворення (<a href="https://doi.org/10.1146/annurev-ecolsys-112414-054102">Fine 2015</a>); <strong>(4)</strong> навколо тропіків темпи видоутворення вищі, а вимирання – нижчі (<a href="https://doi.org/10.1111/j.1461-0248.2007.01020.x">Mittelbach et al. 2007</a>). Яку нульову модель використати для статистичної перевірки цих гіпотез? Мабуть, рівномірний розподіл різноманіття видів не є найкращою моделлю зважаючи на те, що навколо полюсів менше площі, та й не зрозуміло як врахувати форму планети тощо. Натомість, найпростішою нульовою моделлю варто вважати ефект середнього домену (mid-domain effect): якщо випадкові ареали видів розподілені між полюсами випадково, то суто із геометричних причин більше ареалів перетинатимуться десь між полюсами, й, відповідно, кількість видів буде найбільша посередині, коло екватору (Рис. <a href="numerical-ecology.html#fig:fig-3-11">3.11</a>).</p>
<div class="figure">
<span style="display:block;" id="fig:fig-3-11"></span>
<img src="images/mid_domain.png" alt="Ілюстрація ефекту середнього домену для пояснення розподілу біологічного різноманіття на планеті олівцями в коробці: якщо ареали видів випадкових діапазонів широт випадково розподілені між полюсами, то найбільше перетинів видів існуватиме поблизу полюсу. Фіолетові комірки відповідають кількості пересікань із олівцями за горизонталлю." width="1650"><p class="caption">
Рис. 3.11: Ілюстрація ефекту середнього домену для пояснення розподілу біологічного різноманіття на планеті олівцями в коробці: якщо ареали видів випадкових діапазонів широт випадково розподілені між полюсами, то найбільше перетинів видів існуватиме поблизу полюсу. Фіолетові комірки відповідають кількості пересікань із олівцями за горизонталлю.
</p>
</div>
</div>
<div id="pval" class="section level3" number="3.5.3">
<h3>
<span class="header-section-number">3.5.3</span> Тестування гіпотез<a class="anchor" aria-label="anchor" href="#pval"><i class="fas fa-link"></i></a>
</h3>
<p>Суть будь-якого статистичного тесту полягає в оцінці певної статистики тесту (метрики) і оцінці значущості цієї метрики для висновку щодо істинності статистичної гіпотези за певного нульового розподілу. Залежно від парадигми статистичного аналізу, існують різні способи отримати нульовий розподіл. Наприклад, параметричні тести виходять із багатьох припущень і виводять параметричні нульові розподіли. Саме тому настільки важливо дотримуватись припущень тестів (вибірки мають бути нормально розподілені, мати однакові параметри варіації, бути незалежними тощо – в кожного тесту свій набір припущень).</p>
<p>Результатом статистичного тесту, зазвичай, є оцінка метрики тесту і асоційоване <span class="math inline">\(p\)</span>-значення – cвящений грааль і наріжний камінь всякого аналізу, який всі хочуть оцінити але не всі знають що то таке. Мета статистичного тесту – це оцінити наскільки ймовірно було би отримати певні результати тесту якщо нульова гіпотеза є істинною. Статистичне тестування подібне до судового процесу. Уявіть собі, що підсудного звинувачують у скоєнні злочину (нульова гіпотеза за презумпції невинуватості – підсудний невинний, альтернативна – підсудний винний). Перед судом постає відповідальна й непроста задача прийняти рішення щодо винуватості підсудного за наявних даних. Реальність же може відповідати одному з двох варіантів: або підсудний справді вчинив злочин, або ні.</p>
<ul>
<li>
<p>Якщо підсудний справді не вчиняв злочину (нульова гіпотеза істинна),</p>
<ul>
<li><p>рішення суду щодо винуватості підсудного (хибно-позитивний результат) відправить невинну людину за ґрати, або</p></li>
<li><p>рішення суду щодо невинуватості (дійсно-негативний результат) залишить і підсудного, і суспільство задоволеними,</p></li>
</ul>
</li>
<li>
<p>якщо ж підсудний насправді вчинив злочин (альтернативна гіпотеза істинна), то</p>
<ul>
<li><p>рішення суду щодо винуватості підсудного (дійсно-позитивний результат) виллється у відбування заслуженого покарання, або</p></li>
<li><p>рішення суду щодо невинуватості (хибно-негативний результат) відправить злочинця на волю, що є небезпечним для суспільства.</p></li>
</ul>
</li>
</ul>
<p>Відтак, в такому уявному судовому процесі, залежно від реальності, можна припуститись однієї з двох критичних помилок:</p>
<ul>
<li><p><strong>помилки першого роду</strong> – відправити невинну людину за ґрати – прийняти альтернативну гіпотезу, коли нульова є істинною, або</p></li>
<li><p><strong>помилки другого роду</strong> – відпустити злочинця на волю – прийняти нульову гіпотезу, коли альтернативна є істинною.</p></li>
</ul>
<p>Яка із цих помилок є страшнішою, мабуть, є філософським питанням, і кожне суспільство нехай відповідає на нього самостійно. В науці ж помилка першого роду відповідатиме видаванню за істину доказів, які не відповідають дійсності; в той час як помилка другого роду – ігноруванню фактів. Як на мене, помилка першого роду шкодитиме науковому знанню сильніше.</p>
<p>Якби істина була відома, тоді в кожній парі нульової-альтернативної гіпотез можна було би оцінити дві ймовірності:</p>
<ul>
<li><p><span class="math inline">\(\alpha = P(\text{помилка I роду} = P(\text{відхилити } H_0| H_0 \text{ істина})\)</span>, яку ще називають <strong>рівнем значущості</strong> (<em>significance level</em>), та</p></li>
<li><p><span class="math inline">\(\beta = P(\text{помилка II роду} = P(\text{не відхилити } H_0| H_0 \text{ хибна})\)</span>.</p></li>
</ul>
<p>Два значення <span class="math inline">\(\alpha\)</span> і <span class="math inline">\(\beta\)</span> є зворотньо пов’язаними (Рис. <a href="numerical-ecology.html#fig:fig-3-12">3.12</a>): збільшення одного значення зменшить інше. Відтак, завдання відповідального статистичного аналізу – збалансувати <span class="math inline">\(\alpha\)</span> і <span class="math inline">\(\beta\)</span>, знайти якесь максимальне значення <span class="math inline">\(\alpha\)</span>, яке ми можемо толерувати. На практиці, за таке значення часто приймають <span class="math inline">\(\alpha = 0.05\)</span> (за незалежного повторення 20 експериментів, ми очікуємо принаймні одного хибно-позитивного результату). Втім, критичне значення <span class="math inline">\(\alpha\)</span> для будь-якого тесту повинне враховувати баланс між <span class="math inline">\(\alpha\)</span> і <span class="math inline">\(\beta\)</span>, тим, наскільки небажаними є помилки першого та другого роду. Отже, потрібно завжди пам’ятати що порогове значення <span class="math inline">\(\alpha = 0.05\)</span> є лише умовністю.</p>
<p>Цікавою метрикою тесту, яку варто побічно згадати, є також <strong>потужність тесту</strong> (<em>power</em>) <span class="math inline">\((1 - \beta)\)</span>. Аналіз потужності (<em>power analysis</em>) є поширеним методом знаходження мінімального розміру вибірки для адекватної інтерпретації результатів статистичного тесту. Цей тип аналізу допоможе відповісти на питання “якщо я хочу провести [цей конкретний статистичний тест] із рівнем значущості [<span class="math inline">\(\alpha\)</span>], то який мінімальний розмір вибірки потрібно було би набрати на стадії збору даних?”.</p>
<div class="figure">
<span style="display:block;" id="fig:fig-3-12"></span>
<img src="bookdown-demo_files/figure-html/fig-3-12-1.png" alt="***(a)*** Уявімо, що можливо оцінити розподіл статистики тесту $z$ за нульової ($H_0$) та альтернативної ($H_A$) гіпотез. Тоді було би можливо оцінити пов'язані ймовірності припуститися помилки першого роду ($\alpha$, хибно-позитивне рішення) та помилки другого роду ($\beta$, хибно-негативне рішення) як площі під кривими розподілів статистик. За спостереження статистики тесту $z_{obs}$ на рисунку ***(b)***, $p$-значення відповідає ймовірності спостерігати таке ж або більш екстремальне значення статистики за нульової гіпотези." width="1152"><p class="caption">
Рис. 3.12: <strong><em>(a)</em></strong> Уявімо, що можливо оцінити розподіл статистики тесту <span class="math inline">\(z\)</span> за нульової (<span class="math inline">\(H_0\)</span>) та альтернативної (<span class="math inline">\(H_A\)</span>) гіпотез. Тоді було би можливо оцінити пов’язані ймовірності припуститися помилки першого роду (<span class="math inline">\(\alpha\)</span>, хибно-позитивне рішення) та помилки другого роду (<span class="math inline">\(\beta\)</span>, хибно-негативне рішення) як площі під кривими розподілів статистик. За спостереження статистики тесту <span class="math inline">\(z_{obs}\)</span> на рисунку <strong><em>(b)</em></strong>, <span class="math inline">\(p\)</span>-значення відповідає ймовірності спостерігати таке ж або більш екстремальне значення статистики за нульової гіпотези.
</p>
</div>
<p>Підхід <strong><span class="math inline">\(p\)</span>-значення</strong> (p-value) у статистичному тестуванні намагається оцінити ймовірність того, що <em>значення статистики тесту за істинності нульової гіпотези дорівнюватиме або буде ще більш екстремальним порівняно із спостереженим значенням</em>. Варто зазначити, що на практиці оперують лише точковим спостереженим значенням статистики тесту (наприклад, різницею середніх значень вибірок <span class="math inline">\((\bar{a} - \bar{b})\)</span>) та нульовим розподілом (як би були розподілені <span class="math inline">\((\bar{a} - \bar{b})\)</span> якщо <span class="math inline">\(H_0: \bar{a} = \bar{b}\)</span>?). Відтак, <span class="math inline">\(p\)</span>-значення каже наскільки можна очікувати отриманого результату тесту за нульової гіпотези. Якщо ця ймовірність дуже маленька (наприклад, менше за обране порогове значення <span class="math inline">\(\alpha\)</span>), тоді варто нульову гіпотезу відхилити й існують підстави вважати, що між двома вибірками є істотна різниця. Якщо ж ця ймовірність значна, тоді немає підстав вважати, що отримані результати є чимось більшим аніж статистичним шумом за нульової гіпотези.</p>
<p>Варто звернути увагу на поняття <strong><em>двосторонніх та односторонніх тестів</em></strong> (<em>two-sided</em>, <em>upper-tail one-sided</em>, <em>lower-tail one-sided</em>), які відповідають чітко сформульованій альтернативній гіпотезі. Наприклад, якщо ми перевіряємо факт різниці між двома середніми, то альтернативна гіпотеза виглядає як <span class="math inline">\(H_A: \bar{a} \neq \bar{b} \Leftrightarrow (\bar{a} - \bar{b}) \neq {0}\)</span>, отже, зони відхилення нульової гіпотези будуть знаходитись по обидва боки нульового розподілу із ймовірностями <span class="math inline">\(\alpha/2\)</span>. Якщо ж є підстави вважати що одне середнє більше за інше, альтернативна гіпотеза виглядатиме як <span class="math inline">\(H_A: \bar{a} &gt; \bar{b} \Leftrightarrow (\bar{a} - \bar{b}) &gt; {0}\)</span> або <span class="math inline">\(H_A: \bar{a} &lt; \bar{b} \Leftrightarrow (\bar{a} - \bar{b}) &lt; {0}\)</span>. Відповідно, в такому випадку зона відхилення нульової гіпотези буде знаходитись у верхньому або нижньому хвості нульового розподілу і її ймовірність становитиме <span class="math inline">\(\alpha\)</span>.</p>
<p>Отже, тестування статистичної гіпотези включає наступні кроки:</p>
<ul>
<li><p>формулювання нульової <span class="math inline">\(H_0\)</span> та альтернативної <span class="math inline">\(H_A\)</span> гіпотез,</p></li>
<li><p>вибір критичного значення рівня значущості <span class="math inline">\(\alpha\)</span>,</p></li>
<li><p>отримання вибірки достатнього розміру і обчислення статистики обраного тесту <span class="math inline">\(z\)</span>,</p></li>
<li>
<p>порівняння спостереженої статистики тесту із нульовим розподілом<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Нульовий розподіл може бути згенеровано в межах параметричного тесту (наприклад, як t-розподіл в t-тесті Стьюдента) або пермутаційно.&lt;/p&gt;"><sup>31</sup></a>. Є два поширених способи провести цю операцію:</p>
<ul>
<li><p>древній метод: поглянути в таблицю критичних значень за певних параметрів нульового розподілу (<span class="math inline">\(z_{\alpha}\)</span>) і порівняти спостережену статистику із критичною для певного <span class="math inline">\(\alpha\)</span>,</p></li>
<li><p>адекватніший підхід: обчислити точне значення для параметризованого нульового розподілу та спостереженої статистики тесту,</p></li>
</ul>
</li>
<li><p>зробити висновки щодо відхилення нульової гіпотези.</p></li>
</ul>
</div>
<div id="paradigms" class="section level3" number="3.5.4">
<h3>
<span class="header-section-number">3.5.4</span> Парадигми статистичного аналізу<a class="anchor" aria-label="anchor" href="#paradigms"><i class="fas fa-link"></i></a>
</h3>
<p>Різниця між трьома основними парадигмами статистичного аналізу, їх переваги й недоліки, та чим вони відрізняються доволі влучно описано в підручнику Готеллі та Еллісон <a href="https://learninglink.oup.com/access/gotelli-a-primer-of-ecological-statistics-2e">“Початки екологічної статистики”</a>. В цьому підрозділі я наводжу лише основні ідеї цих парадигм, але варто пам’ятати, що дослідженню кожної з них можна приділити роки життя.</p>
<div id="частотницька-або-фреквентистська-парадигма" class="section level4" number="3.5.4.1">
<h4>
<span class="header-section-number">3.5.4.1</span> Частотницька, або фреквентистська парадигма<a class="anchor" aria-label="anchor" href="#%D1%87%D0%B0%D1%81%D1%82%D0%BE%D1%82%D0%BD%D0%B8%D1%86%D1%8C%D0%BA%D0%B0-%D0%B0%D0%B1%D0%BE-%D1%84%D1%80%D0%B5%D0%BA%D0%B2%D0%B5%D0%BD%D1%82%D0%B8%D1%81%D1%82%D1%81%D1%8C%D0%BA%D0%B0-%D0%BF%D0%B0%D1%80%D0%B0%D0%B4%D0%B8%D0%B3%D0%BC%D0%B0"><i class="fas fa-link"></i></a>
</h4>
<p>Найпоширеніший підхід до статистичного аналізу – частотницький (<em>frequentist approach</em>) – ґрунтується на припущенні, що ймовірність описує ніщо інше, як частоту подій за безкінечного повторення експерименту, й, відповідно, ми можемо спостерігати лише якусь зліченну кількість подій і на їх підставі спробувати оцінити асоційовані ймовірності. Цей підхід часто спрощує спостережувану реальність до ідеальних моделей (які можуть бути дуже складними математично, але все ж простішими за дійсність), які відображені у параметричних моделях – математичних функціях із відносно незначною кількістю параметрів. Відтак, і тестування гіпотез базується на нульових розподілах із визначеним математичним формулюванням й параметрами (такими розподілами є, наприклад, нормальний, t-розподіл Стьюдента, F-розподіл Фішера тощо).</p>
<p>Параметричні методи завжди мають набір припущень, і перед застосуванням цих методів завжди необхідно перевіряти чи ваші дані відповідають цим припущенням. Більшість параметричних методів матимуть припущення, що <strong>(1)</strong> всі спостереження в даних зібрані незалежно й випадково, та <strong>(2)</strong> дані зібрані із генеральних сукупностей із певним визначеним розподілом. Перше припущення не те щоб є специфічним для частотницької парадигми, а є критичним для експериментального дизайну за будь-якого статистичного підходу: вибір залежних між собою спостережень або невипадковий підбір спостережень викликатиме упередження в даних, тож результати всякого статистичного аналізу не будуть адекватними. Друге припущення найчастіше каже, що вибірки в аналізі повинні бути розподілені нормально.</p>
<p>Існують, звісно, і непараметричні методи, однак їх підхід до визначення ймовірності події залишається незмінним. Непараметричні підходи мають послаблені вимоги до розподілу вибірки і є чудовою альтернативою параметричним тестам коли, скажімо, не вдається підтвердити що ваші дані розподілені нормально. Із непараметричними методами завжди варто бути обережними, оскільки навіть якщо вони є надійними (robust) за порушення припущення про розподіл даних, вони є чутливими до всіх інших припущень<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Наприклад, для порівняння середніх двох вибірок хорошим вибором статистичного тесту є t-тест Стьюдента. Однак якщо ваші дані не розподілені нормально, вам можуть порадити тест Вілкоксона (Wilcoxon signed-rank test) або тест Манна-Вітні (Mann–Whitney U test). Мало хто знає, втім, що ці тести дуже чутливі до власних припущень: тест Вілкоксона працює лише для незалежних пар залежних (в межах пар) значень, а тест Манна-Вітні передбачає &lt;em&gt;ідентичні&lt;/em&gt; розподіли між двома групами за нульової гіпотези.&lt;/p&gt;"><sup>32</sup></a>.</p>
</div>
<div id="баєсівська-парадигма" class="section level4" number="3.5.4.2">
<h4>
<span class="header-section-number">3.5.4.2</span> Баєсівська парадигма<a class="anchor" aria-label="anchor" href="#%D0%B1%D0%B0%D1%94%D1%81%D1%96%D0%B2%D1%81%D1%8C%D0%BA%D0%B0-%D0%BF%D0%B0%D1%80%D0%B0%D0%B4%D0%B8%D0%B3%D0%BC%D0%B0"><i class="fas fa-link"></i></a>
</h4>
<p>Баєсівська парадигма є складнішою для інтуїтивного розуміння і вимагає більшої підготовки. Крім того, я неодноразово чув думку, що Баєсівський метод – то часто лише модний спосіб тестувати гіпотезу, для якої звичайний параметричний тест дав би таку ж відповідь. Філософія Баєсівського підходу полягає в тому, що часто в розпорядженні існують попередні дані щодо тестованої гіпотези. У частотницькій парадигмі кожен окремий експеримент відбувається “наосліп”, адже існує припущення, що кожен експеримент отримає якусь репрезентативну вибірку із генеральної сукупності і із його результатів можна судити про тренди в самій генеральній сукупності. Однак, якщо ви проводите експеримент, подібний до якого вже хтось колись проводив, то чи не розсудливіше врахувати ті попередні, <em>пріорні</em> дані?</p>
<p>Баєсівський аналіз намагається поглибити наші знання щодо наукової гіпотези із кожним експериментом із врахуванням попередніх даних, що, в цілому, відповідає сучасному науковому підходу: із кожним експериментом ми шліфуємо наявне наукове знання. Скажімо, ми намагаємось оцінити якийсь параметр в генеральній сукупності. Першим кроком у Баєсівському аналізі було би змиритись із думкою про те, що якщо ми намагаємось оцінити цей параметр і його оновлене оцінене значення, скоріш за все, буде відрізнятись від пріорної оцінки, то цей параметр є не фіксованим значенням, а, радше, розподілом значень. Із попередніх експериментів мають бути наявні оцінки розподілу параметру, тож цей розподіл ми назвемо <em>пріорним</em>, і тепер нашим завданням є оцінити <em>постеріорний</em> розподіл параметру за даних нового експерименту, що можна зробити із застосуванням теореми Баєса:</p>
<p><span class="math display">\[P(\text{гіпотеза}|\text{дані}) = \frac{P(\text{гіпотеза})P(\text{дані}|\text{гіпотеза})}{P(\text{дані})}\]</span></p>
<p>Отже, якщо частотницькі методи питають <em>яка ймовірність отримати значення статистики рівне або більш екстремальне за спостережене значення в наявних даних, якщо нульова гіпотеза істинна</em>, то Баєсівський підхід намагається відповісти <em>яка ймовірність гіпотези про статистику тесту за наявних даних</em>. У цій формулі <span class="math inline">\(P(\text{гіпотеза}|\text{дані})\)</span> називають <strong>постеріорною ймовірністю</strong>, <span class="math inline">\(P(\text{гіпотеза})\)</span> – <strong>пріорною ймовірністю</strong>, <span class="math inline">\(P(\text{дані}|\text{гіпотеза})\)</span> – <strong><a href="numerical-ecology.html#mle">правдоподібністю</a></strong> (що відображає ймовірність спостереження цього конкретного набору даних якщо гіпотеза істинна), в той час як знаменник <span class="math inline">\(P(\text{дані})\)</span> у Баєсівській теоремі є лише нормалізуючою константою (ймовірність даних за усіх можливих гіпотез), якою можна знехтувати і переписати вираз як</p>
<p><span class="math display">\[P(\text{гіпотеза}|\text{дані}) \propto P(\text{гіпотеза})P(\text{дані}|\text{гіпотеза})\]</span></p>
<p>Аби ще сильніше ускладнити інтуїтивне розуміння цієї парадигми, кожну із ймовірностей у цій формулі варто уявляти не як точкове значення ймовірності, а як <a href="numerical-ecology.html#pdfs">розподіли густини ймовірності</a>. Вибір <em>пріорного розподілу</em> залежить від попередньо існуючих даних: навіть із описових даних можна спробувати визначити певний виправданий розподіл. Якщо ж цього не вдається зробити, альтернативою (яку дуже часто використовують) є визначення <em>неінформативного розподілу</em> (uninformative prior), наприклад, нормального розподілу із настільки високим параметром <span class="math inline">\(\sigma^2\)</span>, що на локальних діапазонах він апроксимує до плаского рівномірного розподілу. У виборі неінформативного пріорного розподілу й криється критика повсюдного застосування Баєсівської парадигми: якщо в моделі відсутнє адекватно визначене пріорне знання, то Баєсівський аналіз не має жодних переваг над частотнитцькими методами (<a href="https://doi.org/10.1111/oik.05985">Lemoine 2019</a>). Відтак, вибір пріорного розподілу повинен бути обґрунтованим.</p>
<p>Подальші кроки вимагають оцінки функції правдоподібності даних за істинності гіпотези (<span class="math inline">\(P(\text{дані}|\text{гіпотеза}\)</span>) та нормалізуючої константи інтегрованої правдоподібності (<em>marginal likelihood</em>), що іноді можна зробити аналітично, але, зазвичай, розв’язується за допомогою алгоритмів ітеративно. На виході Баєсівський підхід повертає розподіл постеріорної ймовірності. Як із розподілу зробити висновок? Поширеним інструментом є оцінка <strong>імовірного інтервалу</strong> (<em>credibility interval</em>, не плутати із довірчим інтервалом, <em>confidence interval</em>), наприклад, 95% імовірного інтервалу як 2.5%- і 97.5%-ті перцентилі постеріорного розподілу. Розташування спостереженої статистики тесту відносно 95% імовірного інтервалу постеріорного розподілу (в межах або за межами), відтак, дає підставу зробити висновок щодо статистичної гіпотези.</p>
<p>В контексті Баєсівської парадигми варто згадати методи Монте-Карло ланцюгів Маркова (<em>Markov chain Monte Carlo</em>, MCMC). <strong>Процес Маркова</strong>, або <strong>ланцюг Маркова</strong> – це такий процес, в якому об’єкт в момент часу має певне значення, і переходить в інший стан в наступний момент часу. Такі стани можуть бути як неперервною змінною, так і дискретною; найпростіше для розуміння уявляти скінченний, бажано невеликий, набір дискретних станів. Популярним прикладом є погодні умови в певний день. Скажімо, набір можливих станів в цій системі є <span class="math inline">\(\{\text{сонячно}, \text{хмарно}, \text{дощ}\}\)</span>, і кожен день приймає один із цих станів. Якби такий процес був Марковським, то в кожен окремий день ймовірність стану залежить тільки від стану в попередній день; <em>випадковий процес, в якому ймовірність стану в момент часу за відомої послідовності станів в усі попередні кроки залежить тільки від стану протягом останнього попереднього кроку</em> називають таким, що має властивість Маркова. Ланцюг Маркова описується набором ймовірностей переходу від станів. Ланцюги Маркова часто зображають у вигляді графічних ланцюгів (Рис. <a href="numerical-ecology.html#fig:fig-3-13">3.13</a>), в той час як їх математична репрезентація виглядає як <a href="numerical-ecology.html#matrices">матриця</a> ймовірностей переходів – <strong>матриця переходів</strong> <span class="math inline">\(\mathbf{T}\)</span> (<em>transition matrix</em>). В таких матрицях рядки відповідають попереднім станам, а колонки – наступним. Сума ймовірностей переходів із певного стану (сума рядків) повинна дорівнювати одиниці.</p>
<div class="figure">
<span style="display:block;" id="fig:fig-3-13"></span>
<img src="images/mc.png" alt="Гіпотетичний ланцюг Маркова у графічному вигляді та у вигляді матриці ймовірностей переходів, що описує погоду протягом дня як один із трьох можливих станів. Після сонячного дня варто очікувати сонячний день, після хмарного -- дощів, після дощового -- сонячного тощо." width="1631"><p class="caption">
Рис. 3.13: Гіпотетичний ланцюг Маркова у графічному вигляді та у вигляді матриці ймовірностей переходів, що описує погоду протягом дня як один із трьох можливих станів. Після сонячного дня варто очікувати сонячний день, після хмарного – дощів, після дощового – сонячного тощо.
</p>
</div>
<p>Звісно, розвиток подій в ланцюзі Маркова може залежати від розподілу станів на стадії його ініціалізації – а отже, і на кожному кроці частоту станів можна описати як функцію розподілу ймовірності. Певні ланцюги Маркова можуть досягнути рівноважного стану, коли розподіли ймовірності перестають змінюватись із наступним кроком. Відповідно, <em>стаціонарним</em> (<em>stationary</em>) розподілом називають такий розподіл <span class="math inline">\(\vec{\pi}\)</span>, для якого справджується умова, що <span class="math inline">\(\vec{\pi} \mathbf{T} = \vec{\pi}\)</span>. Пермутації Монте-Карло ланцюгів Маркова відповідають алгоритмам, які ітерують велику кількість кроків крізь такий Марковський процес, стаціонарний розподіл якого відповідає шуканому розподілу. Найвідомішим є <strong>алгоритм Метрополіса-Хастінгса</strong> (<em>Metropolis-Hastings algorithm</em>) для отримання випадкової вибірки <span class="math inline">\(X\)</span> із певного складного розподілу або просто функції <span class="math inline">\(f(x)\)</span>, із якої непросто отримати випадкову змінну аналітичним шляхом. В цьому алгоритмі нарощується ланцюг випадкових значень, що на початку алгоритму обирається із заданого пріорного розподілу. На кожному кроці <span class="math inline">\((i)\)</span> алгоритм бере до уваги значення <span class="math inline">\(x_{i-1}\)</span> із розподілу попереднього кроку, і будує навколо <span class="math inline">\(x_{i-1}\)</span> якийсь визначений розподіл <span class="math inline">\(g(x_{i-1})\)</span> (наприклад, нормальний із фіксованою між кроками варіацією <span class="math inline">\(\mathcal{N}(\mu = x, \sigma^2)\)</span>). Із цього розподілу обираєтся випадкове значення-кандидат <span class="math inline">\(x'\)</span> таке що <span class="math inline">\(x` \sim g(x_{i-1})\)</span> (якщо було обрано нормальний розподіл, то<span class="math inline">\(x' \sim \mathcal{N}(\mu = x_{i-1}, \sigma^2)\)</span>). Після цього алгоритм обирає, чи взяти до уваги <span class="math inline">\(x'\)</span>, при чому ймовірність вибору значення-кандидата <span class="math inline">\(x'\)</span> визначається відношенням <span class="math inline">\(g(x')/g(x_{i-1})\)</span>. Якщо обрано <span class="math inline">\(x'\)</span>, то нове значення в розподілі нарощене протягом цього кроку становитиме <span class="math inline">\(x_i = x'\)</span>, якщо ж ні, то <span class="math inline">\(x_i = x_{i-1}\)</span>. Процедура повторюється багаторазово (розряду тисяч разів), і в результаті видає ланцюг <span class="math inline">\(\{x_1, x_2, x_3, \cdots, x_n\}\)</span>. Із цього ланцюга викидається певна кількість перших <span class="math inline">\(m\)</span> ланок, оскільки в них алгоритм іноді видає невдалі значення – цей період називають розігрівом (<em>burn-in</em>). Залишок ланцюга <span class="math inline">\(\{x_{m+1}, x_{m+2}, \cdots, x_n\}\)</span> ж утворює вибірку, розподіл якої апроксимує до шуканого. Якщо шуканий розподіл відповідає постеріорному розподілу в Баєсівській парадигмі, а функція <span class="math inline">\(g(x)\)</span> пропорційна до функції правдоподібності даних за тестованої гіпотези, то цей алгоритм є вдалим вибором для Баєсівського аналізу. В мережі нескладно знайти <a href="https://blog.djnavarro.net/posts/2023-04-12_metropolis-hastings/">приклади</a> простих алгоритмів МСМС на R.</p>
</div>
<div id="permutation-paradigm" class="section level4" number="3.5.4.3">
<h4>
<span class="header-section-number">3.5.4.3</span> Пермутаційний аналіз<a class="anchor" aria-label="anchor" href="#permutation-paradigm"><i class="fas fa-link"></i></a>
</h4>
<p>Принцип роботи МСМС навіює певний настрій ітеративного стилю статистичного аналізу: якщо не вдається вирішити проблему аналітично, просто напишіть алгоритм, який оцінить оптимальне рішення! Ба більше, як вже було згадано вище, параметричні та непараметричні тести працюють лише коли дані відповідають всім припущенням цих тестів. Якщо це не відповідає дійсності, то ітеративні обчислення також можуть стати в нагоді, варто лише пам’ятати процедури тестування гіпотез.</p>
<p>Уявіть два угруповання, <span class="math inline">\(A\)</span> і <span class="math inline">\(B\)</span>. Кожне угруповання описується як кількість особин певного виду (обмежимо <span class="math inline">\(\gamma\)</span>-різноманіття, тобто сумарне різноманіття між угрупованнями, до десяти видів), що для цієї ілюстрації ми згенеруємо випадково із розподілу Пуасона.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">comA</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">comB</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">5</span><span class="op">)</span></span></code></pre></div>
<p>Якщо позначити кожен вид, то ми можемо поглянути на ці два угрупованні як на таблицю:</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">coms</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>A <span class="op">=</span> <span class="va">comA</span>, B <span class="op">=</span> <span class="va">comB</span>, row.names <span class="op">=</span> <span class="va">letters</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">coms</span></span></code></pre></div>
<pre><code>##    A B
## a  5 4
## b 10 4
## c  6 4
## d  7 3
## e  5 7
## f  3 3
## g  7 4
## h  9 2
## i  4 4
## j  3 3</code></pre>
<p>І от питання: наскільки ці угруповання подібні між собою? Для відповіді є цілий набір різноманітних <a href="comecol.html#similarity">індексів подібності</a>, однак в цьому прикладі можемо обмежитись доволі стародавньою метрикою – Евклідовою відстанню. Зі школи можна пригадати, що квадрат гіпотенузи дорівнює сумі квадратів катетів. Якщо задуматись, то довжина гіпотенузи є дистанцією між двома кутами прямокутного трикутника, координати яких становлять <span class="math inline">\((x = \text{довжина катету 2}, y = 0)\)</span> та <span class="math inline">\((x = 0, y = \text{довжина катету 1})\)</span>. Відповідно, цю теорему Піфагора можна використати для розрахунку дистанції між двома точками <span class="math inline">\(p\)</span> і <span class="math inline">\(q\)</span> в двовимірному просторі із координатами <span class="math inline">\((p_1, p_2)\)</span> та <span class="math inline">\((q_1, q_2)\)</span>: <span class="math inline">\(d_{p, q}= \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}\)</span>. Краса Евклідової дистанції в тому, що її формулу можна екстраполювати на будь-яку кількість вимірів (позначимо вимірність як <span class="math inline">\(n: i = 1, 2, 3, \cdots, n\)</span>): <span class="math inline">\(d(p, q) = \sqrt{\sum_{i=1}^n (p_i - q_i)^2}\)</span>. Евклідова дистанція вказує на те, наскільки два об’єкти близькі один до одного в просторі, і якщо уявити угруповання такими об’єктами, то Евклідову дистанцію можна спробувати використати для оцінки того, наскільки два угруповання подібні один до одного<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Евклідова дистанція не є найбільш поширеним показником подібності угруповань, але в цьому контексті є зручною для прикладу.&lt;/p&gt;"><sup>33</sup></a>.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">eucl_dist</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">d_obs</span> <span class="op">&lt;-</span> <span class="fu">eucl_dist</span><span class="op">(</span><span class="va">coms</span><span class="op">$</span><span class="va">A</span>, <span class="va">coms</span><span class="op">$</span><span class="va">B</span><span class="op">)</span></span>
<span><span class="va">d_obs</span></span></code></pre></div>
<pre><code>## [1] 10.90871</code></pre>
<p>І отже ми отримуємо якесь значення дистанції – нашу статистику. Тепер постає інше питання, що це значення значить? Чи наші угруповання подібні між собою, чи ні? Це є гарним питанням для застосування статистичного тестування, в якому <em>нульовою гіпотезою</em> є те, що два угруповання походять із однієї генеральної сукупності (метаугруповання – множини угруповань), а <em>альтернативна гіпотеза</em> – що два угруповання походять із різних метаугруповань. Для тестування нульової гіпотези не залишається нічого, окрім як отримати нульовий розподіл для наших даних.</p>
<p>Можливо, існує спосіб аналітично знайти розподіл очікуваних Евклідових дистанцій для вибірок із однієї генеральної сукупності, однак вирішення такої задачі вимагатиме чимало часу і ми не знаємо чи це взагалі можливо. Тут у нагоді стає метод <strong>бутстреп</strong> (<em>bootstrap</em>), який дозволяє оцінити нульовий розподіл на підставі наявних даних. Симулювати нульову гіпотезу нескладно: для цього лише необхідно перемішати чисельності видів в спостережених угрупованнях. В цій конкретній ситуації, втім, постає питання адекватних нульових розподілів, адже за нульової гіпотези чисельності видів можуть бути маніпульовані тільки для кожного виду окремо (тобто не буде коректним замінити чисельність виду a в угруповання А чисельністю виду j із угруповання В, адже чисельності видів не є незалежними; втім, це не було би проблемою для незалежних змінних на кшталт результатів морфологічних промірів випадкових особин). Відтак, визначмо функцію, яка перемішуватиме чисельності видів таким чином, що симульована чисельність виду в угрупованні може рівноймовірно походити з угруповання А чи В. Варто зазначити, що для бутстрепу необхідно використовувати процедуру відбору із заміщенням (<em>sampling with replacement</em>)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Якщо із вибірки обрати певний елемент, після чого цей елемент не можна обрати ще раз, такий відбір називається відбором без заміщення (&lt;em&gt;sampling without replacement&lt;/em&gt;). Наприклад, ви намагаєтесь оцінити розподіл кольорів в кошику із кольоровими кульками: у відборі без заміщення кожну кульку дістати можна тільки раз. На противагу, якщо ви дістаєте одну кульку, записуєте її колір, і кладете назад у кошик, це є прикладом відбору із заміщенням. Очевидно, розмір вибірки не може бути більшим за розмір вихідної вибірки за відбору без заміщення, однак може бути більшим за відбору із заміщенням.&lt;/p&gt;"><sup>34</sup></a>.</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># для всяких пермутацій необхідно повернути випадкове зерно до замовчування</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/rm.html">rm</a></span><span class="op">(</span><span class="va">.Random.seed</span>, envir<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/environment.html">globalenv</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">mix_coms</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">com</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">out</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">com</span>, <span class="fl">1</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, size <span class="op">=</span> <span class="fl">2</span>, replace <span class="op">=</span> <span class="cn">T</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">out</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">com</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">out</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">new_coms</span> <span class="op">&lt;-</span> <span class="fu">mix_coms</span><span class="op">(</span><span class="va">coms</span><span class="op">)</span></span>
<span><span class="va">new_coms</span></span></code></pre></div>
<pre><code>##   A B
## a 4 4
## b 4 4
## c 6 4
## d 3 3
## e 7 7
## f 3 3
## g 7 7
## h 2 9
## i 4 4
## j 3 3</code></pre>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">eucl_dist</span><span class="op">(</span><span class="va">new_coms</span><span class="op">$</span><span class="va">A</span>, <span class="va">new_coms</span><span class="op">$</span><span class="va">B</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 7.28011</code></pre>
<p>Тепер цю операцію можна повторити багато разів, скажімо, десять тисяч разів, аби згенерувати розподіл статистики.</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">d_null</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="fl">10000</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10000</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">new_coms</span> <span class="op">&lt;-</span> <span class="fu">mix_coms</span><span class="op">(</span><span class="va">coms</span><span class="op">)</span></span>
<span>  <span class="va">d_null</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu">eucl_dist</span><span class="op">(</span><span class="va">new_coms</span><span class="op">$</span><span class="va">A</span>, <span class="va">new_coms</span><span class="op">$</span><span class="va">B</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Погляньмо на розподіл цієї статистики (чорна лінія відповідає ядерній оцінці густини розподілу) і порівняймо його із спостереженим значенням (червона лінія).</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">d_null</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>xlab <span class="op">=</span> <span class="st">"Евклідова відстань"</span>, ylab <span class="op">=</span> <span class="st">"KDE"</span>, main <span class="op">=</span> <span class="st">""</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>v <span class="op">=</span> <span class="va">d_obs</span>, lwd <span class="op">=</span> <span class="fl">2</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-15-1.png" width="672"></div>
<p>Оскільки за нульової гіпотези можна очікувати, що значення відстані наближатиметься до нуля (до того ж, дистанція не може бути негативною), екстремальні значення нульового розподілу відповідатимуть значним позитивним значенням. Ця логіка дозволяє оцінити псевдо-<span class="math inline">\(p\)</span>-значення як для одностороннього тесту:</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">d_null</span><span class="op">[</span><span class="va">d_null</span> <span class="op">&gt;=</span> <span class="va">d_obs</span><span class="op">]</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">d_null</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.0084</code></pre>
<p>Отже, в генерованому розподілі нульової вибірки, ймовірність отримати спостережене або більше за спостережене значення статистики <span class="math inline">\(p \approx 0.01\)</span> (оскільки процес стохастичний, оцінка відрізнятиметься за кожного компілювання), що дозволяє відхилити нульову гіпотезу і стверджувати що два угруповання відрізняються між собою.</p>
<p>Пермутаційні методи є доволі пластичними, адже їх можна застосовувати для будь-яких статистик і будь-яких розподілів вихідних даних. Великим недоліком, втім, є те, що цей метод є комп’ютер-інтенсивним, і для великих наборів даних обчислення вимагатимуть чимало комп’ютерного часу (секунди, хвилини, іноді дні).</p>
</div>
</div>
</div>
<div id="stat-models" class="section level2" number="3.6">
<h2>
<span class="header-section-number">3.6</span> Експеримент і модель<a class="anchor" aria-label="anchor" href="#stat-models"><i class="fas fa-link"></i></a>
</h2>
<p>За визначенням, модель є спрощеним, узагальненим, концептуальним уявленням про явище реального світу. Моделі ніколи не відображають реальність всеохоплююче, адже природні процеси часто є занадто складними системами аби їх вичерпно описати. Моделі часто також мають певні межі, поза якими адекватне узагальнення не є адекватним. Втім, так чи інакше, моделювання є доволі потужним інструментом тестування гіпотез, без якого екологія не може існувати. Навіть якщо на систему впливають сотні різних чинників, комбінований вплив яких є дуже складним і неочікуваним, моделі із лише декількома змінними часто є достатніми для бодай якого та й висновку.</p>
<p>Для побудування моделі необхідні вхідні емпіричні дані, які в ідеальних умовах мають походити із експерименту – контрольоване маніпулювання окремого чинника із супутнім спостереженням за поведінкою системи. В екології угруповань та екосистем проведення контрольованого експерименту не завжди є можливими, адже досліджувані системи є дуже великими, однак це не означає що неможливо зібрати дані. Спостереження також може генерувати корисні дані для моделювання. Процедуру збору даних для побудування моделі називають експериментальним дизайном. Адекватний статистичний аналіз неможливий без адекватного експериментального дизайну, якими би потужними чи модними не були статистичні методи.</p>
<p>Уявіть гіпотезу, що частота співання серед самців зяблика залежить від інтенсивності освітлення навколишнього середовища. Для перевірки цієї гіпотези можна розробити дизайн експерименту. Наприклад, в декількох точках розвісити автоматичні звукові рекордери і датчики освітлення. Яких помилок можна припуститись в цьому дизайні? Можна розвісити датчики настільки близько один до одного, що декілька датчиків будуть одночасно записувати одних і тих же особин: в такому випадку спостереження не будуть незалежними, що суперечитиме припущенням більшості статистичних тестів. Можна розвісити датчики освітлення занадто далеко від рекордерів: тоді спостереження не будуть між собою пов’язані, і всякі результати не матимуть жодного сенсу. Можна повісити датчики в екотопі чи континенті, де зяблики не трапляються… Гаразд, скажімо, експериментальний дизайн адекватний, дані зібрано, і знайдено взаємозв’язок між інтенсивністю освітлення й частотою співання. Знайдений зв’язок являє собою модель. Які її межі? Наприклад, така модель може передбачити, що в умовах нульового освітлення (в печері) варто очікувати негативної частоти співання, а за дуже інтенсивного освітлення, як-то прямо під прожектором, самець буде співати і не затикатись. Обидва передбаченнями є неадекватними. Крім того, чи експеримент врахував всі можливі фактори? Адже поведінка птахів може бути пов’язаною не тільки із освітленням, а й з температурою, наявністю корму, гніздових територій, інших самців, самок тощо, і навіть коли всі ці фактори враховано, то у одного конкретного зяблика може просто не бути настрою співати. Відтак, наша модель не є вичерпною, і питання лише в тому, чи є статистично значуща роль <em>тільки</em> інтенсивності освітлення – а багатьма іншими факторами іноді варто просто знехтувати.</p>
<div id="pseudoreplication" class="section level3" number="3.6.1">
<h3>
<span class="header-section-number">3.6.1</span> Експериментальний дизайн та псевдореплікація<a class="anchor" aria-label="anchor" href="#pseudoreplication"><i class="fas fa-link"></i></a>
</h3>
<p>Уявіть, що ви намагаєтесь дослідити вплив якоїсь хімічної сполуки в ґрунті на процеси росту рослин. Ви відбираєте сотню особин рослин однакового віку та фізіологічного стану із однієї генетичної лінії, висаджуєте кожну рослину в окремий вазон, і виставляєте їх в дві теплиці по п’ятдесят вазонів на теплицю. В одній теплиці ви додаєте однакову кількість хімічної сполуки до кожного вазону, в іншому – ні. За декілька тижнів настає час зібрати результати, і ви обережно вимірюєте морфологічні параметри кожної рослини: ріст, кількість листків, сумарну площу листків, концентрацію хлорофілу, суху біомасу. Прийшов час проводити статистичну обробку даних, ви дбайливо перевірили розподіли кожної змінної, і еврика! Тест Стьюдента показує значущу відмінність між двома групами за всіма параметрами. Виявляється, додавання цієї хімічної сполуки пов’язане із активнішим ростом рослин. Час подавати заявку на патент?</p>
<p>Не так швидко. Який був розмір вашої вибірки? Сотня особин, тож <span class="math inline">\(n = 100\)</span>? Чи, оскільки і кожній групі було по п’ятдесят особин, то <span class="math inline">\(n = 50\)</span>? Насправді, <span class="math inline">\(n = 2\)</span>. Можливо, теплиця із кращим ростом рослин стояла в місці із кращою експозицією до сонячних променів, або під нею зарита труба із теплою водою, або її нещодавно відремонтували і там краща термоізоляція, або вона ближче до виходу й аспіранти постійно ходили в неї на перекур. Можливостей є настільки безліч, що всі їх контролювати із таким експериментальним дизайном неможливо. Жахіття <strong>змішувальних змінних</strong> (<em>confounding variable</em>, таких змінних що впливають і на незалежну, і на залежну змінну, й відтак спричиняють коваріацію між ними без жодного причинно-наслідкового зв’язку) й помилок експериментального дизайну треба завжди мати на увазі. Цей же експеримент став жертвою невдалого дизайну із псевдореплікацією і лише довів ефект теплиці на ріст рослин.</p>
<p>Поняття <strong>псевдореплікації</strong> (<em>pseudoreplication</em>) ввів <a href="https://doi.org/10.2307/1942661">Харлберт 1984 року</a> і визначив його як “використання статистичного умовиводу для тестування експериментального ефекту на даних із експериментів де ефект не є реплікованим (хоча вибірки можуть бути реплікованими) або репліканти не є статистично незалежними”. В цьому формулюванні, під “експериментальним ефектом” (<em>treatment</em>) мається на увазі будь-яке спеціальне відношення до зразків, яке є під питанням в експерименті (наприклад, додавання хімікатів), і яке розділяє вибірку на “експеримент” і “контроль”. Вибірки в експериментальній чи/та контрольній групах можуть бути реплікованими (тобто мати більше за один зразок), але репліканти – підмножини вибірки, в яких всі елементи отримують однаковий експериментальний ефект – не обов’язково відповідають цим групам. У прикладі вище реплікантами є не окремі зразки в експериментальній/контрольній групах, а теплиці, адже в межах теплиці експериментальний ефект однаковий. На противагу, якби всі зразки були в одній теплиці, тоді ефект теплиці можна було б елімінувати і кожен окремий вазон майже можна було б вважати реплікантом (майже, адже зразки з однієї теплиці все ще не є статистично незалежними). Ще краще, якщо цього дозволяють ресурси, було б мати множину теплиць в яких випадковим чином розподілені зразки із експериментальної та контрольної груп. В такому випадку ефект теплиць можна було б врахувати в статистичному аналізі, наприклад, за рахунок <strong>змішаних моделей</strong> (<em>mixed-effect model</em>) із рандомним ефектом теплиці.</p>
<p>Визначення Харлберта робить акцент на статистичній незалежності, якої часто дуже складно досягнути. В ідеальному випадку псевдореплікації можна уникнути коли нема підстав вважати, що одні й ті ж чинники впливають на різні зразки. В екології особливу увагу варто приділяти просторовій та часовій незалежності. Якщо дані зібрані із різних просторових точок, можна виправдано очікувати що близькі між собою точки будуть менш незалежними одна від одної порівняно із далекими точками (наприклад, вимірювання вмісту газів в межах міста не будуть незалежними, бо всі зразки є під впливом одного мікроклімату). В таких випадках варто зважати на <strong>просторову автокореляцію</strong> (<em>spatial autocorrelation</em>) – залежність точок даних від їх взаємного розміщення в просторі – і використовувати специфічні методи аналізу що враховують цю автокореляцію<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;&lt;strong&gt;Автокореляція&lt;/strong&gt; – це кореляція змінної із самою собою, тобто залежність значень у вибірці від інших значень в цій вибірці.&lt;/p&gt;"><sup>35</sup></a>. Подібно, вимірювання змінних в часі також не є незалежними, адже значення змінної в момент часу залежить від значень цієї змінної в інші моменти часу (наприклад, середня добова температура сьогодні залежить від температури вчора – набагато ймовірніше що між цими значеннями незначна різниця, адже різкі перепади температури є відносно рідкісним явищем). Будь-які змінні в часі варто аналізувати за допомогою методів <strong>часових серій</strong> (<em>time series</em>). Варто зважати, що будь-який натяк на відсутність статистичної незалежності у вибірці зводить нанівець використання більшості класичних статистичних тестів, а відтак їх результати не є достовірними.</p>
</div>
<div id="regression" class="section level3" number="3.6.2">
<h3>
<span class="header-section-number">3.6.2</span> Дані та проблема моделювання<a class="anchor" aria-label="anchor" href="#regression"><i class="fas fa-link"></i></a>
</h3>
<p>У житті кожного польовика наступає такий момент, коли стадія планування дослідження (= розробки експериментального дизайну) із усіма врахованими застереженнями щодо псевдореплікації та незалежності спостережень давно позаду, труднощі польових досліджень подолані, все, що могло піти не так, пішло не так і ці помилки виправлені, і можна гордо сказати що збір даних закінчено. В цей момент ейфорія доконаності перспективи прокидатись о 5 ранку і лізти в, як воно завжди буває, найнеочікуваніші місця по об’єкт досліджень швидко заміщується жахом від усвідомлення того, що тепер всі зібрані дані пора би аналізувати. Статистичних тестів існує безліч, і до одного набору даних можна застосувати багато різних методів, більшість із яких видадуть якийсь результат. Тож який підхід обрати? Цей підрозділ не дасть відповіді на це питання, однак може підштовхнути до перших кроків.</p>
<p>Однією ремаркою буде <strong>формат даних</strong>. Часто на момент оцифровування польових нотаток з’являється спокуса зробити нудну таблицю більш візуально привабливою (наприклад, додати заголовок, виділити клітинки обабіч для нотаток, додати відсотки чи одиниці вимірювання коло значень, вписати декілька значень в одну клітинку тощо). Такий підхід, звісно, дещо спрощує і без того нудний процес оцифровування, однак стає помітною проблемою коли настає час ці дані аналізувати. Справа в тім, що аналіз оперуватиме <strong>змінними</strong> (variables) – послідовностями значень певного параметру, кожне з яких відповідає окремому спостереженню. Відтак, колонки в таблицях із даними повинні відповідати різним змінним, рядки – окремим спостереженням або вимірюванням, а в одній клітинці має бути лише одне значення. Такий підхід дехто називає <strong><em>“охайними даними”</em></strong> (<em>tidy data</em>, <a href="https://r4ds.had.co.nz/tidy-data.html">Wickham &amp; Grolemund (2017)</a>), адже він дозволяє зберігати дані в такому вигляді, що їх одразу можна аналізувати (Рис. <a href="numerical-ecology.html#fig:fig-3-14">3.14</a>). Завжди варто мати на увазі, що колись ці дані доведеться зберегти у форматі *.csv, який являє собою лишень текст, де рядки таблиці записані рядками тексту, а значення клітинок розділені комами. Відтак, наприклад, варто уникати використання коми в клітинках (що ніколи не знадобиться якщо в клітинці тільки одне значення), а в якості десяткового розділювача використовувати крапку (“одна десята” має бути “0.1”, а не “0,1” – програма на кшталт R просто не зрозуміє що то за кома).</p>
<div class="figure">
<span style="display:block;" id="fig:fig-3-14"></span>
<img src="images/tidydata.png" alt="Уявімо дослідження ефекту структури лісу на населення птахів. Приклад **(А)** відображає дещо невдалий формат даних: шапка документу займає декілька рядків (їх все одно доведеться видалити для подальшої обробки даних, тож пояснення для значень змінних варто тримати в окремому файлі, наприклад, в документації до набору даних), дата й час не однаковому форматі (їх можна вносити як текст аби уникнути автоматичного форматування, а на етапі роботи з даними можна використати функції бібліотеки `lubridate` для R), температура й хмарність є двома різними змінними із різними одиницями вимірювання (додавання одиниць вимірювання в клітинки перетворить їх вміст в текст, а додавання коми в клітинках стане на заваді адекватного зчитування даних у форматі *.csv), колонка дерев має нефіксовану кількість значень в кожній клітинці. Натомість, ті ж дані можна оцифрувати в межах парадигми охайних даних **(В)**, при чому декількома способами. Зображений тут спосіб не викличе необхідності перемикати налаштування мови під час аналізу даних, адже назви колонок прописані англійською мовою, а масив даних легко може бути імпортованим в R." width="958"><p class="caption">
Рис. 3.14: Уявімо дослідження ефекту структури лісу на населення птахів. Приклад <strong>(А)</strong> відображає дещо невдалий формат даних: шапка документу займає декілька рядків (їх все одно доведеться видалити для подальшої обробки даних, тож пояснення для значень змінних варто тримати в окремому файлі, наприклад, в документації до набору даних), дата й час не однаковому форматі (їх можна вносити як текст аби уникнути автоматичного форматування, а на етапі роботи з даними можна використати функції бібліотеки <code>lubridate</code> для R), температура й хмарність є двома різними змінними із різними одиницями вимірювання (додавання одиниць вимірювання в клітинки перетворить їх вміст в текст, а додавання коми в клітинках стане на заваді адекватного зчитування даних у форматі *.csv), колонка дерев має нефіксовану кількість значень в кожній клітинці. Натомість, ті ж дані можна оцифрувати в межах парадигми охайних даних <strong>(В)</strong>, при чому декількома способами. Зображений тут спосіб не викличе необхідності перемикати налаштування мови під час аналізу даних, адже назви колонок прописані англійською мовою, а масив даних легко може бути імпортованим в R.
</p>
</div>
<p>Коли говорити про змінні, то дані можуть мати один із багатьох можливих форматів. Одна змінна може мати лише один формат даних, і, скоріш за все, це буде один із наступних:</p>
<ul>
<li><p><strong>логічні дані</strong> (<em>Boolean</em>), які приймають одне із двох можливих значень (1/0, правда/неправда, True/False);</p></li>
<li><p><strong>числові дані</strong> (<em>numeric</em>), що можуть включати як континуальні значення (<em>double</em>/<em>float</em>, наприклад, вага, зріст тощо), так і дискретні числа (<em>integer</em>, наприклад, кількість особин);</p></li>
<li><p><strong>текст</strong> (<em>character</em>), будь-який набір символів який має принаймні один символ, що не є числом (власне, чому не варто додавати одиниці вимірювання до значень, які за своєю природою є числом);</p></li>
<li><p><strong>категорійні дані</strong> (<em>factor</em>), в яких одне спостереження може приймати одне значення із певного скінченного набору можливих значень (наприклад, вид, стадія життєвого циклу тощо);</p></li>
<li><p><strong>дата/час</strong> (<em>date/time</em>) є особливим форматом даних із яким треба бути дуже обережним, адже форматування іноді може дивно поводитись (наприклад, автоматично переводитись в дискретну кількість секунд із якогось моменту типу 1970-01-01 00:00:00), а неповні дані бувають неочевидними (наприклад, якщо надано тільки місяць і день, то не завжди вдається вгадати рік); найповнішим форматом є “YYYY-MM-DDThh:mm:ss+hh:mm”, наприклад, “2024-10-15T22:43:25-04:00” каже “рік 2024, місяць 10, день 15, година 22, хвилина 43, секунда 25, часовий пояс мінус 4 години від стандартного часу GMT”. Часовий пояс варто наводити, адже в багатьох локаціях наявна змінна літнього і зимового часу; альтернативно, можна наводити час за всесвітнім координованим часом (GMT, позначається як “Z” від Zulu), як-то для попереднього прикладу “2024-10-16T02:43:25Z”.</p></li>
</ul>
<p>Тепер нарешті погляньмо на структуру статистичної моделі. Вся суть моделювання полягає в тому, що дослідник намагається змоделювати певну <strong>залежну змінну</strong> (<em>dependent variable</em>) як функцію однією або декількох незалежних змінних, або <strong>предикторів</strong> (<em>predictor</em>). Для зручності модель можна записати формулою, яка в найпростішій ситуації виглядатиме як</p>
<p><span class="math display">\[y \sim f(x) \Longleftrightarrow y_i = f(x_i) + \epsilon_i\]</span></p>
<p>де ми намагаємось змоделювати кожне спостереження залежної змінної <span class="math inline">\(y\)</span> як функцію змінної <span class="math inline">\(x\)</span> із врахуванням якоїсь статистичної похибки <span class="math inline">\(\epsilon\)</span>.</p>
<p>Відповідно, проблема аналізу даних може становити або проблему <strong>регресії</strong> (<em>regression</em>) якщо залежна змінна має логічні або числові дані, або проблему <strong>класифікації</strong> (<em>classification</em>) якщо залежна змінна є категорійною.</p>
<div id="регресія" class="section level4" number="3.6.2.1">
<h4>
<span class="header-section-number">3.6.2.1</span> Регресія<a class="anchor" aria-label="anchor" href="#%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%96%D1%8F"><i class="fas fa-link"></i></a>
</h4>
<p>Регресія в найпростішому вигляді відповідає побудуванню прямої лінії в двовимірних координатах (Рис. <a href="numerical-ecology.html#fig:fig-3-2">3.2</a>). Таку лінію можна уявити як залежну змінну <span class="math inline">\(y\)</span> у вигляді функції предиктора <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[y \sim x \Longleftrightarrow y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i\]</span></p>
<p>і в такому разі оцінка коефіцієнтів регресії <span class="math inline">\(\beta_0\)</span> (інтерцепт, intercept) та <span class="math inline">\(\beta_1\)</span> (нахил, slope) надасть уявлення про залежність між змінними: якщо <span class="math inline">\(\beta_1 &gt; 0\)</span>, то <span class="math inline">\(y\)</span> збільшується зі збільшенням <span class="math inline">\(x\)</span>, якщо <span class="math inline">\(\beta_1 &lt; 0\)</span> – то існує негативний взаємозв’язок, а статистичне тестування можна зав’язати на нульовій гіпотезі що <span class="math inline">\(\beta_1 = 0\)</span>.</p>
<p>Корисною особливістю лінійної регресії є те, що її можна використати і для моделювання нелінійних залежностей. Наприклад, якщо ми введемо нову змінну <span class="math inline">\(u\)</span>, яка є нелінійною функцією <span class="math inline">\(x\)</span>, скажімо, <span class="math inline">\(u = x^2\)</span>, то нічого не заважає побудувати лінійну регресію</p>
<p><span class="math display">\[y \sim u \Longleftrightarrow y_i = \beta_0 + \beta_1 \cdot u_i + \epsilon_i\]</span></p>
<p>хоча варто мати на увазі, що <strong>поліноміальні</strong> (<em>polynomial</em>) функції є складнішими, тож, наприклад, моделювання поліноміального зв’язку третього порядку матиме вигляд</p>
<p><span class="math display">\[y \sim \text{poly}(x, 3) \Longleftrightarrow y_i = \beta_0 + \beta_1 \cdot x_i + \beta_2 \cdot x_i^2 + \beta_3 \cdot x_i^3 + \epsilon_i\]</span></p>
<p>Крім того, можливо також не обмежувати себе одним предиктором і моделювати залежну змінну за допомогою декількох предикторів. Така регресія зветься <strong>множинною</strong> (<em>multiple regression</em>) і дозволяє оцінити ефект (=коефіцієнт) для кожного предиктора окремо якщо між предикикторами немає взаємної кореляції (<em>multicollinearity</em>):</p>
<p><span class="math display">\[y \sim a + b + c \Longleftrightarrow y_i = \beta_0 + \beta_1 \cdot a_i + \beta_2 \cdot b_i + \beta_3 \cdot c_i + \epsilon_i\]</span></p>
<p>Іншими двома важливими припущеннями базових методів регресії є те, що в моделі відсутня <strong>гетероскедастичність</strong> (<em>heteroscedasticity</em>) – варіація залежної змінної повинна бути незалежною від предикторів, – і те, що розподіл помилки <span class="math inline">\(\epsilon\)</span> є нормальним. Простіші регресійні підходи застосовують <a href="numerical-ecology.html#mle">метод максимальної правдоподібності</a> для оцінки таких значень коефіцієнтів регресії, за яких сума квадратів помилок <span class="math inline">\(\epsilon_i\)</span> є мінімальною, і відтак метод іноді називають <strong>простою регресією найменших квадратів</strong> (<em>ordinary least squares regression</em>, <strong><em>OLS regression</em></strong>). Іноді залежна змінна є не континуальною, а, скажімо, логічною (тобто 0/1) або має один із дискретних розподілів (наприклад, Пуасона). В таких випадках в нагоді стають <strong>узагальнені лінійні моделі</strong> (<em>generalized linear models</em>, <strong><em>GLM</em></strong>). Існують також методи, які дозволяють будувати криві, які локально підбудовують себе під точки спостережень, однак не мають чітко визначених параметрів і використовуються переважно для візуалізації: <strong>узагальнені додатні моделі</strong> (<em>generalized additive models</em>, <strong><em>GAM</em></strong>) та <strong>локально зважені поліноміальні моделі</strong> (<em>locally estimated scatterplot smoothing, local regression</em>, <strong><em>LOESS</em></strong>).</p>
<p>Якщо ж предиктором є не континуальна змінна, а категорійний фактор, така модель являтиме приклад дисперсійного аналізу, або <strong>аналіз варіації</strong> (<em>analysis of variance</em>, <strong><em>ANOVA</em></strong>). ANOVA розділяє залежну змінну на групи, що відповідають різним рівням фактора предиктора, і перевірка статистичної гіпотези зводиться до того, чи варіація між групами є більшою за варіацію в межах груп. Якщо так, то це лише каже що принаймні одна група має значуще відмінні значення залежної змінної від інших груп, однак не каже яка саме – для цього часто застосовують <em>post hoc</em> тест <strong>Тюкі</strong> (<em>Tukey Honestly Significant Difference test</em>, <strong><em>Tukey HSD</em></strong>). Варто пам’ятати, що технічно ANOVA є окремим випадком OLS-регресії, адже тестування гіпотез в регресії використовує то й же механізм, що аналіз варіації (тест Фішера), а фактор можна зобразити у вигляді декількох бінарних колонок, які відповідають рівням фактору (див. <a href="https://mcfromnz.wordpress.com/2011/03/02/anova-type-iiiiii-ss-explained/">коментар</a> щодо типів кодування аналізу варіації).</p>
<p>Аби проілюструвати регресію в R, корисним набором даних може бути <code>iris</code> із стандартних супутній даних в пакеті <code>datasets</code>. В цьому наборі даних наведено проміри в см чотирьох морфологічних ознак квіток: довжина (<code>*.Length</code>) та ширина (<code>*.Width</code>) чашолистків (<code>Sepal.*</code>) та пелюсток (<code>Petal.*</code>), виміряні в 50 особин кожного з трьох видів півників (<em>Iris setosa</em>, <em>I. versicolor</em>, та <em>I. virginica</em>). Відтак, дані мають 150 рядків та 5 колонок:</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># замість того, щоб бачити всі дані, погляньмо на перші декілька рядків</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">iris</span><span class="op">)</span></span></code></pre></div>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa</code></pre>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># скільки рядків і колонок?</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">iris</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 150   5</code></pre>
<p>Побудуймо просту лінійну регресію, в якій <code>Sepal.Length</code> є функцією від <code>Petal.Length</code>:</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># запишемо регресію в об'єкт</span></span>
<span><span class="va">fit_iris1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Sepal.Length</span> <span class="op">~</span> <span class="va">Petal.Length</span>, data <span class="op">=</span> <span class="va">iris</span><span class="op">)</span></span>
<span><span class="co"># поглягньмо на коефіцієнти регресії</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit_iris1</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sepal.Length ~ Petal.Length, data = iris)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.24675 -0.29657 -0.01515  0.27676  1.00269 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.30660    0.07839   54.94   &lt;2e-16 ***
## Petal.Length  0.40892    0.01889   21.65   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.4071 on 148 degrees of freedom
## Multiple R-squared:   0.76,  Adjusted R-squared:  0.7583 
## F-statistic: 468.6 on 1 and 148 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Виглядає, що регресію можна описати лінією <span class="math inline">\(y = 4.307 + 0.409 \cdot x\)</span>. Перевірмо, наскільки добре це описує дані:</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># намалюймо точки даних</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">iris</span><span class="op">$</span><span class="va">Petal.Length</span>, <span class="va">iris</span><span class="op">$</span><span class="va">Sepal.Length</span>, </span>
<span>     xlab <span class="op">=</span> <span class="st">"Petal Length, cm"</span>, ylab <span class="op">=</span> <span class="st">"Sepal Length, cm"</span>, </span>
<span>     pch <span class="op">=</span> <span class="fl">16</span><span class="op">)</span></span>
<span><span class="co"># промалюймо лінію регресії</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>a <span class="op">=</span> <span class="fl">4.3066</span>, b <span class="op">=</span> <span class="fl">0.40892</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-19-1.png" width="672"></div>
<p>Подібним чином можна побудувати й множинну регресію (напр., <code>lm(Sepal.Length ~ Petal.Length + Petal.Width, data = iris)</code>), й усіляко досліджувати лінійні та криволінійні взаємозалежності. Завжди варто мати на увазі, втім, що наявність статистично значущих зв’язків не передбачає причинно-наслідкових зв’язків. Наприклад, чи можна справді вважати що довжина пелюсток визначає довжину чашолистків? Натомість, мабуть, біологічно обґрунтованим стало би припущення, що на обидві ці морфологічні ознаки впливають одні й ті ж довкіллєві чи генетичні фактори<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Зручним уявним експериментом для прикладу абсурдності ототожнювання кореляцій (чи інших статистичних зв’язків) та каузацій є наступна ситуація: нескладно уявити, що кількість пожежників, залучених до гасіння пожеж, позитивно пов’язана зі збитками від пожежі – то чи значить це, що пожежники завдають збитків?&lt;/p&gt;"><sup>36</sup></a>.</p>
</div>
<div id="classifier" class="section level4" number="3.6.2.2">
<h4>
<span class="header-section-number">3.6.2.2</span> Класифікація<a class="anchor" aria-label="anchor" href="#classifier"><i class="fas fa-link"></i></a>
</h4>
<p>Що ж робити, якщо залежна змінна в моделі є не континуальною, а фактором? Наприклад, чи можна за комбінацією морфологічних промірів квітки передбачити вид півників? Таке завдання відповідатиме проблемі класифікації.</p>
<p>Класифікаційна модель отримує на вхід набір дескрипторів репліканта (рядку даних), і на підставі цих даних намагається оцінити ймовірності того, що об’єкт належить до певного <em>класу</em>, наприклад, для трьох видів півників,</p>
<p><span class="math display">\[
\begin{cases}
    P(y_i \in \text{setosa}) = f(x_i) \\
    P(y_i \in \text{versicolor}) = f(x_i) \\
    P(y_i \in \text{virginica}) = f(x_i)
\end{cases}
\]</span></p>
<p>При чому варто очікувати, що <span class="math inline">\(P(y_i \in \text{setosa}) + P(y_i \in \text{versicolor}) + P(y_i \in \text{virginica}) = 1\)</span>.</p>
<p>Існує чимало алгоритмів класифікації, і не всі з них напряму обчислюють ймовірності, але ідея подібна: для кожного рядку даних алгоритм намагається вгадати клас спостереження. Спробуймо використати один із найбільш класичних алгоритмів, <strong>k-найближчих сусідів</strong> (<em>k-nearest neighbors</em>, <strong><em>KNN</em></strong>), для класифікації півників. Механізм алгоритму дуже простий: для кожного нового спостереження, KNN дивиться на найближчі <span class="math inline">\(k\)</span> спостережень і визначає клас нового спостереження як найбільш поширений серед цих <span class="math inline">\(k\)</span> спостережень. Наприклад, якщо <span class="math inline">\(k=3\)</span>, і ми намагаємось вгадати клас для спостереження, найближчі сусіди якого є двома <em>setosa</em> і одним <em>virginica</em>, то нове спостереження визначимо як <em>setosa</em>.</p>
<p>KNN є прикладом алгоритму <strong>машинного навчання із учителем</strong> (<em>supervised machine learning</em>). Такі алгоритми вимагають вхідного, <em>навчального</em>, набору даних із відомою класифікацією (тому й називаються <em>supervised</em>), і очікують нових точок даних для застосування щойно навченого класифікатора. Наприклад, для даних <em>iris</em>, де маємо 150 спостережень, найбільш доцільним питанням із використанням KNN було би “от ми маємо нове спостереження із промірами пелюсток та чашолистиків, але ми не знаємо виду – який це вид?” Тоді KNN пошукає найближчих сусідів і видасть результат.</p>
<p>Вибір значення <span class="math inline">\(k\)</span> є наріжним каменем використання цього методу, адже невідомо скільки найближчих сусідів є забагато чи замало для ефективного алгоритму. Значення <span class="math inline">\(k=3\)</span> і <span class="math inline">\(k=5\)</span> є доволі поширеними, але довільними. Для виправданого вибору цього параметру, варто проводити <a href="numerical-ecology.html#crossval">валідацію</a> класифікатора, про що поговоримо пізніше.</p>
<p>Іншим застосуванням класифікатора може бути поділ простору параметрів. Справа в тім, що один класифікатор, навчений на скінченній кількості спостережень, теоретично можна використати для класифікації незліченної кількості точок, а відтак і прокласифікувати цілий простір замість декількох точок. Наприклад, погляньмо на двовимірний простір промірів пелюсток півників.</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># намалюймо точки даних</span></span>
<span><span class="va">iris</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">Petal.Width</span>, y <span class="op">=</span> <span class="va">Petal.Length</span>, color <span class="op">=</span> <span class="va">Species</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-20-1.png" width="672"></div>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># побудуємо KNN класифікатор із k = 5</span></span>
<span><span class="co"># library(caret)</span></span>
<span><span class="va">fit_iris2</span> <span class="op">&lt;-</span> <span class="fu">caret</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">Species</span> <span class="op">~</span> <span class="va">Petal.Width</span> <span class="op">+</span> <span class="va">Petal.Length</span>, </span>
<span>                   data <span class="op">=</span> <span class="va">iris</span>, </span>
<span>                   method <span class="op">=</span> <span class="st">"knn"</span>, </span>
<span>                   tuneGrid <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>k <span class="op">=</span> <span class="fl">5</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># генерація точок в просторі параметру</span></span>
<span><span class="va">xg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">iris</span><span class="op">$</span><span class="va">Petal.Width</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">iris</span><span class="op">$</span><span class="va">Petal.Width</span><span class="op">)</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="va">yg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">iris</span><span class="op">$</span><span class="va">Petal.Length</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">iris</span><span class="op">$</span><span class="va">Petal.Length</span><span class="op">)</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="va">xyg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span><span class="va">xg</span>, <span class="va">yg</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">xyg</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Petal.Width"</span>, <span class="st">"Petal.Length"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># застосуємо і запишемо класифікацію нових точок</span></span>
<span><span class="va">xyg</span><span class="op">$</span><span class="va">Species</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">fit_iris2</span>, <span class="va">xyg</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># промалюємо простір</span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>   <span class="fu">geom_raster</span><span class="op">(</span>data <span class="op">=</span> <span class="va">xyg</span>,</span>
<span>               <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">Petal.Width</span>, y <span class="op">=</span> <span class="va">Petal.Length</span>, fill <span class="op">=</span> <span class="va">Species</span><span class="op">)</span>,</span>
<span>               alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span>data <span class="op">=</span> <span class="va">iris</span>,</span>
<span>             <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">Petal.Width</span>, y <span class="op">=</span> <span class="va">Petal.Length</span>, color <span class="op">=</span> <span class="va">Species</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-20-2.png" width="672"></div>
<p>Тобто будь-яка нова точка спостережень в червоній зоні буде класифікована як <em>setosa</em>, адже для будь-якої точки в цій зоні більшість найближчих 5 сусідів із вхідного набору даних належать до цього виду; всі нові точки в зеленій зоні будуть класифіковані як <em>versicolor</em>, а в блакитній – як <em>virginica</em>. Цей приклад використав лише два параметри (<code>Petal.Width</code> і <code>Petal.Length</code>), однак KNN може працювати із будь-якою кількістю параметрів (навіть лише з одним, якщо треба).</p>
<p>Цікаво, що KNN можна застосувати і до проблеми регресії, якщо залежна змінна є континуальною. В такому випадку, класифікатор шукатиме середнє значення (або іншу статистику) для <span class="math inline">\(k\)</span> найближчих сусідів.</p>
<p>Загалом же, існує чимало інших алгоритмів класифікації. Деякі із них кластеризують точки без вхідних даних щодо класів спостережень і є, відтак, алгоритмами машинного навчання <em>без учителя</em> (<em>unsupervised machine learning</em>) – наприклад, метод <strong>K-середніх</strong> (<em>K-means</em>), який ітеративно шукає найкращий поділ хмари точок на <span class="math inline">\(K\)</span> кластерів. Тема машинного навчання є дуже популярною, й, відтак, нові алгоритми з’являються доволі часто.</p>
</div>
</div>
<div id="aic" class="section level3" number="3.6.3">
<h3>
<span class="header-section-number">3.6.3</span> Парсимонійна модель та вибір моделі<a class="anchor" aria-label="anchor" href="#aic"><i class="fas fa-link"></i></a>
</h3>
<p>Ми неодноразово вже проговорили, що системи в екології бувають дуже складними, мають безліч факторів що впливають один на одного та на залежні змінні, в яких ми зацікавлені. Давайте поговоримо про це ще раз, цього разу зі згадкою про класичний філософський принцип <strong>бритви Оккама</strong>: не варто ускладнювати припущення без необхідності. Коли еколог намагається описати систему дослідження у вигляді статистичної моделі, варто намагатись зробити таку модель настільки простою, наскільки це можливо без втрати змісту моделі. Пошук найкращої моделі, відтак, виглядатиме як пошук балансу між рівнем складності (скільки параметрів чи факторів можна з неї викинути?) та передбачувальної здатності (якщо модель занадто проста, чи вона хоч щось може пояснити?) моделі, і модель із найкращим таким балансом називають <strong>парсимонійною</strong> (<em>parsimonious</em>).</p>
<p>Загалом, це все, звісно, прекрасно, але то лише філософія. Чи можна принцип Оккама застосувати на практиці? Для цього необхідно було би мати якісь математичні способи оцінити складність моделі, її передбачувальну здатність, та баланс між ними. Оскільки ми вже знаємо, що статистична модель може мати безліч параметрів (як-то <a href="numerical-ecology.html#regression">предиктори в множинній регресії чи класифікаторі</a>, кожен зі своїм коефіцієнтом), кількість змінних в моделі (<span class="math inline">\(k\)</span>) може виступити пристойним оцінщиком ступеню складності. Водночас, оцінка лог-правдоподібності моделі (<span class="math inline">\(\ln\mathcal{L}\)</span>) надає змогу оцінити наскільки добре модель описує дані, відтак виступає гарним оцінщиком передбачувальної здатності. Існує два загальноприйнятих шляхів об’єднати ці два значення в оцінку “парсимонійності” моделі:</p>
<ul>
<li><p><strong>інформаційний критерій Акайке</strong> (<em>Akaike information criterion</em>, <strong><em>AIC</em></strong>): <span class="math inline">\(2k-2\ln\mathcal{L}\)</span>,</p></li>
<li><p><strong>інформаційний критерій Баєса</strong> (<em>Bayesian information criterion</em>, <strong><em>BIC</em></strong>): <span class="math inline">\(\ln (n) \cdot k - 2\ln\mathcal{L}\)</span>.</p></li>
</ul>
<p>В обох випадках, ліва частина формули визначає штраф за складність моделі, виражену через кількість змінних <span class="math inline">\(k\)</span>. Єдина різниця між цими двома критеріями – це те, що BIC зважує складність моделі на логарифм розміру вибірки (<span class="math inline">\(n\)</span>), в той час як AIC має фіксовану вагу (<span class="math inline">\(2\)</span>) для кожної змінної. BIC є не надто поширеним в екології (на відміну від біоінформатики), і більшість експериментальних робіт використовують AIC. Це не лише забаганка моди в наукових дисциплінах: справа в тім, що у великих наборах даних (<span class="math inline">\(n &gt;&gt; k\)</span>, тисячі й мільйони спостережень) AIC недостатньо сильно штрафує за складність моделі, в той час будь-яка, навіть найгірша, модель підбудується до великого набору даних. Відтак, BIC є більш прийнятним вибором для роботи із великими наборами даних, а AIC – для локальних досліджень (<a href="https://biol607.github.io/readings/Aho_2014_ecolog_bic.pdf">Aho et al. 2014</a>, <a href="https://doi.org/10.1111/2041-210X.12541">Brewer et al. 2016</a>).</p>
<p>Формули AIC та BIC є доволі простими, і з них нескладно побачити що інформаційний критерій матиме високе значення для складних моделей (<span class="math inline">\(k &gt;&gt; 1\)</span>) та низьких правдоподібностей (<span class="math inline">\(\mathcal{L} \rightarrow 0 \Rightarrow \ln (\mathcal{L}) \rightarrow - \infty\)</span>), але матиме низьке значення для простих моделей (<span class="math inline">\(k \rightarrow 1\)</span>) із високою правдоподібністю (<span class="math inline">\(\mathcal{L} \rightarrow 1 \Rightarrow \ln (\mathcal{L}) \rightarrow 0\)</span>). Тут і криється один момент щодо інформаційного підходу: значення інформаційного критерія є безрозмірними, і самі по собі нічого не кажуть про парсимонійність моделі. Скажімо, побудована модель має <code>AIC = -7653.783</code>, і що з того? Натомість, інформаційні критерії використовують для <em>порівняння моделей</em>, для пошуку найбільш парсимонійної моделі із набору моделей-кандидаток.</p>
<p>Процедура <strong>вибору моделі</strong> (<em>model selection</em>) передбачає набір моделей-кандидаток, в яких залежна змінна є спільною, однак предиктори відрізняються. Наприклад, в множинній регресії можливо побудувати чимало комбінацій взаємодій між предикторами, наприклад, із двома предикторами:</p>
<ul>
<li><p><code>y ~ a</code>, <span class="math inline">\(y_i = \beta_0 + \beta_1 \cdot a_i + \epsilon_i\)</span>,</p></li>
<li><p><code>y ~ b</code>, <span class="math inline">\(y_i = \beta_0 + \beta_2 \cdot b_i + \epsilon_i\)</span>,</p></li>
<li><p><code>y ~ a + b</code>, <span class="math inline">\(y_i = \beta_0 + \beta_3 \cdot a_i + \beta_4 \cdot b_i + \epsilon_i\)</span>,</p></li>
<li><p><code>y ~ a:b</code>, <span class="math inline">\(y_i = \beta_0 + \beta_5 \cdot (a_i \cdot b_i) + \epsilon_i\)</span>,</p></li>
<li><p><code>y ~ a*b</code>, тотожно до <code>y ~ a + b + a:b</code>, <span class="math inline">\(y_i = \beta_0 + \beta_6 \cdot a_i + \beta_7 \cdot b_i + \beta_8 \cdot (a_i \cdot b_i) + \epsilon_i\)</span>.</p></li>
</ul>
<p>Нескладно уявити, наскільки багато комбінацій можна побудувати для регресії із багатьма предикторами. Вибір найкращої моделі, в такому випадку, дозволяє обрати найкращу комбінацію предикторів. Звісно, для пошуку найкращої моделі можна скористатися <strong>покроковою регресією</strong> (<em>stepwise regression</em>), коли ми або починаємо з найпростішої моделі й додаємо більше й більше змінних (<strong><em>прямий добір</em></strong>, <em>forward selection</em>), або починаємо із найскладнішої моделі і почергово елімінуємо з неї змінні (<strong><em>зворотній добір</em></strong>, <em>backward selection</em>), і все поки не отримаємо модель з такою комбінацією змінних, яка найкраще описує дані. Такий підхід досі використовують, однак від нього варто відмовитись через упередженість й множинне тестування гіпотез (<a href="https://doi.org/10.1111/j.1365-2656.2006.01141.x">Whittingham et al. 2006</a>). Інформаційний підхід є адекватнішою альтернативою алгоритмам, які генерують безліч моделей-кандидаток, однак варто мати на увазі одне правило: <em>всі моделі-кандидатки мають відповідати обдуманому, реалістичному сценарію</em> аби уникнути використання множини моделей із абсурдними комбінаціями змінних. Використання безлічі необґрунтованих моделей або всіх можливих комбінацій змінних є прикладами <strong>просіювання даних</strong> (<em>data dredging</em>) – небажаної тактики статистичного аналізу, яка часто закінчується знаходженням абсурдних, проте статистично значущих закономірностей (<a href="https://doi.org/10.2307/3803199">Anderson et al. 2000</a>).</p>
<p>І от в межах інформаційного підходу (<em>informational approach</em>, термін для використання АІС або ВІС) ми маємо якийсь набір моделей. Тепер час розрахувати обраний інформаційний критерій (скоріш за все, АІС) для кожної моделі окремо, і перелічити моделі в порядку зростання АІС. Модель із найнижчим значенням АІС можна вважати найближчою до парсимонійної<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Це не означає що модель із найнижчим значенням АІС є парсимонійною; грубо кажучи, така модель є просто найменш не-парсимонійною.&lt;/p&gt;"><sup>37</sup></a>, а всі моделі в яких значення АІС не є більшими за дві одиниці (пам’ятаймо, що АІС та ВІС не мають одиниць вимірювання) можна вважати не набагато гіршими і теж врахувати. Ці “найкращі” моделі тепер можна сміливо використати для висновків, наприклад, із тестування гіпотез щодо коефіцієнтів регресії в моделі. Для подачі результатів аналізу в межах інформаційного підходу варто подавати формулу моделі, кількість змінних (<span class="math inline">\(k\)</span>), лог-правдоподібність (<span class="math inline">\(\ln \mathcal{L}\)</span>), значення АІС (чи ВІС), та різницю між АІС моделі та найнижчим значенням АІС в наборі кандидатів (ΔAIC). Часто моделі із <span class="math inline">\(\Delta \text{AIC} \leq 2\)</span> виділяють в таблицях жирним шрифтом.</p>
</div>
<div id="prcomp" class="section level3" number="3.6.4">
<h3>
<span class="header-section-number">3.6.4</span> Багатовимірна статистика<a class="anchor" aria-label="anchor" href="#prcomp"><i class="fas fa-link"></i></a>
</h3>
<p>Як би не хотілося уникнути говорити занадто багато про статистику, ще однією темою, котру треба зачепити, є багатовимірність даних. Це може звучати дуже абстрактно, але у всякому аналізі трапляється проблема багатовимірності, адже кожна змінна є одним окремим виміром даних. Відтак, наприклад, знайомий вже набір даних про квітки півників має аж п’ять вимірів: <code>Sepal.Length</code>, <code>Sepal.Width</code>, <code>Petal.Length</code>, <code>Petal.Width</code>, та <code>Species</code>. В польових дослідженнях, скоріш за все, досліднику доведеться збирати проміри ще більшої кількості параметрів.</p>
<p>Для усвідомлення багатовимірності допомагає згадування теореми Піфагора й Евклідової дистанції. Звісно, твердження “квадрат гіпотенузи дорівнює сумі квадратів катетів” може звучати не надто переконливо. Уявіть, однак, одновимірний простір. Якщо в одному вимірі <span class="math inline">\(x\)</span> ми маємо дві точки <span class="math inline">\(a = (x_a)\)</span> та <span class="math inline">\(b = (x_b)\)</span>, то дистанцію між ними можна нескладно обчислити як абсолютну різницю між їх координатами, <span class="math inline">\(d_{a, b} = |x_a - x_b|\)</span>. Ну, скажімо, добре. Як щодо двовимірного простору? Тут допоможе та ж теорема Піфагора коли уявити ці дві точки <span class="math inline">\(a = (x_a, y_a)\)</span> і <span class="math inline">\(b = (x_b, y_b)\)</span> як два кути прямокутного трикутника, і тоді дистанція між ними дорівнюватиме гіпотенузі цього трикутника: <span class="math inline">\(d_{a, b} = \sqrt{(x_a - x_b)^2 + (y_a - y_b)^2}\)</span>. Повертаючись до одновимірного простору, абсолютну різницю можна дуже легко зобразити як <span class="math inline">\(d_{a,b} = \sqrt{(x_a - x_b)^2}\)</span>. Цю формулу можна генералізувати і для тривимірного простору, для <span class="math inline">\(a = (x_a, y_a, z_a)\)</span> та <span class="math inline">\(b = (x_b, y_b, z_b)\)</span>, <span class="math inline">\(d_{a, b} = \sqrt{(x_a - x_b)^2 + (y_a - y_b)^2 + (z_a - z_b)^2}\)</span>. Краса Евклідової дистанції в тому, що навіть якщо ми можемо уявити одно-, дво-, і три-вимірний простір, але не чотири- і більш-вимірний, то обчислення дистанції все одно працюватиме в скількох завгодно вимірах. Відтак, ми можемо обчислити дистанцію між двома точками (= спостереженнями) у скількох-завгодно-вимірному (= кількість змінних для спостереження) просторі. Для точок <span class="math inline">\(p, q\)</span> в <span class="math inline">\(i = 1, 2, \cdots, n\)</span> вимірах, Евклідова дистанція становитиме <span class="math inline">\(d(p, q) = \sqrt{\sum \limits_{i=1}^{n}(p_i - q_i)^2}\)</span>.</p>
<p>Отже, багатовимірність даних всього лише відповідає кількості змінних в наборі даних. В <a href="numerical-ecology.html#aic">попередньому розділі</a> ми побачили, що існує спосіб вибрати лише обмежений набір змінних, який можна використати для побудови цілком пристойної моделі. Але що якщо ми вважаємо що <em>всі</em> змінні є важливими, і жодну не можна просто так взяти і викинути? Для таких випадків існують методи, що дозволяють трансформувати багато змінних в декілька – методи <strong>ординації</strong> (<em>ordination</em>).</p>
<div id="метод-головних-компонент" class="section level4" number="3.6.4.1">
<h4>
<span class="header-section-number">3.6.4.1</span> Метод головних компонент<a class="anchor" aria-label="anchor" href="#%D0%BC%D0%B5%D1%82%D0%BE%D0%B4-%D0%B3%D0%BE%D0%BB%D0%BE%D0%B2%D0%BD%D0%B8%D1%85-%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82"><i class="fas fa-link"></i></a>
</h4>
<p>Найбільш базовим і поширеним методом ординації є <strong>метод головних компонент</strong> (<em>principal component analysis</em>, <strong><em>PCA</em></strong>). Суть РСА полягає в перетворенні набору вхідних змінних <span class="math inline">\(X = \{x_1, x_2, \cdots, x_n\}\)</span> в таку їх <em>лінійну комбінацію</em> <span class="math inline">\(Z = \{z_1, z_2, \cdots, z_n\}\)</span> де <span class="math inline">\(z_i = \phi_{i,1} \cdot x_1 + \phi_{i,2} \cdot x_2 + \cdots + \phi_{i,n} \cdot x_n\)</span>, а коваріація між вихідними змінними <span class="math inline">\(\text{Cov}(z_i, z_j) \rightarrow 0\)</span>. Завдання цього методу полягає в знаходженні коефіцієнтів <span class="math inline">\(\phi_{i, j}\)</span> для кожної змінної <span class="math inline">\(z_i\)</span> та <span class="math inline">\(x_j\)</span>. Вихідні змінні <span class="math inline">\(Z = \{z_1, z_2, \cdots, z_n\}\)</span> називають <strong>головними компонентами</strong>.</p>
<p>Хоча й методи ординації іноді й називають також методами <strong>зменшення розмірності</strong> даних, це є правдою лише частково: справа в тім, що РСА повертає стільки ж вихідних головних компонент, скільки було вхідних змінних. Корисними властивостями головних компонент є те, що, на відміну від вхідних змінних у більшості випадків, вони є <em>ортогональними</em> – між головними компонентами відсутня кореляція. Іншою корисною властивістю є те, що хоча й сумарна варіація головних компонент дорівнює сумарній варіації вхідних змінних, варіація невідворотно зменшується із порядком головної компоненти. Відтак, найвища варіація спостерігатиметься в першій головній компоненті <span class="math inline">\(z_1\)</span>, трошки менше варіації – в другій головній компоненті <span class="math inline">\(z_2\)</span>, і так далі до найнижчої варіації в останній головній компоненті <span class="math inline">\(z_n\)</span>. Це дозволяє охопити якомога більше варіації вихідних даних (в яких може бути дуже багато змінних) у всього лише декількох перших головних компонент.</p>
<p>Аби зрозуміти логіку РСА, можна уявити що цей метод дозволяє поглянути на багатовимірні дані із найбільш інформативного кута. Наприклад, у наборі даних про квітки півників є чотири континуальні змінні:</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">iris</span><span class="op">)</span></span></code></pre></div>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa</code></pre>
<p>Як би ми не намагались, побудувати чотиривимірний графік є трохи поза межами нашого розуміння дійсності. Звісно, можна подивитись на дані в різних комбінаціях двовимірних графіків:</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://rpkgs.datanovia.com/ggpubr/">ggpubr</a></span><span class="op">)</span> <span class="co"># бібліотека для комбінування графіків в один рисунок</span></span>
<span></span>
<span><span class="fu"><a href="https://rpkgs.datanovia.com/ggpubr/reference/ggarrange.html">ggarrange</a></span><span class="op">(</span></span>
<span>  <span class="va">iris</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Sepal.Length</span>, y <span class="op">=</span> <span class="va">Sepal.Width</span>, color <span class="op">=</span> <span class="va">Species</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position<span class="op">=</span><span class="st">"none"</span><span class="op">)</span>,</span>
<span>  <span class="va">iris</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Sepal.Length</span>, y <span class="op">=</span> <span class="va">Petal.Length</span>, color <span class="op">=</span> <span class="va">Species</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position<span class="op">=</span><span class="st">"none"</span><span class="op">)</span>,</span>
<span>  <span class="va">iris</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Sepal.Length</span>, y <span class="op">=</span> <span class="va">Petal.Width</span>, color <span class="op">=</span> <span class="va">Species</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position<span class="op">=</span><span class="st">"none"</span><span class="op">)</span>,</span>
<span>  <span class="va">iris</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Sepal.Width</span>, y <span class="op">=</span> <span class="va">Petal.Length</span>, color <span class="op">=</span> <span class="va">Species</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position<span class="op">=</span><span class="st">"none"</span><span class="op">)</span>,</span>
<span>  <span class="va">iris</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Sepal.Width</span>, y <span class="op">=</span> <span class="va">Petal.Width</span>, color <span class="op">=</span> <span class="va">Species</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position<span class="op">=</span><span class="st">"none"</span><span class="op">)</span>,</span>
<span>  <span class="va">iris</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Petal.Length</span>, y <span class="op">=</span> <span class="va">Petal.Width</span>, color <span class="op">=</span> <span class="va">Species</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position<span class="op">=</span><span class="st">"none"</span><span class="op">)</span>,</span>
<span>  ncol <span class="op">=</span> <span class="fl">3</span>, nrow <span class="op">=</span> <span class="fl">2</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-22-1.png" width="672"></div>
<p>Однак, як перебудувати виміри таким чином, аби поглянути на цю чотиривимірну хмару точок під таким кутом, за якого ми побачимо найбільше варіації? Тут на допомогу і приходить РСА. В наступному коді ми виконуємо декілька кроків:</p>
<ol style="list-style-type: decimal">
<li><p>застосовуємо <a href="numerical-ecology.html#norm-dirstr">z-стандартизацію</a> аби головні компоненти не були упереджені на користь змінних із високою варіацією<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Z-стандартизація має бути обов’язковим кроком, якщо змінні мають різні одиниці вимірювання, адже варіація в, скажімо, зрості людей (м) буде нижчою за варіацію ваги (кг) суто через те що ми маємо справу із одиницями метрів та десятками кілограмів (пам’ятаймо, що одиницею вимірювання варіації змінної є одиниці самої змінної). Загалом, z-стандартизація є хорошою практикою для уникнення різних артефактів в аналізі.&lt;/p&gt;"><sup>38</sup></a>;</p></li>
<li><p>застосовуємо метод головних компонент (функція <code>prcomp</code>);</p></li>
<li><p>малюємо хмару точок в просторі <code>Petal.Length</code>-<code>Sepal.Length</code> (ці змінні мають найбільшу варіацію до стандартизації) і осі перших двох компонент – таке зображення РСА не є типовим, але є сподівання що воно допоможе зрозуміти що відбувається, і як перші дві головні компоненти є ортогональними, хоча й кут між ними не виглядає на 90° (насправді він становить 90°, просто ми дивимось на двовимірну проекцію чотиривимірного простору);</p></li>
<li><p>малюємо цю ж хмару точок в проекції перших двох компонент, у випадку чого класично осі позначають як номер головної компоненти (Principal Component, PC) і відсоток сумарної варіації даних, що припадає на ці головні компоненти;</p></li>
<li><p>дивимось на розподіл варіації даних за головними компонентами – скільки інформації ми змогли захопити в перших двох компонентах?</p></li>
</ol>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Крок 1: z-стандартизація</span></span>
<span></span>
<span><span class="va">iris_scaled</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/scale.html">scale</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co"># тепер у кожної змінної середнє дорівнює 0, sd = 1</span></span>
<span></span>
<span><span class="co"># додамо змінну з видом з ориінального набору даних</span></span>
<span><span class="va">iris_scaled</span><span class="op">$</span><span class="va">species</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">$</span><span class="va">Species</span></span>
<span></span>
<span><span class="co"># Крок 2: РСА</span></span>
<span></span>
<span><span class="va">iris_pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="va">iris_scaled</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Крок 3: проекція оригінальних стандартизованих даних</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Petal.Length</span>, y <span class="op">=</span> <span class="va">Sepal.Length</span>, shape <span class="op">=</span> <span class="va">species</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">iris_scaled</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_abline</a></span><span class="op">(</span>slope <span class="op">=</span> <span class="va">iris_pca</span><span class="op">$</span><span class="va">rotation</span><span class="op">[</span><span class="st">"Sepal.Length"</span>, <span class="st">"PC1"</span><span class="op">]</span><span class="op">/</span><span class="va">iris_pca</span><span class="op">$</span><span class="va">rotation</span><span class="op">[</span><span class="st">"Petal.Length"</span>, <span class="st">"PC1"</span><span class="op">]</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"red"</span>, alpha <span class="op">=</span> <span class="fl">0.7</span>, lty <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_abline</a></span><span class="op">(</span>slope <span class="op">=</span> <span class="va">iris_pca</span><span class="op">$</span><span class="va">rotation</span><span class="op">[</span><span class="st">"Sepal.Length"</span>, <span class="st">"PC2"</span><span class="op">]</span><span class="op">/</span><span class="va">iris_pca</span><span class="op">$</span><span class="va">rotation</span><span class="op">[</span><span class="st">"Petal.Length"</span>, <span class="st">"PC2"</span><span class="op">]</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"blue"</span>, alpha <span class="op">=</span> <span class="fl">0.7</span>, lty <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-23-1.png" width="672"></div>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Крок 4: проекція перших двох головних компонент</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">autoplot</a></span><span class="op">(</span><span class="va">iris_pca</span>, data <span class="op">=</span> <span class="va">iris</span>, shape <span class="op">=</span> <span class="st">'Species'</span>,</span>
<span>         loadings <span class="op">=</span> <span class="cn">T</span>, loadings.colour <span class="op">=</span> <span class="st">'gray'</span>,</span>
<span>         loadings.label <span class="op">=</span> <span class="cn">T</span>, loadings.label.size <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_y_reverse</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span>, color <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">0</span>, color <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-23-2.png" width="672"></div>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Сірі стрілки позначають осі оригінальних змінних в проекції РС1-РС2.</span></span>
<span><span class="co"># Зверніть увагу що вісь РС2 інвертована,</span></span>
<span><span class="co"># просто так легше візуально знайти відповідні точки з попереднього графіку.</span></span>
<span><span class="co"># Відсотки в назвах осей відповідають розподілу варіації</span></span>
<span></span>
<span><span class="co"># Крок 5: Розподіл варіації</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">iris_pca</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-23-3.png" width="672"></div>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Скільки варіації головні компоненти мають кумулятивно?</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">iris_pca</span><span class="op">$</span><span class="va">sdev</span><span class="op">*</span><span class="fl">100</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">iris_pca</span><span class="op">$</span><span class="va">sdev</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1]  53.52972  83.48653  95.49021 100.00000</code></pre>
<p>Отак ми й змогли зобразити чотиривимірні дані в двох вимірах, при чому ми бачимо 83.49% варіації всіх даних із використанням тільки першої й другої головних компонент.</p>
<p>Одним практичним застереженням до використання РСА є необхідність уникати лінійних комбінацій змінних у вхідних даних (наприклад, якщо одна змінна є сумою двох інших змінних). Це пов’язано із тим, що під капотом РСА – чутливі підходи <a href="numerical-ecology.html#matrices">лінійної алгебри</a>, і наявність лінійних комбінацій призведе до виродження матриць. Ба більше, необхідно також зважати на чутливість РСА до сильної кореляції між вхідними змінними (<a href="https://doi.org/10.1111/evo.13835">Björklund 2019</a>, див. також <a href="https://stats.stackexchange.com/questions/50537/should-one-remove-highly-correlated-variables-before-doing-pca">дискусію на цю тему</a>). Як би це не було парадоксально, адже властивість ортогональності головних компонент часто використовується для уникнення мультиколінеарності<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Наприклад, оскільки множинна регресія не може бути використана до мультиколінеарних даних, за наявності кореляції між вхідними змінними можна використати РСА і застосувати регресію до головних компонент (&lt;a href="https://www.statlearning.com/"&gt;James et al. 2021&lt;/a&gt;).&lt;/p&gt;'><sup>39</sup></a>, однак бажано уникати сильно корельованих змінних у вхідних даних до РСА. На лінійно-алгебраїчну природу РСА також натякає той факт, що варіацією головних компонент є ніщо інше як власні значення (eigenvalues) матриці кореляції/коваріації вхідних змінних, а коефіцієнтами <span class="math inline">\(\phi_{i,j}\)</span> – власні вектори (eigenvectors)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;В контексті РСА власні вектори часто називають &lt;strong&gt;навантаженням&lt;/strong&gt; (&lt;em&gt;loading&lt;/em&gt;, &lt;em&gt;rotation&lt;/em&gt;) – ці значення й пов’язують координати нових точок головних компонент із координатами вихідних змінних.&lt;/p&gt;"><sup>40</sup></a>.</p>
<p>Іншою поширеною помилкою є сліпе використання РСА, коли дослідник забуває про біологічний зміст кожної головної компоненти. Головні компоненти є просто лінійними комбінаціями вхідних змінних, і твердження на кшталт “головна компонента 1 значуще впливає на залежну змінну” не має жодного сенсу допоки дослідник не з’ясує, із якими вхідними змінними ця перша головна компонента пов’язана найбільше. Для висновків щодо змісту головних компонент можна використати просту кореляцію між кожною окремою головною компонентою і вхідними змінними:</p>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/taiyun/corrplot">corrplot</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/corrplot/man/corrplot.html">corrplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span>, <span class="va">iris_pca</span><span class="op">$</span><span class="va">x</span><span class="op">)</span>, </span>
<span>         title <span class="op">=</span> <span class="st">""</span>, method <span class="op">=</span> <span class="st">"ellipse"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-24-1.png" width="672"></div>
<p>На цьому прикладі ми бачимо, що перша головна компонента “ввібрала” в себе найбільше інформації щодо форми пелюсток (змінні <code>Petal.Length</code>, <code>Petal.Width</code>), в той час як друга – про форму чашолистків (сильна негативна кореляція із <code>Sepal.Width</code>).</p>
</div>
<div id="інші-методи-ординації" class="section level4" number="3.6.4.2">
<h4>
<span class="header-section-number">3.6.4.2</span> Інші методи ординації<a class="anchor" aria-label="anchor" href="#%D1%96%D0%BD%D1%88%D1%96-%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D0%B8-%D0%BE%D1%80%D0%B4%D0%B8%D0%BD%D0%B0%D1%86%D1%96%D1%97"><i class="fas fa-link"></i></a>
</h4>
<p>Значним обмеженням РСА є те, що цей метод може бути застосований лише для континуальних змінних, а в екології дані можуть мати вигляд інших типів змінних. В той час як РСА є найбільш популярним методом, він є і найбільш базовим, на основі якого створено чимало варіацій ординації:</p>
<ul>
<li><p><strong>Аналіз головних координат</strong> (<em>principal coordinates analysis</em>, <strong><em>PCoA</em></strong>) замість набору даних, що описують об’єкти континуальними змінними, використовує матрицю дистанцій між об’єктами. Такий підхід є доволі зручним коли дані містять не-континуальні змінні для пар значень яких можна оцінити дистанцію між значеннями.</p></li>
<li><p><strong>Неметричне багатовимірне шкалювання</strong> (<em>non-metric multidimensional scaling</em>, <strong><em>NMDS</em></strong>) є підтипом РСоА, особливо популярним в екології угруповань. Цей метод уявляє угруповання видів в багатовимірному просторі чисельності цих видів (тобто скільки видів, стільки й вимірів), і використовує рангування чисельностей видів для знаходження оптимальної позиції угруповань в просторі із зменшеною кількістю вимірів (зазвичай, в двовимірному просторі). Див. <a href="https://jonlefcheck.net/2012/10/24/nmds-tutorial-in-r/">це застосування методу в R</a> із трохи більш детальним описом методу.</p></li>
<li><p><strong>Аналіз канонічної відповідності</strong> (<em>canonical correspondence analysis</em>, <strong><em>CCA</em></strong>) припускає що певні змінні (наприклад, чисельності видів) мають не лінійну, а, скоріш, куполо-подібний зв’язок із іншими змінними (наприклад, параметрами середовища). На відміну від попередніх методів, ССА вимагає два набори даних: наприклад, про чисельності видів і параметри середовища, і на виході надає змогу розглянути які змінні із двох наборів даних пов’язані між собою. Див. більше деталей <a href="https://rfunctions.blogspot.com/2016/11/canonical-correspondence-analysis-cca.html">тут</a>.</p></li>
<li><p><strong>Аналіз надлишковості</strong> (<em>redundancy analysis</em>, <strong><em>RDA</em></strong>) є подібним методом до ССА, однак в той час як ССА визначає симетричні взаємозв’язки між змінними в двох наборах даних, RDA вимагає попередньої інформації про те який набір даних містить предиктори, а який – залежні змінні. Див. більше деталей <a href="https://r.qcbs.ca/workshop10/book-en/redundancy-analysis.html">тут</a>.</p></li>
</ul>
<p>В той час як РСА, РСоА, та NMDS використовуються тільки для зменшення розмірності даних, ССА та RDA можуть бути корисними і для тестування гіпотез щодо взаємозв’язків в багатовимірних даних.</p>
</div>
</div>
</div>
<div id="infer" class="section level2" number="3.7">
<h2>
<span class="header-section-number">3.7</span> Передбачення, умовивід, та валідація<a class="anchor" aria-label="anchor" href="#infer"><i class="fas fa-link"></i></a>
</h2>
<blockquote>
<p>“Всі моделі є хибними.</p>
<p>Але деякі з них ще й корисні.”</p>
<p>— за <a href="https://doi.org/10.1080%2F01621459.1976.10480949">Джорджем Боксом</a></p>
</blockquote>
<p>Здавалось би, гаразд, якщо у дослідника є базове розуміння математичних тем, усвідомлення поняття ймовірності, достатньо ґрунтовні навички використання наукового методу для тестування гіпотез, яке-не-яке розуміння статистичного аналізу, то із цим усім можна проводити доволі непогані дослідження в екології. Загалом, мабуть, воно так і є, однак ще до початку проведення аналізу (а, як так, то і до проведення експерименту) необхідно розуміти кінцеву мету дослідження — чи ми очікуємо відповідь на конкретне фундаментальне питання, чи результати матимуть більше прикладну роль для подальших розробок. Хоча ці два звірі і є двома боками боками однієї монети, у них доволі специфічні й відмінні присмаки. Цей розділ буде завершальним у цій математичній секції, і являтиме собою настанову, фінальними міркуваннями, про котрі варто іноді задуматись в аналізі даних: (1) що саме ми хочемо досягнути цим аналізом, і (2) чи проведений аналіз є притомним.</p>
<div id="inference" class="section level3" number="3.7.1">
<h3>
<span class="header-section-number">3.7.1</span> Статистичний умовивід і обґрунтоване передбачення<a class="anchor" aria-label="anchor" href="#inference"><i class="fas fa-link"></i></a>
</h3>
<p>Всякий статистичний аналіз можна звести до ситуації, коли існує набір предикторів <span class="math inline">\(X\)</span>, певна залежна змінна <span class="math inline">\(Y\)</span>, і аналіз зводиться до пошуку такої функції <span class="math inline">\(f\)</span>, що <span class="math inline">\(Y = f(X) + \epsilon\)</span>, де <span class="math inline">\(\epsilon\)</span> позначає випадкову помилку, а функція <span class="math inline">\(f\)</span> може бути <a href="numerical-ecology.html#regression">регресією, класифікатором</a>, чи якоюсь складною моделлю. Ми ніколи не дізнаємось справжній вигляд <span class="math inline">\(f\)</span> (бо це може бути дуже складною функцією), але інструменти статистичного аналізу дозволяють знайти якусь апроксимацію до <span class="math inline">\(f\)</span>, скажімо, <span class="math inline">\(\hat{f}\)</span>, що дозволятиме побудувати модель вигляду <span class="math inline">\(\hat{Y} = \hat{f}(X)\)</span>. Навіщо ж оцінювати <span class="math inline">\(\hat{f}\)</span>? Власне, оцінка <span class="math inline">\(\hat{f}\)</span> дозволяє нам робити <strong>передбачення</strong> (<em>prediction</em>) або <strong>умовивід</strong> (<em>inference</em>) (<a href="https://doi.org/10.1007/978-1-0716-1418-1_2">James et al. 2021</a>).</p>
<div id="передбачення" class="section level4" number="3.7.1.1">
<h4>
<span class="header-section-number">3.7.1.1</span> Передбачення<a class="anchor" aria-label="anchor" href="#%D0%BF%D0%B5%D1%80%D0%B5%D0%B4%D0%B1%D0%B0%D1%87%D0%B5%D0%BD%D0%BD%D1%8F"><i class="fas fa-link"></i></a>
</h4>
<p>Передбачення корисне в ситуаціях, коли дані щодо змінних в <span class="math inline">\(X\)</span> можна легко отримати у великих вибірках<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Наприклад, дистанційне зондування поверхні планети – мультиспектральні супутникові знімки із програм Sentinel та Landsat є у вільному доступі й оновлюються кожні 10–16 днів.&lt;/p&gt;"><sup>41</sup></a>, однак збір даних щодо <span class="math inline">\(Y\)</span> вимагає інтенсивної польової роботи. Якщо є необхідність оцінити <span class="math inline">\(Y\)</span> опосередковано через <span class="math inline">\(X\)</span>, тоді можна скористатися моделлю <span class="math inline">\(\hat{Y} = \hat{f}(X)\)</span>. В такому випадку, сама модель <span class="math inline">\(\hat{f}\)</span> не є у фокусі нашої уваги – нехай це буде хоч <em>якась</em> модель, аби тільки <span class="math inline">\(\hat{Y}\)</span> приблизно відповідало дійсності.</p>
<p>Звісно, всяка модель є лише спрощенням складної реальності, і оцінене <span class="math inline">\(\hat{Y}\)</span> ніколи не дорівнюватиме невідомому та реальному <span class="math inline">\(Y\)</span>. Помилка передбачення (різниця між <span class="math inline">\(\hat{Y}\)</span> та <span class="math inline">\(Y\)</span>) складається із двох частин: по-перше, завжди існує <em>незменшувана помилка</em> (<em>irreducible error</em>), що походить із статистичного шуму в досліджуваній системі – та сама <span class="math inline">\(\epsilon\)</span>; по-друге, модель сама по собі може містити певні <a href="numerical-ecology.html#bias-variance">упередження щодо даних</a>, що можна змінити модифікаціями моделі, й, відтак, ця помилка є <em>зменшуваною</em> (<em>reducible error</em>). Якщо уявити що вся варіація в даних походить від <span class="math inline">\(\epsilon\)</span>, то <a href="numerical-ecology.html#pdfs">математичне очікування</a> помилки передбачення дорівнює</p>
<p><span class="math display">\[\mathbb{E}[(Y - \hat{Y})^2] = \mathbb{E} [[f(X) + \epsilon - \hat{f}(X)]^2] = [f(X) - \hat{f}(X)]^2 + Var(\epsilon)\]</span></p>
<p>де <span class="math inline">\([f(X) - \hat{f}(X)]^2\)</span> відповідає зменшуваній помилці – усередненому очікуванню помилки передбачення, а <span class="math inline">\(Var(\epsilon)\)</span> – варіації незменшуваної помилки.</p>
<p>Варто зауважити, що оскільки проблема передбачення фокусується на <span class="math inline">\(\hat{Y}\)</span>, в той час як сама модель <span class="math inline">\(\hat{f}\)</span> є, радше, інструментом<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Іншими словами, нам в принципі все одно яку модель використовувати, нехай це буде хоч &lt;a href="https://uk.wikipedia.org/wiki/%D0%A7%D0%BE%D1%80%D0%BD%D0%B8%D0%B9_%D1%8F%D1%89%D0%B8%D0%BA"&gt;чорна скринька&lt;/a&gt;, аби тільки на виході із моделі виходило хороше передбачення.&lt;/p&gt;'><sup>42</sup></a>, то використання статистичного аналізу для передбачення іноді дозволяє послабити вимоги до припущень методів. Наприклад, однією із залізних вимог множинної лінійної регресії є відсутність колінеарності між предикторами – тобто предиктори не мають мати кореляцію між собою. Регресійна модель із колінеарними предикторами не є адекватною, адже весь математичний апарат регресії ґрунтується на припущенні, що предиктори є незалежними<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;… а також гомоскедастичними і нормально розподіленими – дуже раджу читати про припущення методів перед їх використанням!&lt;/p&gt;"><sup>43</sup></a>. Іронія полягає в тому, що використання такої моделі є цілком прийнятним для передбачення якщо <a href="numerical-ecology.html#crossval">її передбачення є точними</a> (знову ж, нам все одно <em>як</em> працює модель, аби тільки вона працювала), хоча її й не можна використовувати для умовиводу (див. приклад нижче).</p>
</div>
<div id="умовивід" class="section level4" number="3.7.1.2">
<h4>
<span class="header-section-number">3.7.1.2</span> Умовивід<a class="anchor" aria-label="anchor" href="#%D1%83%D0%BC%D0%BE%D0%B2%D0%B8%D0%B2%D1%96%D0%B4"><i class="fas fa-link"></i></a>
</h4>
<p>Як вже можна було здогадатись, якщо для моделі <span class="math inline">\(\hat{Y} = \hat{f}(X)\)</span> задача передбачення звертає найбільше уваги на результат моделі <span class="math inline">\(\hat{Y}\)</span>, то задача умовиводу більше дивиться на саму модель <span class="math inline">\(\hat{f}\)</span>. У фундаментальних дослідженнях дослідник часто питатиме щось на кшталт “а як впливає ця змінна на нашу систему?” або “чи ця змінна є важливою?” В таких ситуаціях до моделі не можна ставитись як до чорної скриньки, а навпаки, ми намагаємось розплутати взаємозв’язки в досліджуваній системі і з’ясувати що до чого шляхом тествання чітко сформульованих статистичних гіпотез. Очікувано, чим складніші математичні методи застосовуються в аналізі, тим складніше буває інтерпретація результатів. Відтак, для умовиводу краще застосовувати простіші методи (наприклад, лінійну регресію замість узагальнених додатніх моделей) – хоча всяку модель, яку можна використати для умовиводу, можна використати і для передбачення. Звісно, завжди варто мати на увазі що статистична значущість не тотожна біологічній значущості, отже, задача умовиводу в чистому вигляді не повинна всеціло покладатись на статистичний аналіз, а, радше, використовувати його для аргументації. Чистий умовивід тісно переплітається із філософією науки та базовими методами <a href="numerical-ecology.html#pval">перевірки гіпотез</a>. На практиці ж, наукове дослідження балансуватиме між умовиводом та передбаченням, тож варто просто мати на увазі ці два кути погляду на статистичний аналіз.</p>
<p>На завершення, різницю між передбаченням і умовиводом із лінійною регресією можна проілюструвати простим прикладом із генерованими даними.</p>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># створимо випадкову змінну із нормальним розподілом та 100 елементами</span></span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># функція для генерації випадкової змінної із визначеною кореляцією</span></span>
<span><span class="co"># до вхідної змінної</span></span>
<span><span class="va">corvar</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">rho</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">orth</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span><span class="op">$</span><span class="va">residuals</span></span>
<span>  <span class="va">rho</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">orth</span><span class="op">)</span><span class="op">*</span><span class="va">x</span> <span class="op">+</span> <span class="va">orth</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">rho</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># створимо другу випадкову змінну</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="fu">corvar</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x1</span>, rho <span class="op">=</span> <span class="fl">0.99</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># перевіримо чи кореляція між х1 і х2 становить 0.99</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.99</code></pre>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># згенеруємо залежну змінну як функцію х1 та х2 із рандомною помилкою</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">1</span><span class="op">*</span><span class="va">x1</span> <span class="op">+</span> <span class="fl">1</span><span class="op">*</span><span class="va">x2</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x1</span><span class="op">)</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">0.25</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># побудуймо множинну регресію</span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># погляньмо на коефіцієнти регресії</span></span>
<span><span class="co"># ми очікуємо на 1 для інтерцепту, 1 для х1, та 1 для х2</span></span>
<span><span class="va">fit</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">.</span><span class="op">$</span><span class="va">coefficients</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span>digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code>##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)    1.026      0.026  39.370    0.000
## x1             1.129      0.203   5.553    0.000
## x2             0.503      0.673   0.748    0.456</code></pre>
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># як бачимо, оцінка параметрів неадекватна, </span></span>
<span><span class="co"># цю модель не можна використовувати для умовиводу</span></span>
<span></span>
<span><span class="co"># наскільки передбачення моделі далекі від істини?</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">y</span>,</span>
<span>     y <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">fitted.values</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">'Згенеровані ("реальні") значення Y'</span>, ylab <span class="op">=</span> <span class="st">"Передбачені значення Y"</span>,</span>
<span>     pch <span class="op">=</span> <span class="fl">16</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>a <span class="op">=</span> <span class="fl">0</span>, b <span class="op">=</span> <span class="fl">1</span>, lty <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-25-1.png" width="672"></div>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># виглядає, що модель передбачає Y доволі точно</span></span></code></pre></div>
</div>
</div>
<div id="crossval" class="section level3" number="3.7.2">
<h3>
<span class="header-section-number">3.7.2</span> Крос-валідація<a class="anchor" aria-label="anchor" href="#crossval"><i class="fas fa-link"></i></a>
</h3>
<p>Коли у дослідника є побудована модель (не важливо, чи для передбачення, чи для умовиводу), очевидним питанням є наскільки ця модель є адекватною. Звісно, конкретні методи мають певні метрики пристосованості (<em>fitness</em>) даних до моделі, як-то <span class="math inline">\(R^2\)</span> і <span class="math inline">\(R_{adj}^2\)</span> для лінійних моделей, що описує частку варіації в даних що пояснюється моделлю, не кажучи вже про <a href="numerical-ecology.html#aic">інформаційні метрики</a>. Цікавим підходом до валідації моделі, що є набагато більш гнучким відносно різноманіття методів є <strong>крос-валідація</strong> (<em>cross-validation</em>).</p>
<p>Із прикладу в попередньому розділі можна побачити, що за наявності вхідних даних, робочої статистичної моделі, та передбачень моделі можна порівняти вихідну залежну змінну <span class="math inline">\(Y\)</span> та її апроксимацію, передбачену моделлю (<span class="math inline">\(\hat{Y}\)</span>). Для кожного спостереження можна оцінити, наскільки далеко передбачення моделі від реальності: <span class="math inline">\((y_i - \hat{y_i})^2\)</span>, а відтак можна і додати відхилення в межах усієї моделі у певну статистику, що зветься <strong>середнім квадратом відхилень</strong> (<em>mean sqared error</em>, <strong><em>MSE</em></strong>):</p>
<p><span class="math display">\[\frac{1}{n} \sum_{i = 1}^n ( y_i - \hat{y_i} )^2\]</span></p>
<p>Звісно, для одного набору даних і однієї моделі можна розрахувати MSE тільки раз, та й це не надто має сенс, адже якщо в даних вже є відома залежна змінна, то навіщо її передбачати? Однак, пригадаймо <a href="numerical-ecology.html#permutation-paradigm">пермутаційні методи</a> – якщо є лише одна вибірка, то із неї все одно можна згенерувати багато значень статистики. Подібний принцип працює і із крос-валідацією: чому б не розділити вхідний набір даних на два випадкові набори? Тоді можна побудувати модель на одному наборі (<em>тренувальні дані</em>), спробувати передбачити залежну змінну в іншому наборі (<em>валідаційні дані</em>), і розрахувати MSE для передбачених значень у валідаційному наборі. Оскільки поділ вхідних даних на тренувальний та валідаційний набори є довільним, цю процедуру можна повторювати багаторазово із випадковим розподілом спостережень між наборами.</p>
<p>Існує чимало алгоритмів крос-валідації, із яких популярними є два:</p>
<ul>
<li>
<p><strong>крос-валідація із виключенням по одному</strong> (<em>leave-one-out cross-validation</em>, <strong><em>LOOCV</em></strong>):</p>
<ol style="list-style-type: decimal">
<li><p>для кожного спостереження <span class="math inline">\(i\)</span> у наборі даних із <span class="math inline">\(n\)</span> спостережень, побудуймо окрему модель (<span class="math inline">\(\hat{f_i}\)</span>) на тренувальних даних, із яких видалено <span class="math inline">\(i\)</span>-те спостереження (розмір тренувального набору становитиме <span class="math inline">\(n-1\)</span>);</p></li>
<li><p>використаймо модель <span class="math inline">\(\hat{f_i}\)</span> на валідаційному спостереженні <span class="math inline">\(x_i\)</span> для передбачення значення залежної змінної <span class="math inline">\(\hat{y_i}\)</span>, розрахуймо квадрат відхилення <span class="math inline">\((y_i - \hat{y_i})^2\)</span>;</p></li>
<li><p>повторимо для всіх <span class="math inline">\(i = 1, 2, \cdots, n\)</span>, і розрахуймо MSE як <span class="math inline">\(\frac{1}{n} \sum_{i = 1}^n ( y_i - \hat{y_i} )^2\)</span>.</p></li>
</ol>
</li>
<li>
<p><strong>k-кратна крос-валідація</strong> (<em>k-fold cross-validation</em>, <strong><em>k-fold CV</em></strong>):</p>
<ol style="list-style-type: decimal">
<li><p>розділимо вхідний набір даних на <span class="math inline">\(k\)</span> випадкових набори приблизно однакового розміру (<span class="math inline">\(\approx n/k\)</span>): <span class="math inline">\(T_1, T_2, \cdots, T_k\)</span>;</p></li>
<li><p>для кожного <span class="math inline">\(i = 1, 2, \cdots, k\)</span>, використаймо всі набори окрім <span class="math inline">\(i\)</span>-того в якості тренувальних даних для моделі <span class="math inline">\(\hat{f_i}\)</span>;</p></li>
<li><p>використаймо модель <span class="math inline">\(\hat{f_i}\)</span> на валідаційному наборі <span class="math inline">\(T_i\)</span> для передбачення значень залежної змінної <span class="math inline">\(\hat{Y_i}\)</span>, розрахуймо MSE;</p></li>
<li><p>повторення процедури для всіх <span class="math inline">\(T_1, T_2, \cdots, T_k\)</span> генерує <span class="math inline">\(k\)</span> оцінок MSE, які можна усереднити як <span class="math inline">\(\frac{1}{k}\sum_{i=1}^k \text{MSE}_i\)</span>.</p></li>
</ol>
</li>
</ul>
<p>Можна легко уявити, що LOOCV є окремим випадком k-fold CV, в якому <span class="math inline">\(k=n\)</span>.</p>
<p>Процедура крос-валідації корисна для моделей із гнучкими параметрами, котрі обираються довільно, і оптимальне значення параметру залежить від даних. Наприклад, в алгоритмі <a href="numerical-ecology.html#classifier">k-найближчих сусідів (KNN)</a> можна використати будь-яке значення <span class="math inline">\(k\)</span>, однак яке із них найкраще? Для крос-валідації в R існують спеціальні бібліотеки (наприклад, <a href="https://rpubs.com/njvijay/16444"><code>caret::train()</code></a>), однак, залежно від ситуації (наприклад, використання незвичної родини моделей), іноді простіше буває писати алгоритм самостійно.</p>
</div>
<div id="bias-variance" class="section level3" number="3.7.3">
<h3>
<span class="header-section-number">3.7.3</span> Компроміс між упередженням та варіацією<a class="anchor" aria-label="anchor" href="#bias-variance"><i class="fas fa-link"></i></a>
</h3>
<p>На завершення, із параметризацією моделей завжди варто бути обережними, бо із цим дуже легко перестаратись. Наприклад, із використанням поліномільної регресії буває нескладно побудувати таку криву, яка майже точно проходить через всі точки. Наприклад, згенеруймо хмару випадкових точок і побудуймо поліноміальні моделі різної складності:</p>
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">11</span><span class="op">)</span></span>
<span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">20</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="fl">0</span>, <span class="fl">0.5</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">tibble</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, formula <span class="op">=</span> <span class="st">"y ~ poly(x, 1)"</span>, se <span class="op">=</span> <span class="cn">F</span>,</span>
<span>              <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span>, color <span class="op">=</span> <span class="st">"p = 1"</span><span class="op">)</span>, show_guide <span class="op">=</span> <span class="cn">T</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, formula <span class="op">=</span> <span class="st">"y ~ poly(x, 2)"</span>, se <span class="op">=</span> <span class="cn">F</span>, </span>
<span>              <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span>, color <span class="op">=</span> <span class="st">"p = 2"</span><span class="op">)</span>, show_guide <span class="op">=</span> <span class="cn">T</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, formula <span class="op">=</span> <span class="st">"y ~ poly(x, 5)"</span>, se <span class="op">=</span> <span class="cn">F</span>, </span>
<span>              <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span>, color <span class="op">=</span> <span class="st">"p = 5"</span><span class="op">)</span>, show_guide <span class="op">=</span> <span class="cn">T</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, formula <span class="op">=</span> <span class="st">"y ~ poly(x, 10)"</span>, se <span class="op">=</span> <span class="cn">F</span>, </span>
<span>              <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span>, color <span class="op">=</span> <span class="st">"p = 9"</span><span class="op">)</span>, show_guide <span class="op">=</span> <span class="cn">T</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_manual.html">scale_color_manual</a></span><span class="op">(</span>name <span class="op">=</span> <span class="st">"Порядок"</span>,</span>
<span>                     values <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"p = 1"</span> <span class="op">=</span> <span class="st">"blue"</span>, <span class="st">"p = 2"</span> <span class="op">=</span> <span class="st">"darkblue"</span>, </span>
<span>                                <span class="st">"p = 5"</span> <span class="op">=</span> <span class="st">"red"</span>, <span class="st">"p = 9"</span> <span class="op">=</span> <span class="st">"darkred"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>, size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-26-1.png" width="672"></div>
<p>Очевидно, що поліноміальна регресія 9-го порядку набагато краще описує дані, аніж першого. З іншого боку, модель всього лише описує позитивний зв’язок змінних <span class="math inline">\(x\)</span> та <span class="math inline">\(y\)</span>, і дані мали би описуватись як <span class="math inline">\(y = 1 + 1 x + \epsilon\)</span>, а не як cкладний поліном на кшталт</p>
<p><span class="math display">\[y = -0.5 + 4x + 0.6x^2 - 0.6x^3 -0.2x^4 -0.1x^5 - 0.2x^6 - 0.1x^7 + 0.2x^8 + 0.8x^9 + \epsilon\]</span></p>
<p>Ці коефіцієнти можна знайти у відповідній моделі:</p>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit9</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">x</span>, <span class="fl">9</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">fit9</span><span class="op">$</span><span class="va">coefficients</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code>## (Intercept) poly(x, 9)1 poly(x, 9)2 poly(x, 9)3 poly(x, 9)4 poly(x, 9)5 
##        -0.5         4.0         0.6        -0.6        -0.2        -0.1 
## poly(x, 9)6 poly(x, 9)7 poly(x, 9)8 poly(x, 9)9 
##        -0.2        -0.1         0.2         0.8</code></pre>
<p>Тож наскільки вдалою є складна поліноміальна модель для опису цих даних? Для власне <em>цього</em> набору даних – дуже вдалою, але проблема в тому, що складна модель підходить тільки для <em>цих</em> даних, і якщо ми згенеруємо інший випадковий набір даних, то попередня модель раптом стане дуже невдалою:</p>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">12</span><span class="op">)</span></span>
<span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">20</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="fl">0</span>, <span class="fl">0.5</span><span class="op">)</span></span>
<span></span>
<span><span class="va">y9</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">fit9</span>, <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1.538</span>, <span class="fl">1.323</span>, <span class="fl">0.01</span><span class="op">)</span><span class="op">)</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, size <span class="op">=</span> <span class="fl">2</span>,</span>
<span>             data <span class="op">=</span> <span class="fu">tibble</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>,</span>
<span>            data <span class="op">=</span> <span class="fu">tibble</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1.538</span>, <span class="fl">1.323</span>, <span class="fl">0.01</span><span class="op">)</span>,</span>
<span>                          y <span class="op">=</span> <span class="va">y9</span><span class="op">)</span>,</span>
<span>            color <span class="op">=</span> <span class="st">"darkred"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-28-1.png" width="672"></div>
<p>Конструювання моделі, що є занадто складною для вхідних даних і, відтак, майже ідеально підбудовується до неї, є практикою що часто називають <strong>пере-пристосуванням</strong>, або перенавчанням (<em>overfitting</em>). Пере-пристосовані моделі, завдяки своїй складності, разом із реальним взаємозв’язком між змінними описують випадковий шум (який є <a href="numerical-ecology.html#inference">незменшуваною помилкою</a>), і відтак не можуть бути використані для адекватного умомиводу та передбачення.</p>
<p>Отже, якщо взяти інший набір даних із тієї ж дослідної системи, то складна пере-пристосована модель, побудована на тих же даних, матиме зовсім інший вигляд (наприклад, коефіцієнти поліноміального рівняння значно зміняться). Властивість моделі змінюватися значною мірою залежно від даних, використаних для її побудування, називають <strong>варіацією</strong> (<em>variance</em>). Модель із високою варіацією буде значно змінюватись за незначних змін у вхідному наборі даних, і високопластичні методи (на кшталт тих же поліномальних моделей високого порядку) мають, зазвичай, високу варіацію. Ідеально, ми намагаємось зменшити варіацію методу, аби побудована модель не сильно залежала від випадкового шуму в даних і не була <a href="https://en.wikipedia.org/wiki/Idiosyncrasy">ідіосинкратичною</a> до конкретного використаного набору даних.</p>
<p>З іншого боку, <strong>упередження</strong> (<em>bias</em>) методу відповідає тій похибці, яку ми вводимо коли намагаємось описати складні взаємозв’язки реального життя простими моделями. Наприклад, проста лінійна регресія є доволі потужним і легко інтерпретованим методом, однак в екологічних системах рідко коли можна спостерігати чіткий лінійний зв’язок. Всякий алгоритм в аналізі даних може містити системну помилку і невірно інтерпретувати дані, і подальше використання цього алгоритму вводитиме упередження. Значне упередження моделі викликатиме <em>недо-пристосування</em>, коли нестача гнучкості моделі змушує модель ігнорувати релевантну інформацію (Рис. <a href="numerical-ecology.html#fig:fig-high-bias">3.15</a>).</p>
<div class="figure">
<span style="display:block;" id="fig:fig-high-bias"></span>
<img src="bookdown-demo_files/figure-html/fig-high-bias-1.png" alt="Використання простого методу для опису складної нелінійної залежності, в принципі, є можливим, однак призводитиме до хибного умовиводу та передбачення. В цьому випадку існує куполоподібна залежність між змінними, однак проста лінійна регресія не вказує на будь-який статистично значущий взаємозв'язок. Приклад описує зв'язок між кількістю відкладених яєць самками американського кліща собачого (*Dermacentor variabilis*) та температурою середовища в період перед відкладанням яєць ([Mount &amp; Haile 1989](https://doi.org/10.1093/jmedent/26.1.60))." width="672"><p class="caption">
Рис. 3.15: Використання простого методу для опису складної нелінійної залежності, в принципі, є можливим, однак призводитиме до хибного умовиводу та передбачення. В цьому випадку існує куполоподібна залежність між змінними, однак проста лінійна регресія не вказує на будь-який статистично значущий взаємозв’язок. Приклад описує зв’язок між кількістю відкладених яєць самками американського кліща собачого (<em>Dermacentor variabilis</em>) та температурою середовища в період перед відкладанням яєць (<a href="https://doi.org/10.1093/jmedent/26.1.60">Mount &amp; Haile 1989</a>).
</p>
</div>
<p>Найкращим методом для статистичного умовиводу та передбачення є такий метод, який має найменше упередження і найменшу варіацію. Однак, на практиці ці дві властивості пов’язані між собою: пластичні методи мають високу варіацію та низьке упередження. Балансування варіації та упередження в аналізі даних часто називають <strong>компромісом упередження та варіації</strong> (<em>bias-variance trade-off</em>), і пошук найкращої моделі зводиться до обережного налаштування параметрів складності моделі, точності її передбачень, та можливості генералізації моделі і її використання на даних, що не були використані на етапі тренування моделі.</p>
</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="about-book.html"><span class="header-section-number">2</span> Про книгу та Зміст</a></div>
<div class="next"><a href="popeco.html"><span class="header-section-number">4</span> Початки популяційної екології</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#numerical-ecology"><span class="header-section-number">3</span> Базові математичні підходи в екології</a></li>
<li>
<a class="nav-link" href="#algebra"><span class="header-section-number">3.1</span> Математична пам’ятка</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#%D0%B4%D1%80%D0%BE%D0%B1%D0%B8"><span class="header-section-number">3.1.1</span> Дроби</a></li>
<li><a class="nav-link" href="#%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%BD%D1%96-%D1%81%D0%B8%D0%BC%D0%B2%D0%BE%D0%BB%D0%B8"><span class="header-section-number">3.1.2</span> Математичні символи</a></li>
<li><a class="nav-link" href="#%D0%BD%D0%B5%D1%80%D1%96%D0%B2%D0%BD%D0%BE%D1%81%D1%82%D1%96"><span class="header-section-number">3.1.3</span> Нерівності</a></li>
<li><a class="nav-link" href="#%D1%81%D1%82%D1%83%D0%BF%D0%B5%D0%BD%D1%96"><span class="header-section-number">3.1.4</span> Ступені</a></li>
<li><a class="nav-link" href="#%D1%80%D1%8F%D0%B4%D0%B8-%D1%87%D0%B8%D1%81%D0%B5%D0%BB"><span class="header-section-number">3.1.5</span> Ряди чисел</a></li>
<li><a class="nav-link" href="#%D1%81%D1%82%D1%83%D0%BF%D0%B5%D0%BD%D1%96-%D0%B0%D1%80%D0%B8%D1%84%D0%BC%D0%B5%D1%82%D0%B8%D1%87%D0%BD%D0%B8%D1%85-%D0%BE%D0%BF%D0%B5%D1%80%D0%B0%D1%86%D1%96%D0%B9"><span class="header-section-number">3.1.6</span> Ступені арифметичних операцій</a></li>
<li><a class="nav-link" href="#%D0%BB%D1%96%D0%BD%D1%96%D0%B9%D0%BD%D1%96-%D1%82%D0%B0-%D0%BF%D0%BE%D0%BB%D1%96%D0%BD%D0%BE%D0%BC%D1%96%D0%B0%D0%BB%D1%8C%D0%BD%D1%96-%D1%84%D1%83%D0%BD%D0%BA%D1%86%D1%96%D1%97"><span class="header-section-number">3.1.7</span> Лінійні та поліноміальні функції</a></li>
<li><a class="nav-link" href="#logs"><span class="header-section-number">3.1.8</span> Логарифми</a></li>
<li><a class="nav-link" href="#%D0%BF%D0%BE%D1%88%D0%B8%D1%80%D0%B5%D0%BD%D1%96-%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%BD%D1%96-%D1%84%D1%83%D0%BD%D0%BA%D1%86%D1%96%D1%97"><span class="header-section-number">3.1.9</span> Поширені математичні функції</a></li>
<li><a class="nav-link" href="#%D0%B2%D0%BB%D0%B0%D1%81%D1%82%D0%B8%D0%B2%D0%BE%D1%81%D1%82%D1%96-%D1%81%D1%83%D0%BC"><span class="header-section-number">3.1.10</span> Властивості сум</a></li>
<li><a class="nav-link" href="#%D0%B2%D0%BB%D0%B0%D1%81%D1%82%D0%B8%D0%B2%D0%BE%D1%81%D1%82%D1%96-%D0%B4%D0%BE%D0%B1%D1%83%D1%82%D0%BA%D1%96%D0%B2"><span class="header-section-number">3.1.11</span> Властивості добутків</a></li>
<li><a class="nav-link" href="#%D0%B4%D0%B8%D1%84%D0%B5%D1%80%D0%B5%D0%BD%D1%86%D1%96%D1%8E%D0%B2%D0%B0%D0%BD%D0%BD%D1%8F"><span class="header-section-number">3.1.12</span> Диференціювання</a></li>
<li><a class="nav-link" href="#%D1%96%D0%BD%D1%82%D0%B5%D0%B3%D1%80%D1%83%D0%B2%D0%B0%D0%BD%D0%BD%D1%8F"><span class="header-section-number">3.1.13</span> Інтегрування</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#matrices"><span class="header-section-number">3.2</span> Лінійна алгебра</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#%D0%B2%D0%B8%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%BD%D1%8F-%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D1%96"><span class="header-section-number">3.2.1</span> Визначення матриці</a></li>
<li><a class="nav-link" href="#%D1%82%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D1%96%D1%97-%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D1%8C"><span class="header-section-number">3.2.2</span> Трансформації матриць</a></li>
<li><a class="nav-link" href="#%D0%BE%D0%BF%D0%B5%D1%80%D0%B0%D1%86%D1%96%D1%97-%D0%BD%D0%B0%D0%B4-%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D1%8F%D0%BC%D0%B8"><span class="header-section-number">3.2.3</span> Операції над матрицями</a></li>
<li><a class="nav-link" href="#%D0%B4%D0%B5%D1%82%D0%B5%D1%80%D0%BC%D1%96%D0%BD%D0%B0%D0%BD%D1%82-%D0%B2%D0%BB%D0%B0%D1%81%D0%BD%D1%96-%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%B8-%D1%82%D0%B0-%D0%B2%D0%BB%D0%B0%D1%81%D0%BD%D0%B5-%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%BD%D1%8F"><span class="header-section-number">3.2.4</span> Детермінант, власні вектори, та власне значення</a></li>
<li><a class="nav-link" href="#matrices_art"><span class="header-section-number">3.2.5</span> Геометричний зміст матриць</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#stats"><span class="header-section-number">3.3</span> Ймовірність у статистиці</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#prob"><span class="header-section-number">3.3.1</span> Ймовірність</a></li>
<li><a class="nav-link" href="#bayes"><span class="header-section-number">3.3.2</span> Теорема Баєса</a></li>
<li><a class="nav-link" href="#mle"><span class="header-section-number">3.3.3</span> Правдоподібність</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#pdf-pmf"><span class="header-section-number">3.4</span> Розподіли ймовірності</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#pdfs"><span class="header-section-number">3.4.1</span> Функції розподілу ймовірності</a></li>
<li><a class="nav-link" href="#bars"><span class="header-section-number">3.4.2</span> Опис розподілу змінної (описова статистика)</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#basic-hypotheses"><span class="header-section-number">3.5</span> Тестування гіпотез</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#hypothesis"><span class="header-section-number">3.5.1</span> Статистична гіпотеза</a></li>
<li><a class="nav-link" href="#nulldistr"><span class="header-section-number">3.5.2</span> Нульовий розподіл</a></li>
<li><a class="nav-link" href="#pval"><span class="header-section-number">3.5.3</span> Тестування гіпотез</a></li>
<li><a class="nav-link" href="#paradigms"><span class="header-section-number">3.5.4</span> Парадигми статистичного аналізу</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#stat-models"><span class="header-section-number">3.6</span> Експеримент і модель</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#pseudoreplication"><span class="header-section-number">3.6.1</span> Експериментальний дизайн та псевдореплікація</a></li>
<li><a class="nav-link" href="#regression"><span class="header-section-number">3.6.2</span> Дані та проблема моделювання</a></li>
<li><a class="nav-link" href="#aic"><span class="header-section-number">3.6.3</span> Парсимонійна модель та вибір моделі</a></li>
<li><a class="nav-link" href="#prcomp"><span class="header-section-number">3.6.4</span> Багатовимірна статистика</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#infer"><span class="header-section-number">3.7</span> Передбачення, умовивід, та валідація</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#inference"><span class="header-section-number">3.7.1</span> Статистичний умовивід і обґрунтоване передбачення</a></li>
<li><a class="nav-link" href="#crossval"><span class="header-section-number">3.7.2</span> Крос-валідація</a></li>
<li><a class="nav-link" href="#bias-variance"><span class="header-section-number">3.7.3</span> Компроміс між упередженням та варіацією</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Вступ до Екології Угруповань</strong>" was written by Олексій Дубовик. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
